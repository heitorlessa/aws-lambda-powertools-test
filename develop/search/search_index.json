{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homepage","text":"<p>Testing new docs pr mechanism take 2</p> <p>Powertools is a developer toolkit to implement Serverless best practices and increase developer velocity.</p> Tip <p>Powertools is also available for Java, TypeScript, and .NET</p> Support this project by becoming a reference customer, sharing your work, or using Layers/SAR  <p>You can choose to support us in three ways:</p> <p>1) Become a reference customer. This gives us permission to list your company in our documentation.</p> <p>2) Share your work. Blog posts, video, sample projects you used Powertools!</p> <p>3) Use Lambda Layers or SAR, if possible. This helps us understand who uses Powertools in a non-intrusive way, and helps us gain future investments for other Powertools languages.</p> <p>When using Layers, you can add Powertools as a dev dependency (or as part of your virtual env) to not impact the development process.</p>"},{"location":"#install","title":"Install","text":"<p>You can install Powertools using one of the following options:</p> <ul> <li>Lambda Layer (x86_64): arn:aws:lambda:{region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:31</li> <li>Lambda Layer (arm64): arn:aws:lambda:{region}:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31</li> <li>Pip: <code>pip install \"aws-lambda-powertools\"</code></li> </ul> Using Pip? You might need to install additional dependencies. <p>Tracer, Validation and Parser require additional dependencies. If you prefer to install all of them, use <code>pip install \"aws-lambda-powertools[all]\"</code>.</p> <p>For example:</p> <ul> <li>Tracer: <code>pip install \"aws-lambda-powertools[tracer]\"</code></li> <li>Validation: <code>pip install \"aws-lambda-powertools[validation]\"</code></li> <li>Parser: <code>pip install \"aws-lambda-powertools[parser]\"</code></li> <li>Tracer and Parser: <code>pip install \"aws-lambda-powertools[tracer,parser]\"</code></li> </ul>"},{"location":"#local-development","title":"Local development","text":"<p>Using Powertools via Lambda Layer? Simply add <code>\"aws-lambda-powertools[all]\"</code> as a development dependency.</p> <p>Powertools relies on the AWS SDK bundled in the Lambda runtime. This helps us achieve an optimal package size and initialization. However, when developing locally, you need to install AWS SDK as a development dependency (not as a production dependency):</p> <ul> <li>Pip: <code>pip install \"aws-lambda-powertools[aws-sdk]\"</code></li> <li>Poetry: <code>poetry add \"aws-lambda-powertools[aws-sdk]\" --group dev</code></li> <li>Pipenv: <code>pipenv install --dev \"aws-lambda-powertools[aws-sdk]\"</code></li> </ul> Why is that necessary? <p>Powertools relies on the AWS SDK being available to use in the target runtime (AWS Lambda).</p> <p>As a result, it affects your favorite IDE in terms of code auto-completion, or running your tests suite locally with no Lambda emulation such as AWS SAM CLI.</p> <p>A word about dependency resolution</p> <p>In this context, <code>[aws-sdk]</code> is an alias to the <code>boto3</code> package. Due to dependency resolution, it'll either install:</p> <ul> <li>(A) the SDK version available in Lambda runtime</li> <li>(B) a more up-to-date version if another package you use also depends on <code>boto3</code>, for example Powertools Tracer</li> </ul>"},{"location":"#lambda-layer","title":"Lambda Layer","text":"As of now, Container Image deployment (OCI) or inline Lambda functions do not support Lambda Layers. <p>Lambda Layer is a .zip file archive that can contain additional code, pre-packaged dependencies, data,  or configuration files. Layers promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic.</p> <p>For our Layers, we compile and optimize all dependencies, and remove duplicate dependencies already available in the Lambda runtime to achieve the most optimal size.</p> <p>You can include Powertools Lambda Layer using AWS Lambda Console, or your preferred deployment framework.</p> Note: Click to expand and copy any regional Lambda Layer ARN x86_64arm64 Region Layer ARN <code>af-south-1</code> arn:aws:lambda:af-south-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>ap-east-1</code> arn:aws:lambda:ap-east-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>ap-northeast-1</code> arn:aws:lambda:ap-northeast-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>ap-northeast-2</code> arn:aws:lambda:ap-northeast-2:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>ap-northeast-3</code> arn:aws:lambda:ap-northeast-3:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>ap-south-1</code> arn:aws:lambda:ap-south-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>ap-south-2</code> arn:aws:lambda:ap-south-2:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>ap-southeast-1</code> arn:aws:lambda:ap-southeast-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>ap-southeast-2</code> arn:aws:lambda:ap-southeast-2:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>ap-southeast-3</code> arn:aws:lambda:ap-southeast-3:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>ap-southeast-4</code> arn:aws:lambda:ap-southeast-4:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>ca-central-1</code> arn:aws:lambda:ca-central-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>eu-central-1</code> arn:aws:lambda:eu-central-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>eu-central-2</code> arn:aws:lambda:eu-central-2:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>eu-north-1</code> arn:aws:lambda:eu-north-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>eu-south-1</code> arn:aws:lambda:eu-south-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>eu-south-2</code> arn:aws:lambda:eu-south-2:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>eu-west-1</code> arn:aws:lambda:eu-west-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>eu-west-2</code> arn:aws:lambda:eu-west-2:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>eu-west-3</code> arn:aws:lambda:eu-west-3:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>me-central-1</code> arn:aws:lambda:me-central-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>me-south-1</code> arn:aws:lambda:me-south-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>sa-east-1</code> arn:aws:lambda:sa-east-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>us-east-1</code> arn:aws:lambda:us-east-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>us-east-2</code> arn:aws:lambda:us-east-2:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>us-west-1</code> arn:aws:lambda:us-west-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 <code>us-west-2</code> arn:aws:lambda:us-west-2:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 Region Layer ARN <code>af-south-1</code> arn:aws:lambda:af-south-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>ap-east-1</code> arn:aws:lambda:ap-east-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>ap-northeast-1</code> arn:aws:lambda:ap-northeast-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>ap-northeast-2</code> arn:aws:lambda:ap-northeast-2:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>ap-northeast-3</code> arn:aws:lambda:ap-northeast-3:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>ap-south-1</code> arn:aws:lambda:ap-south-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>ap-southeast-1</code> arn:aws:lambda:ap-southeast-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>ap-southeast-2</code> arn:aws:lambda:ap-southeast-2:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>ap-southeast-3</code> arn:aws:lambda:ap-southeast-3:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>ca-central-1</code> arn:aws:lambda:ca-central-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>eu-central-1</code> arn:aws:lambda:eu-central-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>eu-north-1</code> arn:aws:lambda:eu-north-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>eu-south-1</code> arn:aws:lambda:eu-south-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>eu-west-1</code> arn:aws:lambda:eu-west-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>eu-west-2</code> arn:aws:lambda:eu-west-2:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>eu-west-3</code> arn:aws:lambda:eu-west-3:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>me-south-1</code> arn:aws:lambda:me-south-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>sa-east-1</code> arn:aws:lambda:sa-east-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>us-east-1</code> arn:aws:lambda:us-east-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>us-east-2</code> arn:aws:lambda:us-east-2:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>us-west-1</code> arn:aws:lambda:us-west-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 <code>us-west-2</code> arn:aws:lambda:us-west-2:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31 Note: Click to expand and copy code snippets for popular frameworks x86_64arm64 SAMServerless frameworkCDKTerraformPulumiAmplify <pre><code>MyLambdaFunction:\nType: AWS::Serverless::Function\nProperties:\nLayers:\n- !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:31\n</code></pre> <pre><code>functions:\nhello:\nhandler: lambda_function.lambda_handler\nlayers:\n- arn:aws:lambda:${aws:region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:31\n</code></pre> <pre><code>from aws_cdk import core, aws_lambda\n\nclass SampleApp(core.Construct):\n\n    def __init__(self, scope: core.Construct, id_: str, env: core.Environment) -&gt; None:\n        super().__init__(scope, id_)\n\n        powertools_layer = aws_lambda.LayerVersion.from_layer_version_arn(\n            self,\n            id=\"lambda-powertools\",\nlayer_version_arn=f\"arn:aws:lambda:{env.region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:31\"\n)\n        aws_lambda.Function(self,\n            'sample-app-lambda',\n            runtime=aws_lambda.Runtime.PYTHON_3_9,\nlayers=[powertools_layer]\n# other props...\n        )\n</code></pre> <pre><code>terraform {\nrequired_version = \"~&gt; 1.0.5\"\nrequired_providers {\naws = \"~&gt; 3.50.0\"\n}\n}\n\nprovider \"aws\" {\nregion  = \"{region}\"\n}\n\nresource \"aws_iam_role\" \"iam_for_lambda\" {\nname = \"iam_for_lambda\"\n\nassume_role_policy = &lt;&lt;EOF\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\n        {\n          \"Action\": \"sts:AssumeRole\",\n          \"Principal\": {\n            \"Service\": \"lambda.amazonaws.com\"\n          },\n          \"Effect\": \"Allow\"\n        }\n      ]\n    }\n    EOF\n}\n\nresource \"aws_lambda_function\" \"test_lambda\" {\nfilename      = \"lambda_function_payload.zip\"\nfunction_name = \"lambda_function_name\"\nrole          = aws_iam_role.iam_for_lambda.arn\nhandler       = \"index.test\"\nruntime       = \"python3.9\"\nlayers        = [\"arn:aws:lambda:{region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:31\"]\nsource_code_hash = filebase64sha256(\"lambda_function_payload.zip\")\n}\n</code></pre> <pre><code>import json\nimport pulumi\nimport pulumi_aws as aws\n\nrole = aws.iam.Role(\"role\",\n    assume_role_policy=json.dumps({\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n        \"Action\": \"sts:AssumeRole\",\n        \"Principal\": {\n            \"Service\": \"lambda.amazonaws.com\"\n        },\n        \"Effect\": \"Allow\"\n        }\n    ]\n    }),\n    managed_policy_arns=[aws.iam.ManagedPolicy.AWS_LAMBDA_BASIC_EXECUTION_ROLE]\n)\n\nlambda_function = aws.lambda_.Function(\"function\",\n    layers=[pulumi.Output.concat(\"arn:aws:lambda:\",aws.get_region_output().name,\":017000801446:layer:AWSLambdaPowertoolsPythonV2:11\")],\n    tracing_config={\n        \"mode\": \"Active\"\n    },\n    runtime=aws.lambda_.Runtime.PYTHON3D9,\n    handler=\"index.handler\",\n    role=role.arn,\n    architectures=[\"x86_64\"],\n    code=pulumi.FileArchive(\"lambda_function_payload.zip\")\n)\n</code></pre> <pre><code># Create a new one with the layer\n\u276f amplify add function\n? Select which capability you want to add: Lambda function (serverless function)\n? Provide an AWS Lambda function name: &lt;NAME-OF-FUNCTION&gt;\n? Choose the runtime that you want to use: Python\n? Do you want to configure advanced settings? Yes\n...\n? Do you want to enable Lambda layers for this function? Yes\n? Enter up to 5 existing Lambda layer ARNs (comma-separated): arn:aws:lambda:eu-central-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31\n\u276f amplify push -y\n\n\n# Updating an existing function and add the layer\n\u276f amplify update function\n? Select the Lambda function you want to update test2\nGeneral information\n- Name: &lt;NAME-OF-FUNCTION&gt;\n? Which setting do you want to update? Lambda layers configuration\n? Do you want to enable Lambda layers for this function? Yes\n? Enter up to 5 existing Lambda layer ARNs (comma-separated): arn:aws:lambda:eu-central-1:017000801446:layer:AWSLambdaPowertoolsPythonV2:31\n? Do you want to edit the local lambda function now? No\n</code></pre> SAMServerless frameworkCDKTerraformPulumiAmplify <pre><code>MyLambdaFunction:\nType: AWS::Serverless::Function\nProperties:\nArchitectures: [arm64]\nLayers:\n- !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31\n</code></pre> <pre><code>functions:\nhello:\nhandler: lambda_function.lambda_handler\narchitecture: arm64\nlayers:\n- arn:aws:lambda:${aws:region}:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31\n</code></pre> <pre><code>from aws_cdk import core, aws_lambda\n\nclass SampleApp(core.Construct):\n\n    def __init__(self, scope: core.Construct, id_: str, env: core.Environment) -&gt; None:\n        super().__init__(scope, id_)\n\n        powertools_layer = aws_lambda.LayerVersion.from_layer_version_arn(\n            self,\n            id=\"lambda-powertools\",\nlayer_version_arn=f\"arn:aws:lambda:{env.region}:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31\"\n)\n        aws_lambda.Function(self,\n            'sample-app-lambda',\n            runtime=aws_lambda.Runtime.PYTHON_3_9,\n            architecture=aws_lambda.Architecture.ARM_64,\nlayers=[powertools_layer]\n# other props...\n        )\n</code></pre> <pre><code>terraform {\nrequired_version = \"~&gt; 1.0.5\"\nrequired_providers {\naws = \"~&gt; 3.50.0\"\n}\n}\n\nprovider \"aws\" {\nregion  = \"{region}\"\n}\n\nresource \"aws_iam_role\" \"iam_for_lambda\" {\nname = \"iam_for_lambda\"\n\nassume_role_policy = &lt;&lt;EOF\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\n        {\n          \"Action\": \"sts:AssumeRole\",\n          \"Principal\": {\n            \"Service\": \"lambda.amazonaws.com\"\n          },\n          \"Effect\": \"Allow\"\n        }\n      ]\n    }\n    EOF\n}\n\nresource \"aws_lambda_function\" \"test_lambda\" {\nfilename      = \"lambda_function_payload.zip\"\nfunction_name = \"lambda_function_name\"\nrole          = aws_iam_role.iam_for_lambda.arn\nhandler       = \"index.test\"\nruntime       = \"python3.9\"\nlayers        = [\"arn:aws:lambda:{region}:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31\"]\narchitectures = [\"arm64\"]\n\nsource_code_hash = filebase64sha256(\"lambda_function_payload.zip\")\n}\n</code></pre> <pre><code>import json\nimport pulumi\nimport pulumi_aws as aws\n\nrole = aws.iam.Role(\"role\",\n    assume_role_policy=json.dumps({\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n        \"Action\": \"sts:AssumeRole\",\n        \"Principal\": {\n            \"Service\": \"lambda.amazonaws.com\"\n        },\n        \"Effect\": \"Allow\"\n        }\n    ]\n    }),\n    managed_policy_arns=[aws.iam.ManagedPolicy.AWS_LAMBDA_BASIC_EXECUTION_ROLE]\n)\n\nlambda_function = aws.lambda_.Function(\"function\",\n    layers=[pulumi.Output.concat(\"arn:aws:lambda:\",aws.get_region_output().name,\":017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:11\")],\n    tracing_config={\n        \"mode\": \"Active\"\n    },\n    runtime=aws.lambda_.Runtime.PYTHON3D9,\n    handler=\"index.handler\",\n    role=role.arn,\n    architectures=[\"arm64\"],\n    code=pulumi.FileArchive(\"lambda_function_payload.zip\")\n)\n</code></pre> <pre><code># Create a new one with the layer\n\u276f amplify add function\n? Select which capability you want to add: Lambda function (serverless function)\n? Provide an AWS Lambda function name: &lt;NAME-OF-FUNCTION&gt;\n? Choose the runtime that you want to use: Python\n? Do you want to configure advanced settings? Yes\n...\n? Do you want to enable Lambda layers for this function? Yes\n? Enter up to 5 existing Lambda layer ARNs (comma-separated): arn:aws:lambda:eu-central-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31\n\u276f amplify push -y\n\n\n# Updating an existing function and add the layer\n\u276f amplify update function\n? Select the Lambda function you want to update test2\nGeneral information\n- Name: &lt;NAME-OF-FUNCTION&gt;\n? Which setting do you want to update? Lambda layers configuration\n? Do you want to enable Lambda layers for this function? Yes\n? Enter up to 5 existing Lambda layer ARNs (comma-separated): arn:aws:lambda:eu-central-1:017000801446:layer:AWSLambdaPowertoolsPythonV2-Arm64:31\n? Do you want to edit the local lambda function now? No\n</code></pre> Want to inspect the contents of the Layer? <p>Change {region} to your AWS region, e.g. <code>eu-west-1</code></p> AWS CLI<pre><code>aws lambda get-layer-version-by-arn --arn arn:aws:lambda:{region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:31 --region {region}\n</code></pre> <p>The pre-signed URL to download this Lambda Layer will be within <code>Location</code> key.</p>"},{"location":"#sar","title":"SAR","text":"<p>Serverless Application Repository (SAR) App deploys a CloudFormation stack with a copy of our Lambda Layer in your AWS account and region.</p> <p>Compared with the public Layer ARN option, SAR allows you to choose a semantic version and deploys a Layer in your target account.</p> App ARN Description aws-lambda-powertools-python-layer arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer Contains all extra dependencies (e.g: pydantic). aws-lambda-powertools-python-layer-arm64 arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer-arm64 Contains all extra dependencies (e.g: pydantic). For arm64 functions. Click to expand and copy SAR code snippets for popular frameworks <p>You can create a shared Lambda Layers stack and make this along with other account level layers stack.</p> SAMServerless frameworkCDKTerraform <pre><code>AwsLambdaPowertoolsPythonLayer:\nType: AWS::Serverless::Application\nProperties:\nLocation:\nApplicationId: arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer\nSemanticVersion: 2.0.0 # change to latest semantic version available in SAR\nMyLambdaFunction:\nType: AWS::Serverless::Function\nProperties:\nLayers:\n# fetch Layer ARN from SAR App stack output\n- !GetAtt AwsLambdaPowertoolsPythonLayer.Outputs.LayerVersionArn\n</code></pre> <pre><code>functions:\nmain:\nhandler: lambda_function.lambda_handler\nlayers:\n- !GetAtt AwsLambdaPowertoolsPythonLayer.Outputs.LayerVersionArn\nresources:\nTransform: AWS::Serverless-2016-10-31\nResources:****\nAwsLambdaPowertoolsPythonLayer:\nType: AWS::Serverless::Application\nProperties:\nLocation:\nApplicationId: arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer\n# Find latest from github.com/awslabs/aws-lambda-powertools-python/releases\nSemanticVersion: 2.0.0\n</code></pre> <pre><code>from aws_cdk import core, aws_sam as sam, aws_lambda\n\nPOWERTOOLS_BASE_NAME = 'AWSLambdaPowertools'\n# Find latest from github.com/awslabs/aws-lambda-powertools-python/releases\nPOWERTOOLS_VER = '2.0.0'\nPOWERTOOLS_ARN = 'arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer'\n\nclass SampleApp(core.Construct):\n\n    def __init__(self, scope: core.Construct, id_: str) -&gt; None:\n        super().__init__(scope, id_)\n\n        # Launches SAR App as CloudFormation nested stack and return Lambda Layer\npowertools_app = sam.CfnApplication(self,\nf'{POWERTOOLS_BASE_NAME}Application',\n            location={\n                'applicationId': POWERTOOLS_ARN,\n                'semanticVersion': POWERTOOLS_VER\n            },\n        )\n\npowertools_layer_arn = powertools_app.get_att(\"Outputs.LayerVersionArn\").to_string()\npowertools_layer_version = aws_lambda.LayerVersion.from_layer_version_arn(self, f'{POWERTOOLS_BASE_NAME}', powertools_layer_arn)\naws_lambda.Function(self,\n            'sample-app-lambda',\n            runtime=aws_lambda.Runtime.PYTHON_3_8,\n            function_name='sample-lambda',\n            code=aws_lambda.Code.asset('./src'),\n            handler='app.handler',\nlayers: [powertools_layer_version]\n)\n</code></pre> <p>Credits to Dani Comnea for providing the Terraform equivalent.</p> <pre><code>terraform {\nrequired_version = \"~&gt; 0.13\"\nrequired_providers {\naws = \"~&gt; 3.50.0\"\n}\n}\n\nprovider \"aws\" {\nregion  = \"us-east-1\"\n}\n\nresource \"aws_serverlessapplicationrepository_cloudformation_stack\" \"deploy_sar_stack\" {\nname = \"aws-lambda-powertools-python-layer\"\napplication_id   = data.aws_serverlessapplicationrepository_application.sar_app.application_id\nsemantic_version = data.aws_serverlessapplicationrepository_application.sar_app.semantic_version\ncapabilities = [\n\"CAPABILITY_IAM\",\n\"CAPABILITY_NAMED_IAM\"\n]\n}\n\ndata \"aws_serverlessapplicationrepository_application\" \"sar_app\" {\napplication_id   = \"arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer\"\nsemantic_version = var.aws_powertools_version\n}\n\nvariable \"aws_powertools_version\" {\ntype        = string\ndefault     = \"2.0.0\"\ndescription = \"The AWS Powertools release version\"\n}\n\noutput \"deployed_powertools_sar_version\" {\nvalue = data.aws_serverlessapplicationrepository_application.sar_app.semantic_version\n}\n\n# Fetch Powertools Layer ARN from deployed SAR App\noutput \"aws_lambda_powertools_layer_arn\" {\nvalue = aws_serverlessapplicationrepository_cloudformation_stack.deploy_sar_stack.outputs.LayerVersionArn\n}\n</code></pre> Example: Least-privileged IAM permissions to deploy Layer <p>Credits to mwarkentin for providing the scoped down IAM permissions.</p> <p>The region and the account id for <code>CloudFormationTransform</code> and <code>GetCfnTemplate</code> are fixed.</p> template.yml <pre><code>AWSTemplateFormatVersion: \"2010-09-09\"\nResources:\nPowertoolsLayerIamRole:\nType: \"AWS::IAM::Role\"\nProperties:\nAssumeRolePolicyDocument:\nVersion: \"2012-10-17\"\nStatement:\n- Effect: \"Allow\"\nPrincipal:\nService:\n- \"cloudformation.amazonaws.com\"\nAction:\n- \"sts:AssumeRole\"\nPath: \"/\"\nPowertoolsLayerIamPolicy:\nType: \"AWS::IAM::Policy\"\nProperties:\nPolicyName: PowertoolsLambdaLayerPolicy\nPolicyDocument:\nVersion: \"2012-10-17\"\nStatement:\n- Sid: CloudFormationTransform\nEffect: Allow\nAction: cloudformation:CreateChangeSet\nResource:\n- arn:aws:cloudformation:us-east-1:aws:transform/Serverless-2016-10-31\n- Sid: GetCfnTemplate\nEffect: Allow\nAction:\n- serverlessrepo:CreateCloudFormationTemplate\n- serverlessrepo:GetCloudFormationTemplate\nResource:\n# this is arn of the powertools SAR app\n- arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer\n- Sid: S3AccessLayer\nEffect: Allow\nAction:\n- s3:GetObject\nResource:\n# AWS publishes to an external S3 bucket locked down to your account ID\n# The below example is us publishing lambda powertools\n# Bucket: awsserverlessrepo-changesets-plntc6bfnfj\n# Key: *****/arn:aws:serverlessrepo:eu-west-1:057560766410:applications-aws-lambda-powertools-python-layer-versions-1.10.2/aeeccf50-****-****-****-*********\n- arn:aws:s3:::awsserverlessrepo-changesets-*/*\n- Sid: GetLayerVersion\nEffect: Allow\nAction:\n- lambda:PublishLayerVersion\n- lambda:GetLayerVersion\nResource:\n- !Sub arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:layer:aws-lambda-powertools-python-layer*\nRoles:\n- Ref: \"PowertoolsLayerIamRole\"\n</code></pre> Click to expand and copy an AWS CLI command to list all versions available in SAR <p>You can fetch available versions via SAR ListApplicationVersions API:</p> AWS CLI example<pre><code>aws serverlessrepo list-application-versions \\\n--application-id arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer\n</code></pre>"},{"location":"#quick-getting-started","title":"Quick getting started","text":"Hello world example using SAM CLI<pre><code>sam init --app-template hello-world-powertools-python --name sam-app --package-type Zip --runtime python3.10 --no-tracing\n</code></pre>"},{"location":"#features","title":"Features","text":"<p>Core utilities such as Tracing, Logging, Metrics, and Event Handler will be available across all Powertools languages. Additional utilities are subjective to each language ecosystem and customer demand.</p> Utility Description Tracing Decorators and utilities to trace Lambda function handlers, and both synchronous and asynchronous functions Logger Structured logging made easier, and decorator to enrich structured logging with key Lambda context details Metrics Custom Metrics created asynchronously via CloudWatch Embedded Metric Format (EMF) Event handler: AppSync AppSync event handler for Lambda Direct Resolver and Amplify GraphQL Transformer function Event handler: API Gateway, ALB and Lambda Function URL Amazon API Gateway REST/HTTP API and ALB event handler for Lambda functions invoked using Proxy integration, and Lambda Function URL Middleware factory Decorator factory to create your own middleware to run logic before, and after each Lambda invocation Parameters Retrieve parameter values from AWS Systems Manager Parameter Store, AWS Secrets Manager, or Amazon DynamoDB, and cache them for a specific amount of time Batch processing Handle partial failures for AWS SQS batch processing Typing Static typing classes to speedup development in your IDE Validation JSON Schema validator for inbound events and responses Event source data classes Data classes describing the schema of common Lambda event triggers Parser Data parsing and deep validation using Pydantic Idempotency Idempotent Lambda handler Feature Flags A simple rule engine to evaluate when one or multiple features should be enabled depending on the input Streaming Streams datasets larger than the available memory as streaming data."},{"location":"#environment-variables","title":"Environment variables","text":"Info <p>Explicit parameters take precedence over environment variables</p> Environment variable Description Utility Default POWERTOOLS_SERVICE_NAME Sets service name used for tracing namespace, metrics dimension and structured logging All <code>\"service_undefined\"</code> POWERTOOLS_METRICS_NAMESPACE Sets namespace used for metrics Metrics <code>None</code> POWERTOOLS_TRACE_DISABLED Explicitly disables tracing Tracing <code>false</code> POWERTOOLS_TRACER_CAPTURE_RESPONSE Captures Lambda or method return as metadata. Tracing <code>true</code> POWERTOOLS_TRACER_CAPTURE_ERROR Captures Lambda or method exception as metadata. Tracing <code>true</code> POWERTOOLS_TRACE_MIDDLEWARES Creates sub-segment for each custom middleware Middleware factory <code>false</code> POWERTOOLS_LOGGER_LOG_EVENT Logs incoming event Logging <code>false</code> POWERTOOLS_LOGGER_SAMPLE_RATE Debug log sampling Logging <code>0</code> POWERTOOLS_LOG_DEDUPLICATION_DISABLED Disables log deduplication filter protection to use Pytest Live Log feature Logging <code>false</code> POWERTOOLS_PARAMETERS_MAX_AGE Adjust how long values are kept in cache (in seconds) Parameters <code>5</code> POWERTOOLS_PARAMETERS_SSM_DECRYPT Sets whether to decrypt or not values retrieved from AWS SSM Parameters Store Parameters <code>false</code> POWERTOOLS_DEV Increases verbosity across utilities Multiple; see POWERTOOLS_DEV effect below <code>false</code> LOG_LEVEL Sets logging level Logging <code>INFO</code>"},{"location":"#optimizing-for-non-production-environments","title":"Optimizing for non-production environments","text":"<p>Whether you're prototyping locally or against a non-production environment, you can use <code>POWERTOOLS_DEV</code> to increase verbosity across multiple utilities.</p> Info <p>We will emit a warning when <code>POWERTOOLS_DEV</code> is enabled to help you detect misuse in production environments.</p> <p>When <code>POWERTOOLS_DEV</code> is set to a truthy value (<code>1</code>, <code>true</code>), it'll have the following effects:</p> Utility Effect Logger Increase JSON indentation to 4. This will ease local debugging when running functions locally under emulators or direct calls while not affecting unit tests Event Handler Enable full traceback errors in the response, indent request/responses, and CORS in dev mode (<code>*</code>). Tracer Future-proof safety to disables tracing operations in non-Lambda environments. This already happens automatically in the Tracer utility."},{"location":"#debug-mode","title":"Debug mode","text":"<p>As a best practice for libraries, Powertools module logging statements are suppressed.</p> <p>When necessary, you can use <code>POWERTOOLS_DEBUG</code> environment variable to enable debugging. This will provide additional information on every internal operation.</p>"},{"location":"#how-to-support-aws-lambda-powertools-for-python","title":"How to support AWS Lambda Powertools for Python?","text":""},{"location":"#becoming-a-reference-customer","title":"Becoming a reference customer","text":"<p>Knowing which companies are using this library is important to help prioritize the project internally. If your company is using AWS Lambda Powertools for Python, you can request to have your name and logo added to the README file by raising a Support Lambda Powertools (become a reference) issue.</p> <p>The following companies, among others, use Powertools:</p> <ul> <li>CPQi (Exadel Financial Services)</li> <li>CloudZero</li> <li>CyberArk</li> <li>globaldatanet</li> <li>IMS</li> <li>Propellor.ai</li> <li>TopSport</li> <li>Trek10</li> </ul>"},{"location":"#sharing-your-work","title":"Sharing your work","text":"<p>Share what you did with Powertools \ud83d\udc9e\ud83d\udc9e. Blog post, workshops, presentation, sample apps and others. Check out what the community has already shared about Powertools here.</p>"},{"location":"#using-lambda-layer-or-sar","title":"Using Lambda Layer or SAR","text":"<p>This helps us understand who uses Powertools in a non-intrusive way, and helps us gain future investments for other Powertools languages. When using Layers, you can add Powertools as a dev dependency (or as part of your virtual env) to not impact the development process.</p>"},{"location":"#tenets","title":"Tenets","text":"<p>These are our core principles to guide our decision making.</p> <ul> <li>AWS Lambda only. We optimise for AWS Lambda function environments and supported runtimes only. Utilities might work with web frameworks and non-Lambda environments, though they are not officially supported.</li> <li>Eases the adoption of best practices. The main priority of the utilities is to facilitate best practices adoption, as defined in the AWS Well-Architected Serverless Lens; all other functionality is optional.</li> <li>Keep it lean. Additional dependencies are carefully considered for security and ease of maintenance, and prevent negatively impacting startup time.</li> <li>We strive for backwards compatibility. New features and changes should keep backwards compatibility. If a breaking change cannot be avoided, the deprecation and migration process should be clearly defined.</li> <li>We work backwards from the community. We aim to strike a balance of what would work best for 80% of customers. Emerging practices are considered and discussed via Requests for Comment (RFCs)</li> <li>Progressive. Utilities are designed to be incrementally adoptable for customers at any stage of their Serverless journey. They follow language idioms and their community\u2019s common practices.</li> </ul> <p>Testing new docs pr mechanism take 10</p> <p>Testing new docs pr mechanism take 11</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#maintenance","title":"Maintenance","text":"<ul> <li>ci: isolate git commands in a script for debuggability</li> <li>ci: test orphan branch cleanup</li> <li>ci: correct pr body message</li> <li>ci: changelog rebuild (#7)</li> <li>ci: create a temp branch with action workflow run id take 9</li> <li>ci: create a temp branch with action workflow run id take 8</li> <li>ci: create a temp branch with action workflow run id take 7</li> <li>ci: create a temp branch with action workflow run id take 6</li> <li>ci: create a temp branch with action workflow run id take 5</li> <li>ci: create a temp branch with action workflow run id take 4</li> <li>ci: create a temp branch with action workflow run id take 3</li> <li>ci: create a temp branch with action workflow run id take 2</li> <li>ci: create a temp branch with action workflow run id</li> <li>ci: add gh_token env var for gh cli</li> <li>ci: automate PR creation for changelog take 1</li> <li>ci: test release drafter (#4)</li> <li>ci: update todo for tomorrow</li> </ul>"},{"location":"changelog/#v010-2023-05-02","title":"v0.1.0 - 2023-05-02","text":""},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>ci: add permission for release drafting</li> <li>ci: pin sigstore correctly</li> <li>ci: pin 3rd party gh action</li> <li>ci: id-token typo</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>jmespath: fix MD037/no-space-in-emphasis</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>ci: include signing artifacts in release</li> </ul>"},{"location":"changelog/#maintenance_1","title":"Maintenance","text":"<ul> <li>split build, sign, and release</li> <li>split jobs and permissions</li> <li>sync base repo</li> <li>cleanup release targeting test only</li> <li>project init</li> </ul>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#overview","title":"Overview","text":"<p>This is our public roadmap that outlines the high level direction we are working towards, namely Themes. We update this document when our priorities change: security and stability is our top priority.</p> <p>See our latest list of activities \u00bb</p>"},{"location":"roadmap/#themes","title":"Themes","text":"<p>Operational Excellence is priority number 1.</p> <p>Themes are key activities maintainers are focusing on, besides bug reports. These are updated periodically and you can find the latest under Epics in our public board.</p>"},{"location":"roadmap/#increased-end-to-end-coverage","title":"Increased end-to-end coverage","text":"<p>We continue to work on increasing end-to-end coverage for all features. Our main challenge is testing contracts for Lambda Event Sources (Parser, Event Source Data Classes) due to the lack of an official JSON schema.</p> <p>Some Lambda Event Sources require clusters (e.g., MSK) leading to additional delays of up to 30m in the end-to-end feedback loop. We need a RFC to start discussing viable options, and whether we should publish JSON Schemas from identified contracts.</p>"},{"location":"roadmap/#observability-providers","title":"Observability providers","text":"<p>We want to extend Tracer, Metrics, and Logger to support any observability provider. We need a RFC to define a contract and to identify two most requested observability providers that we can work with as an initial step.</p>"},{"location":"roadmap/#lambda-layer-in-release-notes","title":"Lambda Layer in release notes","text":"<p>We want to publish a JSON with a map of region and Lambda Layer ARN as a GitHub Release Note asset.</p> <p>As of V2, we prioritize Lambda Layers being available before release notes are out. This is due to X86 and ARM64 compilation for smaller binaries and extra speed.</p> <p>This means we have room to include a JSON map for Lambda Layers and facilitate automation for customers wanting the latest version as soon as it's available.</p>"},{"location":"roadmap/#strict-typing","title":"Strict typing","text":"<p>We want to enable MyPy strict mode against the code base. We need a RFC to identify most critical areas to start, and do so gradually as to not impact new features and enhancements in parallel.</p> <p>This also means bringing <code>typing-extensions</code> as a runtime dependency to ensure complete coverage across all Python versions. Future wise, we might be able to experiment with MyPyC to compile less performing parts of the code base as a C-Extension.</p>"},{"location":"roadmap/#new-utilities","title":"New utilities","text":"<p>With V2 launched, we want to resume working on new utilities, specifically but not limited to the most commonly asked: (1) Sensitive Data Masking, (2) Integration/End-to-end Testing, and (3) Event Bridge.</p>"},{"location":"roadmap/#open-iteration-planning","title":"Open iteration planning","text":"<p>We want to experiment running a bi-weekly audio channel on Discord to help us prioritize backlog in real-time. Depending on attendance, we might switch to run an office hours instead.</p>"},{"location":"roadmap/#roadmap-status-definition","title":"Roadmap status definition","text":"<p> <pre><code>graph LR\n    Ideas --&gt; Backlog --&gt; Work[\"Working on it\"] --&gt; Merged[\"Coming soon\"] --&gt; Shipped</code></pre> Visual representation </p> <p>Within our public board, you'll see the following values in the <code>Status</code> column:</p> <ul> <li>Ideas. Incoming and existing feature requests that are not being actively considered yet. These will be reviewed when bandwidth permits.</li> <li>Backlog. Accepted feature requests or enhancements that we want to work on.</li> <li>Working on it. Features or enhancements we're currently either researching or implementing it.</li> <li>Coming soon. Any feature, enhancement, or bug fixes that have been merged and are coming in the next release.</li> <li>Shipped. Features or enhancements that are now available in the most recent release.</li> </ul> <p>Tasks or issues with empty <code>Status</code> will be categorized in upcoming review cycles.</p>"},{"location":"roadmap/#process","title":"Process","text":"<p> <pre><code>graph LR\n    PFR[Feature request] --&gt; Triage{Need RFC?}\n    Triage --&gt; |Complex/major change or new utility?| RFC[Ask or write RFC] --&gt; Approval{Approved?}\n    Triage --&gt; |Minor feature or enhancement?| NoRFC[No RFC required] --&gt; Approval\n    Approval --&gt; |Yes| Backlog\n    Approval --&gt; |No | Reject[\"Inform next steps\"]\n    Backlog --&gt; |Prioritized| Implementation\n    Backlog --&gt; |Defer| WelcomeContributions[\"help-wanted label\"]</code></pre> Visual representation </p> <p>Our end-to-end mechanism follows four major steps:</p> <ul> <li>Feature Request. Ideas start with a feature request to outline their use case at a high level. For complex use cases, maintainers might ask for/write a RFC.<ul> <li>Maintainers review requests based on project tenets, customers reaction (\ud83d\udc4d), and use cases.</li> </ul> </li> <li>Request-for-comments (RFC). Design proposals use our RFC issue template to describe its implementation, challenges, developer experience, dependencies, and alternative solutions.<ul> <li>This helps refine the initial idea with community feedback before a decision is made.</li> </ul> </li> <li>Decision. After carefully reviewing and discussing them, maintainers make a final decision on whether to start implementation, defer or reject it, and update everyone with the next steps.</li> <li>Implementation. For approved features, maintainers give priority to the original authors for implementation unless it is a sensitive task that is best handled by maintainers.</li> </ul> See Maintainers document to understand how we triage issues and pull requests, labels and governance."},{"location":"roadmap/#disclaimer","title":"Disclaimer","text":"<p>The AWS Lambda Powertools team values feedback and guidance from its community of users, although final decisions on inclusion into the project will be made by AWS.</p> <p>We determine the high-level direction for our open roadmap based on customer feedback and popularity (\ud83d\udc4d\ud83c\udffd and comments), security and operational impacts, and business value. Where features don\u2019t meet our goals and longer-term strategy, we will communicate that clearly and openly as quickly as possible with an explanation of why the decision was made.</p>"},{"location":"roadmap/#faqs","title":"FAQs","text":"<p>Q: Why did you build this?</p> <p>A: We know that our customers are making decisions and plans based on what we are developing, and we want to provide our customers the insights they need to plan.</p> <p>Q: Why are there no dates on your roadmap?</p> <p>A: Because job zero is security and operational stability, we can't provide specific target dates for features. The roadmap is subject to change at any time, and roadmap issues in this repository do not guarantee a feature will be launched as proposed.</p> <p>Q: How can I provide feedback or ask for more information?</p> <p>A: For existing features, you can directly comment on issues. For anything else, please open an issue.</p>"},{"location":"upgrade/","title":"Upgrade guide","text":""},{"location":"upgrade/#end-of-support-v1","title":"End of support v1","text":"<p>On March 31st, 2023, AWS Lambda Powertools for Python v1 reached end of support and will no longer receive updates or releases. If you are still using v1, we strongly recommend you to read our upgrade guide and update to the latest version.</p> <p>Given our commitment to all of our customers using AWS Lambda Powertools for Python, we will keep Pypi v1 releases and documentation 1.x versions to prevent any disruption.</p>"},{"location":"upgrade/#migrate-to-v2-from-v1","title":"Migrate to v2 from v1","text":"<p>We've made minimal breaking changes to make your transition to v2 as smooth as possible.</p>"},{"location":"upgrade/#quick-summary","title":"Quick summary","text":"Area Change Code change required IAM Permissions change required Batch Removed legacy SQS batch processor in favour of <code>BatchProcessor</code>. Yes - Environment variables Removed legacy <code>POWERTOOLS_EVENT_HANDLER_DEBUG</code> in favour of <code>POWERTOOLS_DEV</code>. - - Event Handler Updated headers response format due to multi-value headers and cookie support. Tests only - Event Source Data Classes Replaced DynamoDBStreamEvent <code>AttributeValue</code> with native Python types. Yes - Feature Flags / Parameters Updated AppConfig API calls due to <code>GetConfiguration</code> API deprecation. - Yes Idempotency Updated partition key to include fully qualified function/method names. - -"},{"location":"upgrade/#first-steps","title":"First Steps","text":"<p>All dependencies are optional now. Tracer, Validation, and Parser now require additional dependencies.</p> <p>Before you start, we suggest making a copy of your current working project or create a new branch with git.</p> <ol> <li>Upgrade Python to at least v3.7</li> <li>Ensure you have the latest version via Lambda Layer or PyPi.</li> <li>Review the following sections to confirm whether they affect your code</li> </ol>"},{"location":"upgrade/#legacy-sqs-batch-processor","title":"Legacy SQS Batch Processor","text":"<p>We removed the deprecated <code>PartialSQSProcessor</code> class and <code>sqs_batch_processor</code> decorator.</p> <p>You can migrate to <code>BatchProcessor</code> with the following changes:</p> <ol> <li>If you use <code>sqs_batch_decorator</code>, change to <code>batch_processor</code> decorator</li> <li>If you use <code>PartialSQSProcessor</code>, change to <code>BatchProcessor</code></li> <li>Enable <code>ReportBatchItemFailures</code> in your Lambda Event Source</li> <li>Change your Lambda Handler to return the new response format</li> </ol> [Before] Decorator[After] Decorator[Before] Context manager[After] Context manager <pre><code>from aws_lambda_powertools.utilities.batch import sqs_batch_processor\ndef record_handler(record):\n    return do_something_with(record[\"body\"])\n\n@sqs_batch_processor(record_handler=record_handler)\ndef lambda_handler(event, context):\n    return {\"statusCode\": 200}\n</code></pre> <pre><code>import json\n\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor\nprocessor = BatchProcessor(event_type=EventType.SQS)\ndef record_handler(record):\n    return do_something_with(record[\"body\"])\n\n@batch_processor(record_handler=record_handler, processor=processor)\ndef lambda_handler(event, context):\nreturn processor.response()\n</code></pre> <pre><code>from aws_lambda_powertools.utilities.batch import PartialSQSProcessor\nfrom botocore.config import Config\nconfig = Config(region_name=\"us-east-1\")\ndef record_handler(record):\n    return_value = do_something_with(record[\"body\"])\n    return return_value\n\n\ndef lambda_handler(event, context):\n    records = event[\"Records\"]\n\nprocessor = PartialSQSProcessor(config=config)\nwith processor(records, record_handler):\n        result = processor.process()\n\nreturn result\n</code></pre> <pre><code>from aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor\ndef record_handler(record):\n    return_value = do_something_with(record[\"body\"])\n    return return_value\n\ndef lambda_handler(event, context):\n    records = event[\"Records\"]\n\nprocessor = BatchProcessor(event_type=EventType.SQS)\nwith processor(records, record_handler):\n        result = processor.process()\n\nreturn processor.response()\n</code></pre>"},{"location":"upgrade/#event-handler-headers-response-format","title":"Event Handler headers response format","text":"<p>No code changes required</p> <p>This only applies if you're using <code>APIGatewayRestResolver</code> and asserting custom header values in your tests.</p> <p>Previously, custom headers were available under <code>headers</code> key in the Event Handler response.</p> V1 response headers<pre><code>{\n\"headers\": {\n\"Content-Type\": \"application/json\"\n    }\n}\n</code></pre> <p>In V2, we add all headers under <code>multiValueHeaders</code> key. This enables seamless support for multi-value headers and cookies in fine grained responses.</p> V2 response headers<pre><code>{\n\"multiValueHeaders\": {\n\"Content-Type\": \"application/json\"\n    }\n}\n</code></pre>"},{"location":"upgrade/#dynamodbstreamevent-in-event-source-data-classes","title":"DynamoDBStreamEvent in Event Source Data Classes","text":"<p>This also applies if you're using DynamoDB BatchProcessor.</p> <p>You will now receive native Python types when accessing DynamoDB records via <code>keys</code>, <code>new_image</code>, and <code>old_image</code> attributes in <code>DynamoDBStreamEvent</code>.</p> <p>Previously, you'd receive a <code>AttributeValue</code> instance and need to deserialize each item to the type you'd want for convenience, or to the type DynamoDB stored via <code>get_value</code> method.</p> <p>With this change, you can access data deserialized as stored in DynamoDB, and no longer need to recursively deserialize nested objects (Maps) if you had them.</p> Note <p>For a lossless conversion of DynamoDB <code>Number</code> type, we follow AWS Python SDK (boto3) approach and convert to <code>Decimal</code>.</p> <pre><code>from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import (\n    DynamoDBStreamEvent,\n    DynamoDBRecordEventName\n)\n\ndef send_to_sqs(data: Dict):\n    body = json.dumps(data)\n    ...\n\n@event_source(data_class=DynamoDBStreamEvent)\ndef lambda_handler(event: DynamoDBStreamEvent, context):\n    for record in event.records:\n\n        # BEFORE\nnew_image: Dict[str, AttributeValue] = record.dynamodb.new_image\nevent_type: AttributeValue = new_image[\"eventType\"].get_value\nif event_type == \"PENDING\":\n# deserialize attribute value into Python native type\n# NOTE: nested objects would need additional logic\ndata = {k: v.get_value for k, v in image.items()}\nsend_to_sqs(data)\n\n        # AFTER\nnew_image: Dict[str, Any] = record.dynamodb.new_image\nif new_image.get(\"eventType\") == \"PENDING\":\nsend_to_sqs(new_image)  # Here new_image is just a Python Dict type\n</code></pre>"},{"location":"upgrade/#feature-flags-and-appconfig-parameter-utility","title":"Feature Flags and AppConfig Parameter utility","text":"<p>No code changes required</p> <p>We replaced <code>GetConfiguration</code> API (now deprecated) with <code>GetLatestConfiguration</code> and <code>StartConfigurationSession</code>.</p> <p>As such, you must update your IAM Role permissions to allow the following IAM actions:</p> <ul> <li><code>appconfig:GetLatestConfiguration</code></li> <li><code>appconfig:StartConfigurationSession</code></li> </ul>"},{"location":"upgrade/#idempotency-partition-key-format","title":"Idempotency partition key format","text":"<p>No code changes required</p> <p>We replaced the DynamoDB partition key format to include fully qualified function/method names. This means that recent non-expired idempotent transactions will be ignored.</p> <p>Previously, we used the function/method name to generate the partition key value.</p> <p>e.g. <code>HelloWorldFunction.lambda_handler#99914b932bd37a50b983c5e7c90ae93b</code></p> <p></p> <p>In V2, we now distinguish between distinct classes or modules that may have the same function/method name.</p> <p>For example, an ABC or Protocol class may have multiple implementations of <code>process_payment</code> method and may have different results.</p> <p>e.g. <code>HelloWorldFunction.app.lambda_handler#99914b932bd37a50b983c5e7c90ae93b</code></p> <p></p>"},{"location":"we_made_this/","title":"We Made This (Community)","text":"<p>This space is dedicated to highlight our awesome community content featuring Powertools \ud83d\ude4f!</p> <p>Get your content featured here!</p>"},{"location":"we_made_this/#connect","title":"Connect","text":"<p>Join us on Discord to connect with the Powertools community \ud83d\udc4b. Ask questions, learn from each other, contribute, hang out with key contributors, and more!</p>"},{"location":"we_made_this/#blog-posts","title":"Blog posts","text":""},{"location":"we_made_this/#aws-lambda-cookbook-following-best-practices-with-lambda-powertools","title":"AWS Lambda Cookbook \u2014 Following best practices with Lambda Powertools","text":"<p>Author: Ran Isenberg </p> <p>A collection of articles explaining in detail how Lambda Powertools helps with a Serverless adoption strategy and its challenges.</p> <ul> <li> <p>Part 1 - Logging</p> </li> <li> <p>Part 2 - Observability: monitoring and tracing</p> </li> <li> <p>Part 3 - Business Domain Observability</p> </li> <li> <p>Part 4 - Environment Variables</p> </li> <li> <p>Part 5 - Input Validation</p> </li> <li> <p>Part 6 - Configuration &amp; Feature Flags</p> </li> </ul>"},{"location":"we_made_this/#making-all-your-apis-idempotent","title":"Making all your APIs idempotent","text":"<p>Author: Michael Walmsley </p> <p>This article dives into what idempotency means for APIs, their use cases, and how to implement them.</p> <ul> <li>blog.walmsles.io/making-all-your-apis-idempotent</li> </ul>"},{"location":"we_made_this/#deep-dive-on-lambda-powertools-idempotency-feature","title":"Deep dive on Lambda Powertools Idempotency feature","text":"<p>Author: Michael Walmsley </p> <p>This article describes how to best calculate your idempotency token, implementation details, and how to handle idempotency in RESTful APIs.</p> <ul> <li>blog.walmsles.io/aws-lambda-powertools-idempotency-a-deeper-dive</li> </ul>"},{"location":"we_made_this/#developing-aws-lambda-functions-with-aws-lambda-powertools","title":"Developing AWS Lambda functions with AWS Lambda Powertools","text":"<p>Author: Stephan Huber </p> <p>This article walks through how to add Powertools to an existing project, covers Tracer, Logger, Metrics, and JSON Schema Validation.</p> <ul> <li>globaldatanet.com/tech-blog/develop-lambda-functions-with-aws-lambda-powertools</li> </ul>"},{"location":"we_made_this/#speed-up-event-driven-projects","title":"Speed-up event-driven projects","text":"<p>Author: Joris Conijn </p> <p>This article walks through a sample AWS EventBridge cookiecutter template presented at the AWS Community Day Netherlands 2022.</p> <ul> <li>binx.io/2022/10/11/speedup-event-driven-projects/</li> <li>Slides</li> </ul>"},{"location":"we_made_this/#implementing-feature-flags-with-aws-appconfig-and-aws-lambda-powertools","title":"Implementing Feature Flags with AWS AppConfig and AWS Lambda Powertools","text":"<p>Author: Ran Isenberg </p> <p>This article walks through how CyberArk uses Powertools to implement Feature Flags with AWS AppConfig</p> <ul> <li>aws.amazon.com/blogs/mt/how-cyberark-implements-feature-flags-with-aws-appconfig</li> </ul>"},{"location":"we_made_this/#videos","title":"Videos","text":""},{"location":"we_made_this/#building-a-resilient-input-handling-with-parser","title":"Building a resilient input handling with Parser","text":"<p>Author: Ran Isenberg </p> <p>When building applications with AWS Lambda it is critical to verify the data structure and validate the input due to the multiple different sources that can trigger them. In this session Ran Isenberg (CyberArk) will present one of the interesting features of AWS Lambda Powertools for python: the parser.</p> <p>In this session you will learn how to increase code quality, extensibility and testability, boost you productivity and ship rock solid apps to production.</p>"},{"location":"we_made_this/#talk-dev-to-me-feature-flags-with-aws-lambda-powertools","title":"Talk DEV to me | Feature Flags with AWS Lambda Powertools","text":"<p>Author: Ran Isenberg </p> <p>A deep dive in the Feature Flags feature along with tips and tricks.</p>"},{"location":"we_made_this/#level-up-your-cicd-with-smart-aws-feature-flags","title":"Level Up Your CI/CD With Smart AWS Feature Flags","text":"<p>Author: Ran Isenberg </p> <p>Feature flags can improve your CI/CD process by enabling capabilities otherwise not possible, thus making them an enabler of DevOps and a crucial part of continuous integration. Partial rollouts, A/B testing, and the ability to quickly change a configuration without redeploying code are advantages you gain by using features flags.</p> <p>In this talk, you will learn the added value of using feature flags as part of your CI/CD process and how AWS Lambda Powertools can help with that.</p>"},{"location":"we_made_this/#workshops","title":"Workshops","text":""},{"location":"we_made_this/#introduction-to-lambda-powertools","title":"Introduction to Lambda Powertools","text":"<p>Author: Michael Walmsley </p> <p>This repo contains documentation for a live coding workshop for the AWS Programming and Tools Meetup in Melbourne. The workshop will start with the SAM Cli \"Hello World\" example API project.</p> <p>Throughout the labs we will introduce each of the AWS Lambda Powertools Core utilities to showcase how simple they are to use and adopt for all your projects, and how powerful they are at bringing you closer to the Well Architected Serverless Lens.</p> <ul> <li> github.com/walmsles/lambda-powertools-coding-workshop</li> </ul> <p>Walk-through video</p>"},{"location":"we_made_this/#sample-projects","title":"Sample projects","text":""},{"location":"we_made_this/#complete-lambda-handler-cookbook","title":"Complete Lambda Handler Cookbook","text":"<p>Author: Ran Isenberg </p> <p>This repository provides a working, deployable, open source based, AWS Lambda handler and AWS CDK Python code.</p> <p>This handler embodies Serverless best practices and has all the bells and whistles for a proper production ready handler. It uses many of the AWS Lambda Powertools utilities for Python.</p> <p> github.com/ran-isenberg/aws-lambda-handler-cookbook</p>"},{"location":"core/logger/","title":"Logger","text":"<p>Logger provides an opinionated logger with output structured as JSON.</p>"},{"location":"core/logger/#key-features","title":"Key features","text":"<ul> <li>Capture key fields from Lambda context, cold start and structures logging output as JSON</li> <li>Log Lambda event when instructed (disabled by default)</li> <li>Log sampling enables DEBUG log level for a percentage of requests (disabled by default)</li> <li>Append additional keys to structured log at any point in time</li> </ul>"},{"location":"core/logger/#getting-started","title":"Getting started","text":"Tip <p>All examples shared in this documentation are available within the project repository.</p> <p>Logger requires two settings:</p> Setting Description Environment variable Constructor parameter Logging level Sets how verbose Logger should be (INFO, by default) <code>LOG_LEVEL</code> <code>level</code> Service Sets service key that will be present across all log statements <code>POWERTOOLS_SERVICE_NAME</code> <code>service</code> AWS Serverless Application Model (SAM) example<pre><code>AWSTemplateFormatVersion: \"2010-09-09\"\nTransform: AWS::Serverless-2016-10-31\nDescription: AWS Lambda Powertools Tracer doc examples\n\nGlobals:\nFunction:\nTimeout: 5\nRuntime: python3.9\nTracing: Active\nEnvironment:\nVariables:\nPOWERTOOLS_SERVICE_NAME: payment\nLOG_LEVEL: INFO\nLayers:\n# Find the latest Layer version in the official documentation\n# https://awslabs.github.io/aws-lambda-powertools-python/latest/#lambda-layer\n- !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:31\n\nResources:\nLoggerLambdaHandlerExample:\nType: AWS::Serverless::Function\nProperties:\nCodeUri: ../src\nHandler: inject_lambda_context.handler\n</code></pre>"},{"location":"core/logger/#standard-structured-keys","title":"Standard structured keys","text":"<p>Your Logger will include the following keys to your structured logging:</p> Key Example Note level: <code>str</code> <code>INFO</code> Logging level location: <code>str</code> <code>collect.handler:1</code> Source code location where statement was executed message: <code>Any</code> <code>Collecting payment</code> Unserializable JSON values are casted as <code>str</code> timestamp: <code>str</code> <code>2021-05-03 10:20:19,650+0200</code> Timestamp with milliseconds, by default uses local timezone service: <code>str</code> <code>payment</code> Service name defined, by default <code>service_undefined</code> xray_trace_id: <code>str</code> <code>1-5759e988-bd862e3fe1be46a994272793</code> When tracing is enabled, it shows X-Ray Trace ID sampling_rate: <code>float</code> <code>0.1</code> When enabled, it shows sampling rate in percentage e.g. 10% exception_name: <code>str</code> <code>ValueError</code> When <code>logger.exception</code> is used and there is an exception exception: <code>str</code> <code>Traceback (most recent call last)..</code> When <code>logger.exception</code> is used and there is an exception"},{"location":"core/logger/#capturing-lambda-context-info","title":"Capturing Lambda context info","text":"<p>You can enrich your structured logs with key Lambda context information via <code>inject_lambda_context</code>.</p> inject_lambda_context.pyinject_lambda_context_output.json <pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\n\n@logger.inject_lambda_context\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    logger.info(\"Collecting payment\")\n\n    # You can log entire objects too\n    logger.info({\"operation\": \"collect_payment\", \"charge_id\": event[\"charge_id\"]})\n    return \"hello world\"\n</code></pre> <pre><code>[\n{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:9\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"cold_start\": true,\n\"function_name\": \"test\",\n\"function_memory_size\": 128,\n\"function_arn\": \"arn:aws:lambda:eu-west-1:12345678910:function:test\",\n\"function_request_id\": \"52fdfc07-2182-154f-163f-5f0f9a621d72\"\n},\n{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:12\",\n\"message\": {\n\"operation\": \"collect_payment\",\n\"charge_id\": \"ch_AZFlk2345C0\"\n},\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"cold_start\": true,\n\"function_name\": \"test\",\n\"function_memory_size\": 128,\n\"function_arn\": \"arn:aws:lambda:eu-west-1:12345678910:function:test\",\n\"function_request_id\": \"52fdfc07-2182-154f-163f-5f0f9a621d72\"\n}\n]\n</code></pre> <p>When used, this will include the following keys:</p> Key Example cold_start: <code>bool</code> <code>false</code> function_name <code>str</code> <code>example-powertools-HelloWorldFunction-1P1Z6B39FLU73</code> function_memory_size: <code>int</code> <code>128</code> function_arn: <code>str</code> <code>arn:aws:lambda:eu-west-1:012345678910:function:example-powertools-HelloWorldFunction-1P1Z6B39FLU73</code> function_request_id: <code>str</code> <code>899856cb-83d1-40d7-8611-9e78f15f32f4</code>"},{"location":"core/logger/#logging-incoming-event","title":"Logging incoming event","text":"<p>When debugging in non-production environments, you can instruct Logger to log the incoming event with <code>log_event</code> param or via <code>POWERTOOLS_LOGGER_LOG_EVENT</code> env var.</p> Warning <p>This is disabled by default to prevent sensitive info being logged</p> Logging incoming event<pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\n\n@logger.inject_lambda_context(log_event=True)\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    return \"hello world\"\n</code></pre>"},{"location":"core/logger/#setting-a-correlation-id","title":"Setting a Correlation ID","text":"<p>You can set a Correlation ID using <code>correlation_id_path</code> param by passing a JMESPath expression.</p> Tip <p>You can retrieve correlation IDs via <code>get_correlation_id</code> method</p> set_correlation_id.pyset_correlation_id_event.jsonset_correlation_id_output.json <pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\n\n@logger.inject_lambda_context(correlation_id_path=\"headers.my_request_id_header\")\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    logger.debug(f\"Correlation ID =&gt; {logger.get_correlation_id()}\")\n    logger.info(\"Collecting payment\")\n\n    return \"hello world\"\n</code></pre> <pre><code>{\n\"headers\": {\n\"my_request_id_header\": \"correlation_id_value\"\n}\n}\n</code></pre> <pre><code>{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:10\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"cold_start\": true,\n\"function_name\": \"test\",\n\"function_memory_size\": 128,\n\"function_arn\": \"arn:aws:lambda:eu-west-1:12345678910:function:test\",\n\"function_request_id\": \"52fdfc07-2182-154f-163f-5f0f9a621d72\",\n\"correlation_id\": \"correlation_id_value\"\n}\n</code></pre>"},{"location":"core/logger/#set_correlation_id-method","title":"set_correlation_id method","text":"<p>You can also use <code>set_correlation_id</code> method to inject it anywhere else in your code. Example below uses Event Source Data Classes utility to easily access events properties.</p> set_correlation_id_method.pyset_correlation_id_method.jsonset_correlation_id_method_output.json <pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\n\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    request = APIGatewayProxyEvent(event)\n\nlogger.set_correlation_id(request.request_context.request_id)\nlogger.info(\"Collecting payment\")\n\n    return \"hello world\"\n</code></pre> <pre><code>{\n\"requestContext\": {\n\"requestId\": \"correlation_id_value\"\n}\n}\n</code></pre> <pre><code>{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:13\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"correlation_id\": \"correlation_id_value\"\n}\n</code></pre>"},{"location":"core/logger/#known-correlation-ids","title":"Known correlation IDs","text":"<p>To ease routine tasks like extracting correlation ID from popular event sources, we provide built-in JMESPath expressions.</p> set_correlation_id_jmespath.pyset_correlation_id_jmespath.jsonset_correlation_id_jmespath_output.json <pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    logger.debug(f\"Correlation ID =&gt; {logger.get_correlation_id()}\")\n    logger.info(\"Collecting payment\")\n\n    return \"hello world\"\n</code></pre> <pre><code>{\n\"requestContext\": {\n\"requestId\": \"correlation_id_value\"\n}\n}\n</code></pre> <pre><code>{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:11\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"cold_start\": true,\n\"function_name\": \"test\",\n\"function_memory_size\": 128,\n\"function_arn\": \"arn:aws:lambda:eu-west-1:12345678910:function:test\",\n\"function_request_id\": \"52fdfc07-2182-154f-163f-5f0f9a621d72\",\n\"correlation_id\": \"correlation_id_value\"\n}\n</code></pre>"},{"location":"core/logger/#appending-additional-keys","title":"Appending additional keys","text":"Info: Custom keys are persisted across warm invocations <p>Always set additional keys as part of your handler to ensure they have the latest value, or explicitly clear them with <code>clear_state=True</code>.</p> <p>You can append additional keys using either mechanism:</p> <ul> <li>Persist new keys across all future log messages via <code>append_keys</code> method</li> <li>Add additional keys on a per log message basis as a keyword=value, or via <code>extra</code> parameter</li> </ul>"},{"location":"core/logger/#append_keys-method","title":"append_keys method","text":"Warning <p><code>append_keys</code> is not thread-safe, please see RFC.</p> <p>You can append your own keys to your existing Logger via <code>append_keys(**additional_key_values)</code> method.</p> append_keys.pyappend_keys_output.json <pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\n\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    order_id = event.get(\"order_id\")\n\n    # this will ensure order_id key always has the latest value before logging\n    # alternative, you can use `clear_state=True` parameter in @inject_lambda_context\nlogger.append_keys(order_id=order_id)\nlogger.info(\"Collecting payment\")\n\n    return \"hello world\"\n</code></pre> <pre><code>{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:11\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"order_id\": \"order_id_value\"\n}\n</code></pre> Tip: Logger will automatically reject any key with a None value <p>If you conditionally add keys depending on the payload, you can follow the example above.</p> <p>This example will add <code>order_id</code> if its value is not empty, and in subsequent invocations where <code>order_id</code> might not be present it'll remove it from the Logger.</p>"},{"location":"core/logger/#ephemeral-metadata","title":"ephemeral metadata","text":"<p>You can pass an arbitrary number of keyword arguments (kwargs) to all log level's methods, e.g. <code>logger.info, logger.warning</code>.</p> <p>Two common use cases for this feature is to enrich log statements with additional metadata, or only add certain keys conditionally.</p> <p>Any keyword argument added will not be persisted in subsequent messages.</p> append_keys_kwargs.pyappend_keys_kwargs_output.json <pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\n\ndef handler(event: dict, context: LambdaContext) -&gt; str:\nlogger.info(\"Collecting payment\", request_id=\"1123\")\nreturn \"hello world\"\n</code></pre> <pre><code>{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:8\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2022-11-26 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"request_id\": \"1123\"\n}\n</code></pre>"},{"location":"core/logger/#extra-parameter","title":"extra parameter","text":"<p>Extra parameter is available for all log levels' methods, as implemented in the standard logging library - e.g. <code>logger.info, logger.warning</code>.</p> <p>It accepts any dictionary, and all keyword arguments will be added as part of the root structure of the logs for that log statement.</p> <p>Any keyword argument added using <code>extra</code> will not be persisted in subsequent messages.</p> append_keys_extra.pyappend_keys_extra_output.json <pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\n\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    fields = {\"request_id\": \"1123\"}\nlogger.info(\"Collecting payment\", extra=fields)\nreturn \"hello world\"\n</code></pre> <pre><code>{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:9\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"request_id\": \"1123\"\n}\n</code></pre>"},{"location":"core/logger/#removing-additional-keys","title":"Removing additional keys","text":"<p>You can remove any additional key from Logger state using <code>remove_keys</code>.</p> remove_keys.pyremove_keys_output.json <pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\n\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    logger.append_keys(sample_key=\"value\")\n    logger.info(\"Collecting payment\")\n\nlogger.remove_keys([\"sample_key\"])\nlogger.info(\"Collecting payment without sample key\")\n\n    return \"hello world\"\n</code></pre> <pre><code>[\n{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:9\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"sample_key\": \"value\"\n},\n{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:12\",\n\"message\": \"Collecting payment without sample key\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\"\n}\n]\n</code></pre>"},{"location":"core/logger/#clearing-all-state","title":"Clearing all state","text":"<p>Logger is commonly initialized in the global scope. Due to Lambda Execution Context reuse, this means that custom keys can be persisted across invocations. If you want all custom keys to be deleted, you can use <code>clear_state=True</code> param in <code>inject_lambda_context</code> decorator.</p> Tip: When is this useful? <p>It is useful when you add multiple custom keys conditionally, instead of setting a default <code>None</code> value if not present. Any key with <code>None</code> value is automatically removed by Logger.</p> Danger: This can have unintended side effects if you use Layers <p>Lambda Layers code is imported before the Lambda handler.</p> <p>This means that <code>clear_state=True</code> will instruct Logger to remove any keys previously added before Lambda handler execution proceeds.</p> <p>You can either avoid running any code as part of Lambda Layers global scope, or override keys with their latest value as part of handler's execution.</p> clear_state.pyclear_state_event_one.jsonclear_state_event_two.json <pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\n\n@logger.inject_lambda_context(clear_state=True)\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    if event.get(\"special_key\"):\n# Should only be available in the first request log\n# as the second request doesn't contain `special_key`\n        logger.append_keys(debugging_key=\"value\")\n\n    logger.info(\"Collecting payment\")\n\n    return \"hello world\"\n</code></pre> <pre><code>{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:10\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"special_key\": \"debug_key\",\n\"cold_start\": true,\n\"function_name\": \"test\",\n\"function_memory_size\": 128,\n\"function_arn\": \"arn:aws:lambda:eu-west-1:12345678910:function:test\",\n\"function_request_id\": \"52fdfc07-2182-154f-163f-5f0f9a621d72\"\n}\n</code></pre> <pre><code>{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:10\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"cold_start\": false,\n\"function_name\": \"test\",\n\"function_memory_size\": 128,\n\"function_arn\": \"arn:aws:lambda:eu-west-1:12345678910:function:test\",\n\"function_request_id\": \"52fdfc07-2182-154f-163f-5f0f9a621d72\"\n}\n</code></pre>"},{"location":"core/logger/#logging-exceptions","title":"Logging exceptions","text":"<p>Use <code>logger.exception</code> method to log contextual information about exceptions. Logger will include <code>exception_name</code> and <code>exception</code> keys to aid troubleshooting and error enumeration.</p> Tip <p>You can use your preferred Log Analytics tool to enumerate and visualize exceptions across all your services using <code>exception_name</code> key.</p> logging_exceptions.pylogging_exceptions_output.json <pre><code>import requests\n\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nENDPOINT = \"http://httpbin.org/status/500\"\nlogger = Logger()\n\n\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    try:\n        ret = requests.get(ENDPOINT)\n        ret.raise_for_status()\n    except requests.HTTPError as e:\nlogger.exception(\"Received a HTTP 5xx error\")\nraise RuntimeError(\"Unable to fullfil request\") from e\n\n    return \"hello world\"\n</code></pre> <pre><code>{\n\"level\": \"ERROR\",\n\"location\": \"collect.handler:15\",\n\"message\": \"Received a HTTP 5xx error\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"exception_name\": \"RuntimeError\",\n\"exception\": \"Traceback (most recent call last):\\n  File \\\"&lt;input&gt;\\\", line 2, in &lt;module&gt; RuntimeError: Unable to fullfil request\"\n}\n</code></pre>"},{"location":"core/logger/#uncaught-exceptions","title":"Uncaught exceptions","text":"<p>CAUTION: some users reported a problem that causes this functionality not to work in the Lambda runtime. We recommend that you don't use this feature for the time being.</p> <p>Logger can optionally log uncaught exceptions by setting <code>log_uncaught_exceptions=True</code> at initialization.</p> <p>Logger will replace any exception hook previously registered via sys.excepthook.</p> What are uncaught exceptions? <p>It's any raised exception that wasn't handled by the <code>except</code> statement, leading a Python program to a non-successful exit.</p> <p>They are typically raised intentionally to signal a problem (<code>raise ValueError</code>), or a propagated exception from elsewhere in your code that you didn't handle it willingly or not (<code>KeyError</code>, <code>jsonDecoderError</code>, etc.).</p> logging_uncaught_exceptions.pylogging_uncaught_exceptions_output.json <pre><code>import requests\n\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nENDPOINT = \"http://httpbin.org/status/500\"\nlogger = Logger(log_uncaught_exceptions=True)\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    ret = requests.get(ENDPOINT)\n    # HTTP 4xx/5xx status will lead to requests.HTTPError\n    # Logger will log this exception before this program exits non-successfully\n    ret.raise_for_status()\n\n    return \"hello world\"\n</code></pre> <pre><code>{\n\"level\": \"ERROR\",\n\"location\": \"log_uncaught_exception_hook:756\",\n\"message\": \"500 Server Error: INTERNAL SERVER ERROR for url: http://httpbin.org/status/500\",\n\"timestamp\": \"2022-11-16 13:51:29,198+0100\",\n\"service\": \"payment\",\n\"exception\": \"Traceback (most recent call last):\\n  File \\\"&lt;input&gt;\\\", line 52, in &lt;module&gt;\\n    handler({}, {})\\n  File \\\"&lt;input&gt;\\\", line 17, in handler\\n    ret.raise_for_status()\\n  File \\\"&lt;input&gt;/lib/python3.9/site-packages/requests/models.py\\\", line 1021, in raise_for_status\\n    raise HTTPError(http_error_msg, response=self)\\nrequests.exceptions.HTTPError: 500 Server Error: INTERNAL SERVER ERROR for url: http://httpbin.org/status/500\",\n\"exception_name\": \"HTTPError\"\n}\n</code></pre>"},{"location":"core/logger/#date-formatting","title":"Date formatting","text":"<p>Logger uses Python's standard logging date format with the addition of timezone: <code>2021-05-03 11:47:12,494+0200</code>.</p> <p>You can easily change the date format using one of the following parameters:</p> <ul> <li><code>datefmt</code>. You can pass any strftime format codes. Use <code>%F</code> if you need milliseconds.</li> <li><code>use_rfc3339</code>. This flag will use a format compliant with both RFC3339 and ISO8601: <code>2022-10-27T16:27:43.738+02:00</code></li> </ul> Prefer using datetime string formats? <p>Use <code>use_datetime_directive</code> flag along with <code>datefmt</code> to instruct Logger to use <code>datetime</code> instead of <code>time.strftime</code>.</p> date_formatting.pydate_formatting_output.json <pre><code>from aws_lambda_powertools import Logger\n\ndate_format = \"%m/%d/%Y %I:%M:%S %p\"\n\nlogger = Logger(service=\"payment\", use_rfc3339=True)\nlogger.info(\"Collecting payment\")\n\nlogger_custom_format = Logger(service=\"loyalty\", datefmt=date_format)\nlogger_custom_format.info(\"Calculating points\")\n</code></pre> <pre><code>[\n{\n\"level\": \"INFO\",\n\"location\": \"&lt;module&gt;:6\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2022-10-28T14:35:03.210+02:00\",\n\"service\": \"payment\"\n},\n{\n\"level\": \"INFO\",\n\"location\": \"&lt;module&gt;:9\",\n\"message\": \"Calculating points\",\n\"timestamp\": \"10/28/2022 02:35:03 PM\",\n\"service\": \"loyalty\"\n}\n]\n</code></pre>"},{"location":"core/logger/#advanced","title":"Advanced","text":""},{"location":"core/logger/#built-in-correlation-id-expressions","title":"Built-in Correlation ID expressions","text":"<p>You can use any of the following built-in JMESPath expressions as part of inject_lambda_context decorator.</p> Note: Any object key named with <code>-</code> must be escaped <p>For example, <code>request.headers.\"x-amzn-trace-id\"</code>.</p> Name Expression Description API_GATEWAY_REST <code>\"requestContext.requestId\"</code> API Gateway REST API request ID API_GATEWAY_HTTP <code>\"requestContext.requestId\"</code> API Gateway HTTP API request ID APPSYNC_RESOLVER <code>'request.headers.\"x-amzn-trace-id\"'</code> AppSync X-Ray Trace ID APPLICATION_LOAD_BALANCER <code>'headers.\"x-amzn-trace-id\"'</code> ALB X-Ray Trace ID EVENT_BRIDGE <code>\"id\"</code> EventBridge Event ID"},{"location":"core/logger/#reusing-logger-across-your-code","title":"Reusing Logger across your code","text":"<p>Similar to Tracer, a new instance that uses the same <code>service</code> name - env var or explicit parameter - will reuse a previous Logger instance. Just like <code>logging.getLogger(\"logger_name\")</code> would in the standard library if called with the same logger name.</p> <p>Notice in the CloudWatch Logs output how <code>payment_id</code> appeared as expected when logging in <code>collect.py</code>.</p> logger_reuse.pylogger_reuse_payment.pylogger_reuse_output.json <pre><code>from logger_reuse_payment import inject_payment_id\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\n\n@logger.inject_lambda_context\ndef handler(event: dict, context: LambdaContext) -&gt; str:\ninject_payment_id(context=event)\nlogger.info(\"Collecting payment\")\nreturn \"hello world\"\n</code></pre> <pre><code>from aws_lambda_powertools import Logger\n\nlogger = Logger()\ndef inject_payment_id(context):\nlogger.append_keys(payment_id=context.get(\"payment_id\"))\n</code></pre> <pre><code>{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:12\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"cold_start\": true,\n\"function_name\": \"test\",\n\"function_memory_size\": 128,\n\"function_arn\": \"arn:aws:lambda:eu-west-1:12345678910:function:test\",\n\"function_request_id\": \"52fdfc07-2182-154f-163f-5f0f9a621d72\",\n\"payment_id\": \"968adaae-a211-47af-bda3-eed3ca2c0ed0\"\n}\n</code></pre> Note: About Child Loggers <p>Coming from standard library, you might be used to use <code>logging.getLogger(__name__)</code>. This will create a new instance of a Logger with a different name.</p> <p>In Powertools, you can have the same effect by using <code>child=True</code> parameter: <code>Logger(child=True)</code>. This creates a new Logger instance named after <code>service.&lt;module&gt;</code>. All state changes will be propagated bi-directionally between Child and Parent.</p> <p>For that reason, there could be side effects depending on the order the Child Logger is instantiated, because Child Loggers don't have a handler.</p> <p>For example, if you instantiated a Child Logger and immediately used <code>logger.append_keys/remove_keys/set_correlation_id</code> to update logging state, this might fail if the Parent Logger wasn't instantiated.</p> <p>In this scenario, you can either ensure any calls manipulating state are only called when a Parent Logger is instantiated (example above), or refrain from using <code>child=True</code> parameter altogether.</p>"},{"location":"core/logger/#sampling-debug-logs","title":"Sampling debug logs","text":"<p>Use sampling when you want to dynamically change your log level to DEBUG based on a percentage of your concurrent/cold start invocations.</p> <p>You can use values ranging from <code>0.0</code> to <code>1</code> (100%) when setting <code>POWERTOOLS_LOGGER_SAMPLE_RATE</code> env var, or <code>sample_rate</code> parameter in Logger.</p> Tip: When is this useful? <p>Let's imagine a sudden spike increase in concurrency triggered a transient issue downstream. When looking into the logs you might not have enough information, and while you can adjust log levels it might not happen again.</p> <p>This feature takes into account transient issues where additional debugging information can be useful.</p> <p>Sampling decision happens at the Logger initialization. This means sampling may happen significantly more or less than depending on your traffic patterns, for example a steady low number of invocations and thus few cold starts.</p> Note <p>Open a feature request if you want Logger to calculate sampling for every invocation</p> sampling_debug_logs.pysampling_debug_logs_output.json <pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n# Sample 10% of debug logs e.g. 0.1\n# NOTE: this evaluation will only occur at cold start\nlogger = Logger(service=\"payment\", sample_rate=0.1)\ndef handler(event: dict, context: LambdaContext):\nlogger.debug(\"Verifying whether order_id is present\")\nlogger.info(\"Collecting payment\")\n\n    return \"hello world\"\n</code></pre> <pre><code>[\n{\n\"level\": \"DEBUG\",\n\"location\": \"collect.handler:7\",\n\"message\": \"Verifying whether order_id is present\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"cold_start\": true,\n\"function_name\": \"test\",\n\"function_memory_size\": 128,\n\"function_arn\": \"arn:aws:lambda:eu-west-1:12345678910:function:test\",\n\"function_request_id\": \"52fdfc07-2182-154f-163f-5f0f9a621d72\",\n\"sampling_rate\": 0.1\n},\n{\n\"level\": \"INFO\",\n\"location\": \"collect.handler:7\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2021-05-03 11:47:12,494+0200\",\n\"service\": \"payment\",\n\"cold_start\": true,\n\"function_name\": \"test\",\n\"function_memory_size\": 128,\n\"function_arn\": \"arn:aws:lambda:eu-west-1:12345678910:function:test\",\n\"function_request_id\": \"52fdfc07-2182-154f-163f-5f0f9a621d72\",\n\"sampling_rate\": 0.1\n}\n]\n</code></pre>"},{"location":"core/logger/#lambdapowertoolsformatter","title":"LambdaPowertoolsFormatter","text":"<p>Logger propagates a few formatting configurations to the built-in <code>LambdaPowertoolsFormatter</code> logging formatter.</p> <p>If you prefer configuring it separately, or you'd want to bring this JSON Formatter to another application, these are the supported settings:</p> Parameter Description Default <code>json_serializer</code> function to serialize <code>obj</code> to a JSON formatted <code>str</code> <code>json.dumps</code> <code>json_deserializer</code> function to deserialize <code>str</code>, <code>bytes</code>, <code>bytearray</code> containing a JSON document to a Python obj <code>json.loads</code> <code>json_default</code> function to coerce unserializable values, when no custom serializer/deserializer is set <code>str</code> <code>datefmt</code> string directives (strftime) to format log timestamp <code>%Y-%m-%d %H:%M:%S,%F%z</code>, where <code>%F</code> is a custom ms directive <code>use_datetime_directive</code> format the <code>datefmt</code> timestamps using <code>datetime</code>, not <code>time</code>  (also supports the custom <code>%F</code> directive for milliseconds) <code>False</code> <code>utc</code> set logging timestamp to UTC <code>False</code> <code>log_record_order</code> set order of log keys when logging <code>[\"level\", \"location\", \"message\", \"timestamp\"]</code> <code>kwargs</code> key-value to be included in log messages <code>None</code> Info <p>When <code>POWERTOOLS_DEV</code> env var is present and set to <code>\"true\"</code>, Logger's default serializer (<code>json.dumps</code>) will pretty-print log messages for easier readability.</p> Pre-configuring Lambda Powertools Formatter<pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter\n# NOTE: Check docs for all available options\n# https://awslabs.github.io/aws-lambda-powertools-python/latest/core/logger/#lambdapowertoolsformatter\n\nformatter = LambdaPowertoolsFormatter(utc=True, log_record_order=[\"message\"])\nlogger = Logger(service=\"example\", logger_formatter=formatter)\n</code></pre>"},{"location":"core/logger/#observability-providers","title":"Observability providers","text":"<p>In this context, an observability provider is an AWS Lambda Partner offering a platform for logging, metrics, traces, etc.</p> <p>You can send logs to the observability provider of your choice via Lambda Extensions. In most cases, you shouldn't need any custom Logger configuration, and logs will be shipped async without any performance impact.</p>"},{"location":"core/logger/#built-in-formatters","title":"Built-in formatters","text":"<p>In rare circumstances where JSON logs are not parsed correctly by your provider, we offer built-in formatters to make this transition easier.</p> Provider Formatter Notes Datadog <code>DatadogLogFormatter</code> Modifies default timestamp to use RFC3339 by default <p>You can use import and use them as any other Logger formatter via <code>logger_formatter</code> parameter:</p> Using built-in Logger Formatters<pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.logging.formatters.datadog import DatadogLogFormatter\nlogger = Logger(service=\"payment\", logger_formatter=DatadogLogFormatter())\nlogger.info(\"hello\")\n</code></pre>"},{"location":"core/logger/#migrating-from-other-loggers","title":"Migrating from other Loggers","text":"<p>If you're migrating from other Loggers, there are few key points to be aware of: Service parameter, Inheriting Loggers, Overriding Log records, and Logging exceptions.</p>"},{"location":"core/logger/#the-service-parameter","title":"The service parameter","text":"<p>Service is what defines the Logger name, including what the Lambda function is responsible for, or part of (e.g payment service).</p> <p>For Logger, the <code>service</code> is the logging key customers can use to search log operations for one or more functions - For example, search for all errors, or messages like X, where service is payment.</p>"},{"location":"core/logger/#inheriting-loggers","title":"Inheriting Loggers","text":"Tip: Prefer Logger Reuse feature over inheritance unless strictly necessary, see caveats. <p>Python Logging hierarchy happens via the dot notation: <code>service</code>, <code>service.child</code>, <code>service.child_2</code></p> <p>For inheritance, Logger uses a <code>child=True</code> parameter along with <code>service</code> being the same value across Loggers.</p> <p>For child Loggers, we introspect the name of your module where <code>Logger(child=True, service=\"name\")</code> is called, and we name your Logger as {service}.{filename}.</p> Danger <p>A common issue when migrating from other Loggers is that <code>service</code> might be defined in the parent Logger (no child param), and not defined in the child Logger:</p> logging_inheritance_bad.pylogging_inheritance_module.py <pre><code>from logging_inheritance_module import inject_payment_id\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n# NOTE: explicit service name differs from Child\n# meaning we will have two Logger instances with different state\n# and an orphan child logger who won't be able to manipulate state\nlogger = Logger(service=\"payment\")\n@logger.inject_lambda_context\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    inject_payment_id(context=event)\n\n    return \"hello world\"\n</code></pre> <pre><code>from aws_lambda_powertools import Logger\nlogger = Logger(child=True)\n\n\ndef inject_payment_id(context):\n    logger.append_keys(payment_id=context.get(\"payment_id\"))\n</code></pre> <p>In this case, Logger will register a Logger named <code>payment</code>, and a Logger named <code>service_undefined</code>. The latter isn't inheriting from the parent, and will have no handler, resulting in no message being logged to standard output.</p> Tip <p>This can be fixed by either ensuring both has the <code>service</code> value as <code>payment</code>, or simply use the environment variable <code>POWERTOOLS_SERVICE_NAME</code> to ensure service value will be the same across all Loggers when not explicitly set.</p> <p>Do this instead:</p> logging_inheritance_good.pylogging_inheritance_module.py <pre><code>from logging_inheritance_module import inject_payment_id\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n# NOTE: explicit service name matches any new Logger\n# because we're using POWERTOOLS_SERVICE_NAME env var\n# but we could equally use the same string as service value, e.g. \"payment\"\nlogger = Logger()\n@logger.inject_lambda_context\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    inject_payment_id(context=event)\n\n    return \"hello world\"\n</code></pre> <pre><code>from aws_lambda_powertools import Logger\nlogger = Logger(child=True)\n\n\ndef inject_payment_id(context):\n    logger.append_keys(payment_id=context.get(\"payment_id\"))\n</code></pre>"},{"location":"core/logger/#overriding-log-records","title":"Overriding Log records","text":"<p>You might want to continue to use the same date formatting style, or override <code>location</code> to display the <code>package.function_name:line_number</code> as you previously had.</p> <p>Logger allows you to either change the format or suppress the following keys at initialization: <code>location</code>, <code>timestamp</code>, <code>xray_trace_id</code>.</p> overriding_log_records.pyoverriding_log_records_output.json <pre><code>from aws_lambda_powertools import Logger\n\nlocation_format = \"[%(funcName)s] %(module)s\"\n\n# override location and timestamp format\nlogger = Logger(service=\"payment\", location=location_format)\nlogger.info(\"Collecting payment\")\n\n# suppress keys with a None value\nlogger_two = Logger(service=\"loyalty\", location=None)\nlogger_two.info(\"Calculating points\")\n</code></pre> <pre><code>[\n{\n\"level\": \"INFO\",\n\"location\": \"[&lt;module&gt;] overriding_log_records\",\n\"message\": \"Collecting payment\",\n\"timestamp\": \"2022-10-28 14:40:43,801+0200\",\n\"service\": \"payment\"\n},\n{\n\"level\": \"INFO\",\n\"message\": \"Calculating points\",\n\"timestamp\": \"2022-10-28 14:40:43,801+0200\",\n\"service\": \"loyalty\"\n}\n]\n</code></pre>"},{"location":"core/logger/#reordering-log-keys-position","title":"Reordering log keys position","text":"<p>You can change the order of standard Logger keys or any keys that will be appended later at runtime via the <code>log_record_order</code> parameter.</p> reordering_log_keys.pyreordering_log_keys_output.json <pre><code>from aws_lambda_powertools import Logger\n\n# make message as the first key\nlogger = Logger(service=\"payment\", log_record_order=[\"message\"])\n# make request_id that will be added later as the first key\nlogger_two = Logger(service=\"order\", log_record_order=[\"request_id\"])\nlogger_two.append_keys(request_id=\"123\")\nlogger.info(\"hello world\")\nlogger_two.info(\"hello world\")\n</code></pre> <pre><code>[\n{\n\"message\": \"hello world\",\n\"level\": \"INFO\",\n\"location\": \"&lt;module&gt;:11\",\n\"timestamp\": \"2022-06-24 11:25:40,143+0200\",\n\"service\": \"payment\"\n},\n{\n\"request_id\": \"123\",\n\"level\": \"INFO\",\n\"location\": \"&lt;module&gt;:12\",\n\"timestamp\": \"2022-06-24 11:25:40,144+0200\",\n\"service\": \"order\",\n\"message\": \"hello universe\"\n}\n]\n</code></pre>"},{"location":"core/logger/#setting-timestamp-to-utc","title":"Setting timestamp to UTC","text":"<p>By default, this Logger and standard logging library emits records using local time timestamp. You can override this behavior via <code>utc</code> parameter:</p> setting_utc_timestamp.pysetting_utc_timestamp_output.json <pre><code>from aws_lambda_powertools import Logger\n\nlogger = Logger(service=\"payment\")\nlogger.info(\"Local time\")\n\nlogger_in_utc = Logger(service=\"order\", utc=True)\nlogger_in_utc.info(\"GMT time zone\")\n</code></pre> <pre><code>[\n{\n\"level\": \"INFO\",\n\"location\": \"&lt;module&gt;:4\",\n\"message\": \"Local time\",\n\"timestamp\": \"2022-06-24 11:39:49,421+0200\",\n\"service\": \"payment\"\n},\n{\n\"level\": \"INFO\",\n\"location\": \"&lt;module&gt;:7\",\n\"message\": \"GMT time zone\",\n\"timestamp\": \"2022-06-24 09:39:49,421+0100\",\n\"service\": \"order\"\n}\n]\n</code></pre>"},{"location":"core/logger/#custom-function-for-unserializable-values","title":"Custom function for unserializable values","text":"<p>By default, Logger uses <code>str</code> to handle values non-serializable by JSON. You can override this behavior via <code>json_default</code> parameter by passing a Callable:</p> unserializable_values.pyunserializable_values_output.json <pre><code>from datetime import date, datetime\n\nfrom aws_lambda_powertools import Logger\n\n\ndef custom_json_default(value: object) -&gt; str:\nif isinstance(value, (datetime, date)):\n        return value.isoformat()\n\n    return f\"&lt;non-serializable: {type(value).__name__}&gt;\"\n\n\nclass Unserializable:\n    pass\n\n\nlogger = Logger(service=\"payment\", json_default=custom_json_default)\nlogger.info({\"ingestion_time\": datetime.utcnow(), \"serialize_me\": Unserializable()})\n</code></pre> <pre><code>{\n\"level\": \"INFO\",\n\"location\": \"&lt;module&gt;:19\",\n\"message\": {\n\"ingestion_time\": \"2022-06-24T10:12:09.526365\",\n\"serialize_me\": \"&lt;non-serializable: Unserializable&gt;\"\n},\n\"timestamp\": \"2022-06-24 12:12:09,526+0200\",\n\"service\": \"payment\"\n}\n</code></pre>"},{"location":"core/logger/#bring-your-own-handler","title":"Bring your own handler","text":"<p>By default, Logger uses StreamHandler and logs to standard output. You can override this behavior via <code>logger_handler</code> parameter:</p> Configure Logger to output to a file<pre><code>import logging\nfrom pathlib import Path\n\nfrom aws_lambda_powertools import Logger\n\nlog_file = Path(\"/tmp/log.json\")\nlog_file_handler = logging.FileHandler(filename=log_file)\nlogger = Logger(service=\"payment\", logger_handler=log_file_handler)\nlogger.info(\"hello world\")\n</code></pre>"},{"location":"core/logger/#bring-your-own-formatter","title":"Bring your own formatter","text":"<p>By default, Logger uses LambdaPowertoolsFormatter that persists its custom structure between non-cold start invocations. There could be scenarios where the existing feature set isn't sufficient to your formatting needs.</p> Info <p>The most common use cases are remapping keys by bringing your existing schema, and redacting sensitive information you know upfront.</p> <p>For these, you can override the <code>serialize</code> method from LambdaPowertoolsFormatter.</p> bring_your_own_formatter.pybring_your_own_formatter_output.json <pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter\nclass CustomFormatter(LambdaPowertoolsFormatter):\ndef serialize(self, log: dict) -&gt; str:\n\"\"\"Serialize final structured log dict to JSON str\"\"\"\n        log[\"event\"] = log.pop(\"message\")  # rename message key to event\n        return self.json_serializer(log)  # use configured json serializer\n\n\nlogger = Logger(service=\"payment\", logger_formatter=CustomFormatter())\nlogger.info(\"hello\")\n</code></pre> <pre><code>{\n\"level\": \"INFO\",\n\"location\": \"&lt;module&gt;:16\",\n\"timestamp\": \"2021-12-30 13:41:53,413+0100\",\n\"service\": \"payment\",\n\"event\": \"hello\"\n}\n</code></pre> <p>The <code>log</code> argument is the final log record containing our standard keys, optionally Lambda context keys, and any custom key you might have added via append_keys or the extra parameter.</p> <p>For exceptional cases where you want to completely replace our formatter logic, you can subclass <code>BasePowertoolsFormatter</code>.</p> Warning <p>You will need to implement <code>append_keys</code>, <code>clear_state</code>, override <code>format</code>, and optionally <code>remove_keys</code> to keep the same feature set Powertools Logger provides. This also means keeping state of logging keys added.</p> bring_your_own_formatter_from_scratch.pybring_your_own_formatter_from_scratch_output.json <pre><code>import json\nimport logging\nfrom typing import Iterable, List, Optional\n\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.logging.formatter import BasePowertoolsFormatter\nclass CustomFormatter(BasePowertoolsFormatter):\ndef __init__(self, log_record_order: Optional[List[str]] = None, *args, **kwargs):\nself.log_record_order = log_record_order or [\"level\", \"location\", \"message\", \"timestamp\"]\nself.log_format = dict.fromkeys(self.log_record_order)\nsuper().__init__(*args, **kwargs)\n\ndef append_keys(self, **additional_keys):\n# also used by `inject_lambda_context` decorator\n        self.log_format.update(additional_keys)\n\ndef remove_keys(self, keys: Iterable[str]):\nfor key in keys:\n            self.log_format.pop(key, None)\n\ndef clear_state(self):\nself.log_format = dict.fromkeys(self.log_record_order)\n\ndef format(self, record: logging.LogRecord) -&gt; str:  # noqa: A003\n\"\"\"Format logging record as structured JSON str\"\"\"\n        return json.dumps(\n            {\n                \"event\": super().format(record),\n                \"timestamp\": self.formatTime(record),\n                \"my_default_key\": \"test\",\n                **self.log_format,\n            }\n        )\n\n\nlogger = Logger(service=\"payment\", logger_formatter=CustomFormatter())\n@logger.inject_lambda_context\ndef handler(event, context):\n    logger.info(\"Collecting payment\")\n</code></pre> <pre><code>{\n\"event\": \"Collecting payment\",\n\"timestamp\": \"2021-05-03 11:47:12,494\",\n\"my_default_key\": \"test\",\n\"cold_start\": true,\n\"function_name\": \"test\",\n\"function_memory_size\": 128,\n\"function_arn\": \"arn:aws:lambda:eu-west-1:12345678910:function:test\",\n\"function_request_id\": \"52fdfc07-2182-154f-163f-5f0f9a621d72\"\n}\n</code></pre>"},{"location":"core/logger/#bring-your-own-json-serializer","title":"Bring your own JSON serializer","text":"<p>By default, Logger uses <code>json.dumps</code> and <code>json.loads</code> as serializer and deserializer respectively. There could be scenarios where you are making use of alternative JSON libraries like orjson.</p> <p>As parameters don't always translate well between them, you can pass any callable that receives a <code>dict</code> and return a <code>str</code>:</p> Using Rust orjson library as serializer<pre><code>import functools\nimport orjson\nfrom aws_lambda_powertools import Logger\n\ncustom_serializer = orjson.dumps\ncustom_deserializer = orjson.loads\nlogger = Logger(service=\"payment\", json_serializer=custom_serializer, json_deserializer=custom_deserializer)\n\n# NOTE: when using parameters, you can pass a partial\ncustom_serializer_with_parameters = functools.partial(orjson.dumps, option=orjson.OPT_SERIALIZE_NUMPY)\nlogger_two = Logger(\n    service=\"payment\", json_serializer=custom_serializer_with_parameters, json_deserializer=custom_deserializer\n)\n</code></pre>"},{"location":"core/logger/#testing-your-code","title":"Testing your code","text":""},{"location":"core/logger/#inject-lambda-context","title":"Inject Lambda Context","text":"<p>When unit testing your code that makes use of <code>inject_lambda_context</code> decorator, you need to pass a dummy Lambda Context, or else Logger will fail.</p> <p>This is a Pytest sample that provides the minimum information necessary for Logger to succeed:</p> fake_lambda_context_for_logger.pyfake_lambda_context_for_logger_module.py <p>Note that dataclasses are available in Python 3.7+ only.</p> <pre><code>from dataclasses import dataclass\n\nimport fake_lambda_context_for_logger_module  # sample module for completeness\nimport pytest\n\n\n@pytest.fixture\ndef lambda_context():\n    @dataclass\n    class LambdaContext:\n        function_name: str = \"test\"\n        memory_limit_in_mb: int = 128\n        invoked_function_arn: str = \"arn:aws:lambda:eu-west-1:809313241:function:test\"\n        aws_request_id: str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\"\n\n    return LambdaContext()\n\n\ndef test_lambda_handler(lambda_context):\n    test_event = {\"test\": \"event\"}\n    fake_lambda_context_for_logger_module.handler(test_event, lambda_context)\n</code></pre> <pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\n\n@logger.inject_lambda_context\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    logger.info(\"Collecting payment\")\n\n    return \"hello world\"\n</code></pre> Tip <p>Check out the built-in Pytest caplog fixture to assert plain log messages</p>"},{"location":"core/logger/#pytest-live-log-feature","title":"Pytest live log feature","text":"<p>Pytest Live Log feature duplicates emitted log messages in order to style log statements according to their levels, for this to work use <code>POWERTOOLS_LOG_DEDUPLICATION_DISABLED</code> env var.</p> Disabling log deduplication to use Pytest live log<pre><code>POWERTOOLS_LOG_DEDUPLICATION_DISABLED=\"1\" pytest -o log_cli=1\n</code></pre> Warning <p>This feature should be used with care, as it explicitly disables our ability to filter propagated messages to the root logger (if configured).</p>"},{"location":"core/logger/#faq","title":"FAQ","text":""},{"location":"core/logger/#how-can-i-enable-boto3-and-botocore-library-logging","title":"How can I enable boto3 and botocore library logging?","text":"<p>You can enable the <code>botocore</code> and <code>boto3</code> logs by using the <code>set_stream_logger</code> method, this method will add a stream handler for the given name and level to the logging module. By default, this logs all boto3 messages to stdout.</p> Enabling AWS SDK logging<pre><code>from typing import Dict, List\n\nimport boto3\n\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nboto3.set_stream_logger()\nboto3.set_stream_logger(\"botocore\")\nlogger = Logger()\nclient = boto3.client(\"s3\")\n\n\ndef handler(event: Dict, context: LambdaContext) -&gt; List:\n    response = client.list_buckets()\n\n    return response.get(\"Buckets\", [])\n</code></pre>"},{"location":"core/logger/#how-can-i-enable-powertools-logging-for-imported-libraries","title":"How can I enable Powertools logging for imported libraries?","text":"<p>You can copy the Logger setup to all or sub-sets of registered external loggers. Use the <code>copy_config_to_registered_logger</code> method to do this.</p> Tip <p>To help differentiate between loggers, we include the standard logger <code>name</code> attribute for all loggers we copied configuration to.</p> <p>By default all registered loggers will be modified. You can change this behavior by providing <code>include</code> and <code>exclude</code> attributes. You can also provide optional <code>log_level</code> attribute external loggers will be configured with.</p> Cloning Logger config to all other registered standard loggers<pre><code>import logging\n\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.logging import utils\n\nlogger = Logger()\n\nexternal_logger = logging.getLogger()\n\nutils.copy_config_to_registered_loggers(source_logger=logger)\nexternal_logger.info(\"test message\")\n</code></pre>"},{"location":"core/logger/#how-can-i-add-standard-library-logging-attributes-to-a-log-record","title":"How can I add standard library logging attributes to a log record?","text":"<p>The Python standard library log records contains a large set of attributes, however only a few are included in Powertools Logger log record by default.</p> <p>You can include any of these logging attributes as key value arguments (<code>kwargs</code>) when instantiating <code>Logger</code> or <code>LambdaPowertoolsFormatter</code>.</p> <p>You can also add them later anywhere in your code with <code>append_keys</code>, or remove them with <code>remove_keys</code> methods.</p> append_and_remove_keys.pyappend_and_remove_keys_output.json <pre><code>from aws_lambda_powertools import Logger\n\nlogger = Logger(service=\"payment\", name=\"%(name)s\")\nlogger.info(\"Name should be equal service value\")\n\nadditional_log_attributes = {\"process\": \"%(process)d\", \"processName\": \"%(processName)s\"}\nlogger.append_keys(**additional_log_attributes)\nlogger.info(\"This will include process ID and name\")\nlogger.remove_keys([\"processName\"])\n# further messages will not include processName\n</code></pre> <pre><code>[\n{\n\"level\": \"INFO\",\n\"location\": \"&lt;module&gt;:16\",\n\"message\": \"Name should be equal service value\",\n\"name\": \"payment\",\n\"service\": \"payment\",\n\"timestamp\": \"2022-07-01 07:09:46,330+0000\"\n},\n{\n\"level\": \"INFO\",\n\"location\": \"&lt;module&gt;:23\",\n\"message\": \"This will include process ID and name\",\n\"name\": \"payment\",\n\"process\": \"9\",\n\"processName\": \"MainProcess\",\n\"service\": \"payment\",\n\"timestamp\": \"2022-07-01 07:09:46,330+0000\"\n}\n]\n</code></pre> <p>For log records originating from Powertools Logger, the <code>name</code> attribute will be the same as <code>service</code>, for log records coming from standard library logger, it will be the name of the logger (i.e. what was used as name argument to <code>logging.getLogger</code>).</p>"},{"location":"core/logger/#whats-the-difference-between-append_keys-and-extra","title":"What's the difference between <code>append_keys</code> and <code>extra</code>?","text":"<p>Keys added with <code>append_keys</code> will persist across multiple log messages while keys added via <code>extra</code> will only be available in a given log message operation.</p> <p>Here's an example where we persist <code>payment_id</code> not <code>request_id</code>. Note that <code>payment_id</code> remains in both log messages while <code>booking_id</code> is only available in the first message.</p> append_keys_vs_extra.pyappend_keys_vs_extra_output.json <pre><code>import os\n\nimport requests\n\nfrom aws_lambda_powertools import Logger\n\nENDPOINT = os.getenv(\"PAYMENT_API\", \"\")\nlogger = Logger(service=\"payment\")\n\n\nclass PaymentError(Exception):\n    ...\n\n\ndef handler(event, context):\nlogger.append_keys(payment_id=\"123456789\")\ncharge_id = event.get(\"charge_id\", \"\")\n\n    try:\n        ret = requests.post(url=f\"{ENDPOINT}/collect\", data={\"charge_id\": charge_id})\n        ret.raise_for_status()\n\nlogger.info(\"Charge collected successfully\", extra={\"charge_id\": charge_id})\nreturn ret.json()\n    except requests.HTTPError as e:\n        raise PaymentError(f\"Unable to collect payment for charge {charge_id}\") from e\n\n    logger.info(\"goodbye\")\n</code></pre> <pre><code>[\n{\n\"level\": \"INFO\",\n\"location\": \"&lt;module&gt;:22\",\n\"message\": \"Charge collected successfully\",\n\"timestamp\": \"2021-01-12 14:09:10,859\",\n\"service\": \"payment\",\n\"sampling_rate\": 0.0,\n\"payment_id\": \"123456789\",\n\"charge_id\": \"75edbad0-0857-4fc9-b547-6180e2f7959b\"\n},\n{\n\"level\": \"INFO\",\n\"location\": \"&lt;module&gt;:27\",\n\"message\": \"goodbye\",\n\"timestamp\": \"2021-01-12 14:09:10,860\",\n\"service\": \"payment\",\n\"sampling_rate\": 0.0,\n\"payment_id\": \"123456789\"\n}\n]\n</code></pre>"},{"location":"core/logger/#how-do-i-aggregate-and-search-powertools-logs-across-accounts","title":"How do I aggregate and search Powertools logs across accounts?","text":"<p>As of now, ElasticSearch (ELK) or 3rd party solutions are best suited to this task. Please refer to this discussion for more details</p>"},{"location":"core/metrics/","title":"Metrics","text":"<p>Metrics creates custom metrics asynchronously by logging metrics to standard output following Amazon CloudWatch Embedded Metric Format (EMF).</p> <p>These metrics can be visualized through Amazon CloudWatch Console.</p>"},{"location":"core/metrics/#key-features","title":"Key features","text":"<ul> <li>Aggregate up to 100 metrics using a single CloudWatch EMF object (large JSON blob)</li> <li>Validate against common metric definitions mistakes (metric unit, values, max dimensions, max metrics, etc)</li> <li>Metrics are created asynchronously by CloudWatch service, no custom stacks needed</li> <li>Context manager to create a one off metric with a different dimension</li> </ul>"},{"location":"core/metrics/#terminologies","title":"Terminologies","text":"<p>If you're new to Amazon CloudWatch, there are two terminologies you must be aware of before using this utility:</p> <ul> <li>Namespace. It's the highest level container that will group multiple metrics from multiple services for a given application, for example <code>ServerlessEcommerce</code>.</li> <li>Dimensions. Metrics metadata in key-value format. They help you slice and dice metrics visualization, for example <code>ColdStart</code> metric by Payment <code>service</code>.</li> <li>Metric. It's the name of the metric, for example: <code>SuccessfulBooking</code> or <code>UpdatedBooking</code>.</li> <li>Unit. It's a value representing the unit of measure for the corresponding metric, for example: <code>Count</code> or <code>Seconds</code>.</li> <li>Resolution. It's a value representing the storage resolution for the corresponding metric. Metrics can be either Standard or High resolution. Read more here.</li> </ul> Metric terminology, visually explained"},{"location":"core/metrics/#getting-started","title":"Getting started","text":"Tip <p>All examples shared in this documentation are available within the project repository.</p> <p>Metric has two global settings that will be used across all metrics emitted:</p> Setting Description Environment variable Constructor parameter Metric namespace Logical container where all metrics will be placed e.g. <code>ServerlessAirline</code> <code>POWERTOOLS_METRICS_NAMESPACE</code> <code>namespace</code> Service Optionally, sets service metric dimension across all metrics e.g. <code>payment</code> <code>POWERTOOLS_SERVICE_NAME</code> <code>service</code> Tip <p>Use your application or main service as the metric namespace to easily group all metrics.</p> AWS Serverless Application Model (SAM) example<pre><code>AWSTemplateFormatVersion: \"2010-09-09\"\nTransform: AWS::Serverless-2016-10-31\nDescription: AWS Lambda Powertools Metrics doc examples\n\nGlobals:\nFunction:\nTimeout: 5\nRuntime: python3.9\nTracing: Active\nEnvironment:\nVariables:\nPOWERTOOLS_SERVICE_NAME: booking\nPOWERTOOLS_METRICS_NAMESPACE: ServerlessAirline\nLayers:\n# Find the latest Layer version in the official documentation\n# https://awslabs.github.io/aws-lambda-powertools-python/latest/#lambda-layer\n- !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:31\n\nResources:\nCaptureLambdaHandlerExample:\nType: AWS::Serverless::Function\nProperties:\nCodeUri: ../src\nHandler: capture_lambda_handler.handler\n</code></pre> Note <p>For brevity, all code snippets in this page will rely on environment variables above being set.</p> <p>This ensures we instantiate <code>metrics = Metrics()</code> over <code>metrics = Metrics(service=\"booking\", namespace=\"ServerlessAirline\")</code>, etc.</p>"},{"location":"core/metrics/#creating-metrics","title":"Creating metrics","text":"<p>You can create metrics using <code>add_metric</code>, and you can create dimensions for all your aggregate metrics using <code>add_dimension</code> method.</p> Tip <p>You can initialize Metrics in any other module too. It'll keep track of your aggregate metrics in memory to optimize costs (one blob instead of multiples).</p> add_metrics.pyadd_dimension.py <pre><code>from aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nmetrics = Metrics()\n\n\n@metrics.log_metrics  # ensures metrics are flushed upon request completion/failure\ndef lambda_handler(event: dict, context: LambdaContext):\nmetrics.add_metric(name=\"SuccessfulBooking\", unit=MetricUnit.Count, value=1)\n</code></pre> <pre><code>import os\n\nfrom aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nSTAGE = os.getenv(\"STAGE\", \"dev\")\nmetrics = Metrics()\n\n\n@metrics.log_metrics  # ensures metrics are flushed upon request completion/failure\ndef lambda_handler(event: dict, context: LambdaContext):\nmetrics.add_dimension(name=\"environment\", value=STAGE)\nmetrics.add_metric(name=\"SuccessfulBooking\", unit=MetricUnit.Count, value=1)\n</code></pre> Tip: Autocomplete Metric Units <p><code>MetricUnit</code> enum facilitate finding a supported metric unit by CloudWatch. Alternatively, you can pass the value as a string if you already know them e.g. <code>unit=\"Count\"</code>.</p> Note: Metrics overflow <p>CloudWatch EMF supports a max of 100 metrics per batch. Metrics utility will flush all metrics when adding the 100th metric. Subsequent metrics (101th+) will be aggregated into a new EMF object, for your convenience.</p> Warning: Do not create metrics or dimensions outside the handler <p>Metrics or dimensions added in the global scope will only be added during cold start. Disregard if you that's the intended behavior.</p>"},{"location":"core/metrics/#adding-high-resolution-metrics","title":"Adding high-resolution metrics","text":"<p>You can create high-resolution metrics passing <code>resolution</code> parameter to <code>add_metric</code>.</p> When is it useful? <p>High-resolution metrics are data with a granularity of one second and are very useful in several situations such as telemetry, time series, real-time incident management, and others.</p> add_high_resolution_metrics.py <pre><code>from aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.metrics import MetricResolution, MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nmetrics = Metrics()\n\n\n@metrics.log_metrics  # ensures metrics are flushed upon request completion/failure\ndef lambda_handler(event: dict, context: LambdaContext):\nmetrics.add_metric(name=\"SuccessfulBooking\", unit=MetricUnit.Count, value=1, resolution=MetricResolution.High)\n</code></pre> Tip: Autocomplete Metric Resolutions <p><code>MetricResolution</code> enum facilitates finding a supported metric resolution by CloudWatch. Alternatively, you can pass the values 1 or 60 (must be one of them) as an integer e.g. <code>resolution=1</code>.</p>"},{"location":"core/metrics/#adding-multi-value-metrics","title":"Adding multi-value metrics","text":"<p>You can call <code>add_metric()</code> with the same metric name multiple times. The values will be grouped together in a list.</p> add_multi_value_metrics.pyadd_multi_value_metrics_output.json <pre><code>import os\n\nfrom aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nSTAGE = os.getenv(\"STAGE\", \"dev\")\nmetrics = Metrics()\n\n\n@metrics.log_metrics  # ensures metrics are flushed upon request completion/failure\ndef lambda_handler(event: dict, context: LambdaContext):\n    metrics.add_dimension(name=\"environment\", value=STAGE)\nmetrics.add_metric(name=\"TurbineReads\", unit=MetricUnit.Count, value=1)\nmetrics.add_metric(name=\"TurbineReads\", unit=MetricUnit.Count, value=8)\n</code></pre> <pre><code>{\n    \"_aws\": {\n        \"Timestamp\": 1656685750622,\n        \"CloudWatchMetrics\": [\n            {\n                \"Namespace\": \"ServerlessAirline\",\n                \"Dimensions\": [\n                    [\n                        \"environment\",\n                        \"service\"\n                    ]\n                ],\n                \"Metrics\": [\n                    {\n\"Name\": \"TurbineReads\",\n\"Unit\": \"Count\"\n                    }\n                ]\n            }\n        ]\n    },\n    \"environment\": \"dev\",\n    \"service\": \"booking\",\n\"TurbineReads\": [\n1.0,\n8.0\n]\n}\n</code></pre>"},{"location":"core/metrics/#adding-default-dimensions","title":"Adding default dimensions","text":"<p>You can use <code>set_default_dimensions</code> method, or <code>default_dimensions</code> parameter in <code>log_metrics</code> decorator, to persist dimensions across Lambda invocations.</p> <p>If you'd like to remove them at some point, you can use <code>clear_default_dimensions</code> method.</p> set_default_dimensions.pyset_default_dimensions_log_metrics.py <pre><code>import os\n\nfrom aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nSTAGE = os.getenv(\"STAGE\", \"dev\")\nmetrics = Metrics()\nmetrics.set_default_dimensions(environment=STAGE, another=\"one\")\n@metrics.log_metrics  # ensures metrics are flushed upon request completion/failure\ndef lambda_handler(event: dict, context: LambdaContext):\n    metrics.add_metric(name=\"TurbineReads\", unit=MetricUnit.Count, value=1)\n    metrics.add_metric(name=\"TurbineReads\", unit=MetricUnit.Count, value=8)\n</code></pre> <pre><code>import os\n\nfrom aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nSTAGE = os.getenv(\"STAGE\", \"dev\")\nmetrics = Metrics()\nDEFAULT_DIMENSIONS = {\"environment\": STAGE, \"another\": \"one\"}\n# ensures metrics are flushed upon request completion/failure\n@metrics.log_metrics(default_dimensions=DEFAULT_DIMENSIONS)\ndef lambda_handler(event: dict, context: LambdaContext):\n    metrics.add_metric(name=\"TurbineReads\", unit=MetricUnit.Count, value=1)\n    metrics.add_metric(name=\"TurbineReads\", unit=MetricUnit.Count, value=8)\n</code></pre>"},{"location":"core/metrics/#flushing-metrics","title":"Flushing metrics","text":"<p>As you finish adding all your metrics, you need to serialize and flush them to standard output. You can do that automatically with the <code>log_metrics</code> decorator.</p> <p>This decorator also validates, serializes, and flushes all your metrics. During metrics validation, if no metrics are provided then a warning will be logged, but no exception will be raised.</p> add_metrics.pylog_metrics_output.json <pre><code>from aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nmetrics = Metrics()\n\n\n@metrics.log_metrics  # ensures metrics are flushed upon request completion/failure\ndef lambda_handler(event: dict, context: LambdaContext):\n    metrics.add_metric(name=\"SuccessfulBooking\", unit=MetricUnit.Count, value=1)\n</code></pre> <pre><code>{\n\"_aws\": {\n\"Timestamp\": 1656686788803,\n\"CloudWatchMetrics\": [\n{\n\"Namespace\": \"ServerlessAirline\",\n\"Dimensions\": [\n[\n\"service\"\n]\n],\n\"Metrics\": [\n{\n\"Name\": \"SuccessfulBooking\",\n\"Unit\": \"Count\"\n}\n]\n}\n]\n},\n\"service\": \"booking\",\n\"SuccessfulBooking\": [\n1.0\n]\n}\n</code></pre> Tip: Metric validation <p>If metrics are provided, and any of the following criteria are not met, <code>SchemaValidationError</code> exception will be raised:</p> <ul> <li>Maximum of 29 user-defined dimensions</li> <li>Namespace is set, and no more than one</li> <li>Metric units must be supported by CloudWatch</li> </ul>"},{"location":"core/metrics/#raising-schemavalidationerror-on-empty-metrics","title":"Raising SchemaValidationError on empty metrics","text":"<p>If you want to ensure at least one metric is always emitted, you can pass <code>raise_on_empty_metrics</code> to the log_metrics decorator:</p> Raising SchemaValidationError exception if no metrics are added<pre><code>from aws_lambda_powertools.metrics import Metrics\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nmetrics = Metrics()\n\n\n@metrics.log_metrics(raise_on_empty_metrics=True)\ndef lambda_handler(event: dict, context: LambdaContext):\n    # no metrics being created will now raise SchemaValidationError\n    ...\n</code></pre> Suppressing warning messages on empty metrics <p>If you expect your function to execute without publishing metrics every time, you can suppress the warning with <code>warnings.filterwarnings(\"ignore\", \"No metrics to publish*\")</code>.</p>"},{"location":"core/metrics/#capturing-cold-start-metric","title":"Capturing cold start metric","text":"<p>You can optionally capture cold start metrics with <code>log_metrics</code> decorator via <code>capture_cold_start_metric</code> param.</p> capture_cold_start_metric.pycapture_cold_start_metric_output.json <pre><code>from aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nmetrics = Metrics()\n\n\n@metrics.log_metrics(capture_cold_start_metric=True)\ndef lambda_handler(event: dict, context: LambdaContext):\n    ...\n</code></pre> <pre><code>{\n\"_aws\": {\n\"Timestamp\": 1656687493142,\n\"CloudWatchMetrics\": [\n{\n\"Namespace\": \"ServerlessAirline\",\n\"Dimensions\": [\n[\n\"function_name\",\n\"service\"\n]\n],\n\"Metrics\": [\n{\n\"Name\": \"ColdStart\",\n\"Unit\": \"Count\"\n}\n]\n}\n]\n},\n\"function_name\": \"test\",\n\"service\": \"booking\",\n\"ColdStart\": [\n1.0\n]\n}\n</code></pre> <p>If it's a cold start invocation, this feature will:</p> <ul> <li>Create a separate EMF blob solely containing a metric named <code>ColdStart</code></li> <li>Add <code>function_name</code> and <code>service</code> dimensions</li> </ul> <p>This has the advantage of keeping cold start metric separate from your application metrics, where you might have unrelated dimensions.</p> Info <p>We do not emit 0 as a value for ColdStart metric for cost reasons. Let us know if you'd prefer a flag to override it.</p>"},{"location":"core/metrics/#advanced","title":"Advanced","text":""},{"location":"core/metrics/#adding-metadata","title":"Adding metadata","text":"<p>You can add high-cardinality data as part of your Metrics log with <code>add_metadata</code> method. This is useful when you want to search highly contextual information along with your metrics in your logs.</p> Info <p>This will not be available during metrics visualization - Use dimensions for this purpose</p> add_metadata.pyadd_metadata_output.json <pre><code>from uuid import uuid4\n\nfrom aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nmetrics = Metrics()\n\n\n@metrics.log_metrics\ndef lambda_handler(event: dict, context: LambdaContext):\n    metrics.add_metric(name=\"SuccessfulBooking\", unit=MetricUnit.Count, value=1)\n    metrics.add_metadata(key=\"booking_id\", value=f\"{uuid4()}\")\n</code></pre> <pre><code>{\n\"_aws\": {\n\"Timestamp\": 1656688250155,\n\"CloudWatchMetrics\": [\n{\n\"Namespace\": \"ServerlessAirline\",\n\"Dimensions\": [\n[\n\"service\"\n]\n],\n\"Metrics\": [\n{\n\"Name\": \"SuccessfulBooking\",\n\"Unit\": \"Count\"\n}\n]\n}\n]\n},\n\"service\": \"booking\",\n\"booking_id\": \"00347014-341d-4b8e-8421-a89d3d588ab3\",\n\"SuccessfulBooking\": [\n1.0\n]\n}\n</code></pre>"},{"location":"core/metrics/#single-metric-with-a-different-dimension","title":"Single metric with a different dimension","text":"<p>CloudWatch EMF uses the same dimensions across all your metrics. Use <code>single_metric</code> if you have a metric that should have different dimensions.</p> Info <p>Generally, this would be an edge case since you pay for unique metric. Keep the following formula in mind:</p> <p>unique metric = (metric_name + dimension_name + dimension_value)</p> single_metric.pysingle_metric_output.json <pre><code>import os\n\nfrom aws_lambda_powertools import single_metric\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nSTAGE = os.getenv(\"STAGE\", \"dev\")\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\nwith single_metric(name=\"MySingleMetric\", unit=MetricUnit.Count, value=1) as metric:\nmetric.add_dimension(name=\"environment\", value=STAGE)\n</code></pre> <pre><code>{\n\"_aws\": {\n\"Timestamp\": 1656689267834,\n\"CloudWatchMetrics\": [\n{\n\"Namespace\": \"ServerlessAirline\",\n\"Dimensions\": [\n[\n\"environment\",\n\"service\"\n]\n],\n\"Metrics\": [\n{\n\"Name\": \"MySingleMetric\",\n\"Unit\": \"Count\"\n}\n]\n}\n]\n},\n\"environment\": \"dev\",\n\"service\": \"booking\",\n\"MySingleMetric\": [\n1.0\n]\n}\n</code></pre> <p>By default it will skip all previously defined dimensions including default dimensions. Use <code>default_dimensions</code> keyword argument if you want to reuse default dimensions or specify custom dimensions from a dictionary.</p> single_metric_default_dimensions_inherit.pysingle_metric_default_dimensions.py <pre><code>import os\n\nfrom aws_lambda_powertools import single_metric\nfrom aws_lambda_powertools.metrics import Metrics, MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nSTAGE = os.getenv(\"STAGE\", \"dev\")\n\nmetrics = Metrics()\nmetrics.set_default_dimensions(environment=STAGE)\ndef lambda_handler(event: dict, context: LambdaContext):\n    with single_metric(\nname=\"RecordsCount\", unit=MetricUnit.Count, value=10, default_dimensions=metrics.default_dimensions\n) as metric:\n        metric.add_dimension(name=\"TableName\", value=\"Users\")\n</code></pre> <pre><code>import os\n\nfrom aws_lambda_powertools import single_metric\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nSTAGE = os.getenv(\"STAGE\", \"dev\")\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n    with single_metric(\nname=\"RecordsCount\", unit=MetricUnit.Count, value=10, default_dimensions={\"environment\": STAGE}\n) as metric:\n        metric.add_dimension(name=\"TableName\", value=\"Users\")\n</code></pre>"},{"location":"core/metrics/#flushing-metrics-manually","title":"Flushing metrics manually","text":"<p>If you are using the AWS Lambda Web Adapter project, or a middleware with custom metric logic, you can use <code>flush_metrics()</code>. This method will serialize, print metrics available to standard output, and clear in-memory metrics data.</p> Warning <p>This does not capture Cold Start metrics, and metric data validation still applies.</p> <p>Contrary to the <code>log_metrics</code> decorator, you are now also responsible to flush metrics in the event of an exception.</p> Manually flushing and clearing metrics from memory<pre><code>from aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nmetrics = Metrics()\n\n\ndef book_flight(flight_id: str, **kwargs): \n    # logic to book flight\n    ...\n    metrics.add_metric(name=\"SuccessfulBooking\", unit=MetricUnit.Count, value=1)\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        book_flight(flight_id=event.get(\"flight_id\", \"\"))\n    finally:\nmetrics.flush_metrics()\n</code></pre>"},{"location":"core/metrics/#metrics-isolation","title":"Metrics isolation","text":"<p>You can use <code>EphemeralMetrics</code> class when looking to isolate multiple instances of metrics with distinct namespaces and/or dimensions.</p> <p>This is a typical use case is for multi-tenant, or emitting same metrics for distinct applications.</p> EphemeralMetrics usage<pre><code>from aws_lambda_powertools.metrics import EphemeralMetrics, MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nmetrics = EphemeralMetrics()\n@metrics.log_metrics\ndef lambda_handler(event: dict, context: LambdaContext):\n    metrics.add_metric(name=\"SuccessfulBooking\", unit=MetricUnit.Count, value=1)\n</code></pre> <p>Differences between <code>EphemeralMetrics</code> and <code>Metrics</code></p> <p><code>EphemeralMetrics</code> has only two differences while keeping nearly the exact same set of features:</p> Feature Metrics EphemeralMetrics Share data across instances (metrics, dimensions, metadata, etc.) Yes - Default dimensions that persists across Lambda invocations (metric flush) Yes - <p>Why not changing the default <code>Metrics</code> behaviour to not share data across instances?</p> <p>This is an intentional design to prevent accidental data deduplication or data loss issues due to CloudWatch EMF metric dimension constraint.</p> <p>In CloudWatch, there are two metric ingestion mechanisms: EMF (async) and <code>PutMetricData</code> API (sync).</p> <p>The former creates metrics asynchronously via CloudWatch Logs, and the latter uses a synchronous and more flexible ingestion API.</p> <p>Key concept</p> <p>CloudWatch considers a metric unique by a combination of metric name, metric namespace, and zero or more metric dimensions.</p> <p>With EMF, metric dimensions are shared with any metrics you define. With <code>PutMetricData</code> API, you can set a list defining one or more metrics with distinct dimensions.</p> <p>This is a subtle yet important distinction. Imagine you had the following metrics to emit:</p> Metric Name Dimension Intent SuccessfulBooking service=\"booking\", tenant_id=\"sample\" Application metric IntegrationLatency service=\"booking\", function_name=\"sample\" Operational metric ColdStart service=\"booking\", function_name=\"sample\" Operational metric <p>The <code>tenant_id</code> dimension could vary leading to two common issues:</p> <ol> <li><code>ColdStart</code> metric will be created multiple times (N * number of unique tenant_id dimension value), despite the <code>function_name</code> being the same</li> <li><code>IntegrationLatency</code> metric will be also created multiple times due to <code>tenant_id</code> as well as <code>function_name</code> (may or not be intentional)</li> </ol> <p>These issues are exacerbated when you create (A) metric dimensions conditionally, (B) multiple metrics' instances throughout your code  instead of reusing them (globals). Subsequent metrics' instances will have (or lack) different metric dimensions resulting in different metrics and data points with the same name.</p> <p>Intentional design to address these scenarios</p> <p>On 1, when you enable capture_start_metric feature, we transparently create and flush an additional EMF JSON Blob that is independent from your application metrics. This prevents data pollution.</p> <p>On 2, you can use <code>EphemeralMetrics</code> to create an additional EMF JSON Blob from your application metric (<code>SuccessfulBooking</code>). This ensures that <code>IntegrationLatency</code> operational metric data points aren't tied to any dynamic dimension values like <code>tenant_id</code>.</p> <p>That is why <code>Metrics</code> shares data across instances by default, as that covers 80% of use cases and different personas using Powertools. This allows them to instantiate <code>Metrics</code> in multiple places throughout their code - be a separate file, a middleware, or an abstraction that sets default dimensions.</p>"},{"location":"core/metrics/#testing-your-code","title":"Testing your code","text":""},{"location":"core/metrics/#environment-variables","title":"Environment variables","text":"Tip <p>Ignore this section, if:</p> <ul> <li>You are explicitly setting namespace/default dimension via <code>namespace</code> and <code>service</code> parameters</li> <li>You're not instantiating <code>Metrics</code> in the global namespace</li> </ul> <p>For example, <code>Metrics(namespace=\"ServerlessAirline\", service=\"booking\")</code></p> <p>Make sure to set <code>POWERTOOLS_METRICS_NAMESPACE</code> and <code>POWERTOOLS_SERVICE_NAME</code> before running your tests to prevent failing on <code>SchemaValidation</code> exception. You can set it before you run tests or via pytest plugins like dotenv.</p> Injecting dummy Metric Namespace before running tests<pre><code>POWERTOOLS_SERVICE_NAME=\"booking\" POWERTOOLS_METRICS_NAMESPACE=\"ServerlessAirline\" python -m pytest\n</code></pre>"},{"location":"core/metrics/#clearing-metrics","title":"Clearing metrics","text":"<p><code>Metrics</code> keep metrics in memory across multiple instances. If you need to test this behavior, you can use the following Pytest fixture to ensure metrics are reset incl. cold start:</p> Clearing metrics between tests<pre><code>import pytest\n\nfrom aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.metrics import metrics as metrics_global\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef reset_metric_set():\n    # Clear out every metric data prior to every test\n    metrics = Metrics()\n    metrics.clear_metrics()\n    metrics_global.is_cold_start = True  # ensure each test has cold start\n    metrics.clear_default_dimensions()  # remove persisted default dimensions, if any\n    yield\n</code></pre>"},{"location":"core/metrics/#functional-testing","title":"Functional testing","text":"<p>You can read standard output and assert whether metrics have been flushed. Here's an example using <code>pytest</code> with <code>capsys</code> built-in fixture:</p> assert_single_emf_blob.pyadd_metrics.pyassert_multiple_emf_blobs.pyassert_multiple_emf_blobs_module.py <pre><code>import json\n\nimport add_metrics\n\n\ndef test_log_metrics(capsys):\nadd_metrics.lambda_handler({}, {})\n\nlog = capsys.readouterr().out.strip()  # remove any extra line\nmetrics_output = json.loads(log)  # deserialize JSON str\n# THEN we should have no exceptions\n    # and a valid EMF object should be flushed correctly\n    assert \"SuccessfulBooking\" in log  # basic string assertion in JSON str\n    assert \"SuccessfulBooking\" in metrics_output[\"_aws\"][\"CloudWatchMetrics\"][0][\"Metrics\"][0][\"Name\"]\n</code></pre> <pre><code>from aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nmetrics = Metrics()\n\n\n@metrics.log_metrics  # ensures metrics are flushed upon request completion/failure\ndef lambda_handler(event: dict, context: LambdaContext):\n    metrics.add_metric(name=\"SuccessfulBooking\", unit=MetricUnit.Count, value=1)\n</code></pre> <p>This will be needed when using <code>capture_cold_start_metric=True</code>, or when both <code>Metrics</code> and <code>single_metric</code> are used.</p> <pre><code>import json\nfrom dataclasses import dataclass\n\nimport assert_multiple_emf_blobs_module\nimport pytest\n\n\n@pytest.fixture\ndef lambda_context():\n    @dataclass\n    class LambdaContext:\n        function_name: str = \"test\"\n        memory_limit_in_mb: int = 128\n        invoked_function_arn: str = \"arn:aws:lambda:eu-west-1:809313241:function:test\"\n        aws_request_id: str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\"\n\n    return LambdaContext()\n\n\ndef capture_metrics_output_multiple_emf_objects(capsys):\nreturn [json.loads(line.strip()) for line in capsys.readouterr().out.split(\"\\n\") if line]\ndef test_log_metrics(capsys, lambda_context):\n    assert_multiple_emf_blobs_module.lambda_handler({}, lambda_context)\n\ncold_start_blob, custom_metrics_blob = capture_metrics_output_multiple_emf_objects(capsys)\n# Since `capture_cold_start_metric` is used\n    # we should have one JSON blob for cold start metric and one for the application\n    assert cold_start_blob[\"ColdStart\"] == [1.0]\n    assert cold_start_blob[\"function_name\"] == \"test\"\n\n    assert \"SuccessfulBooking\" in custom_metrics_blob\n</code></pre> <pre><code>from aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nmetrics = Metrics()\n\n\n@metrics.log_metrics(capture_cold_start_metric=True)\ndef lambda_handler(event: dict, context: LambdaContext):\n    metrics.add_metric(name=\"SuccessfulBooking\", unit=MetricUnit.Count, value=1)\n</code></pre> Tip <p>For more elaborate assertions and comparisons, check out our functional testing for Metrics utility.</p>"},{"location":"core/tracer/","title":"Tracer","text":"<p>Tracer is an opinionated thin wrapper for AWS X-Ray Python SDK.</p> <p></p>"},{"location":"core/tracer/#key-features","title":"Key features","text":"<ul> <li>Auto capture cold start as annotation, and responses or full exceptions as metadata</li> <li>Auto-disable when not running in AWS Lambda environment</li> <li>Support tracing async methods, generators, and context managers</li> <li>Auto patch supported modules by AWS X-Ray</li> </ul>"},{"location":"core/tracer/#getting-started","title":"Getting started","text":"Tip <p>All examples shared in this documentation are available within the project repository.</p> <p>Tracer relies on AWS X-Ray SDK over OpenTelememetry Distro (ADOT) for optimal cold start (lower latency).</p>"},{"location":"core/tracer/#install","title":"Install","text":"<p>This is not necessary if you're installing Powertools via Lambda Layer/SAR</p> <p>Add <code>aws-lambda-powertools[tracer]</code> as a dependency in your preferred tool: e.g., requirements.txt, pyproject.toml. This will ensure you have the required dependencies before using Tracer.</p>"},{"location":"core/tracer/#permissions","title":"Permissions","text":"<p>Before your use this utility, your AWS Lambda function must have permissions to send traces to AWS X-Ray.</p> AWS Serverless Application Model (SAM) example<pre><code>AWSTemplateFormatVersion: \"2010-09-09\"\nTransform: AWS::Serverless-2016-10-31\nDescription: AWS Lambda Powertools Tracer doc examples\n\nGlobals:\nFunction:\nTimeout: 5\nRuntime: python3.9\nTracing: Active\nEnvironment:\nVariables:\nPOWERTOOLS_SERVICE_NAME: payment\nLayers:\n# Find the latest Layer version in the official documentation\n# https://awslabs.github.io/aws-lambda-powertools-python/latest/#lambda-layer\n- !Sub arn:aws:lambda:${AWS::Region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:31\n\nResources:\nCaptureLambdaHandlerExample:\nType: AWS::Serverless::Function\nProperties:\nCodeUri: ../src\nHandler: capture_lambda_handler.handler\n</code></pre>"},{"location":"core/tracer/#lambda-handler","title":"Lambda handler","text":"<p>You can quickly start by initializing <code>Tracer</code> and use <code>capture_lambda_handler</code> decorator for your Lambda handler.</p> Tracing Lambda handler with capture_lambda_handler<pre><code>from aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()  # Sets service via POWERTOOLS_SERVICE_NAME env var\n# OR tracer = Tracer(service=\"example\")\n\n\ndef collect_payment(charge_id: str) -&gt; str:\n    return f\"dummy payment collected for charge: {charge_id}\"\n\n\n@tracer.capture_lambda_handler\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    charge_id = event.get(\"charge_id\", \"\")\n    return collect_payment(charge_id=charge_id)\n</code></pre> <p><code>capture_lambda_handler</code> performs these additional tasks to ease operations:</p> <ul> <li>Creates a <code>ColdStart</code> annotation to easily filter traces that have had an initialization overhead</li> <li>Creates a <code>Service</code> annotation if <code>service</code> parameter or <code>POWERTOOLS_SERVICE_NAME</code> is set</li> <li>Captures any response, or full exceptions generated by the handler, and include as tracing metadata</li> </ul>"},{"location":"core/tracer/#annotations-metadata","title":"Annotations &amp; Metadata","text":"<p>Annotations are key-values associated with traces and indexed by AWS X-Ray. You can use them to filter traces and to create Trace Groups to slice and dice your transactions.</p> Adding annotations with put_annotation method<pre><code>from aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\n\n\ndef collect_payment(charge_id: str) -&gt; str:\ntracer.put_annotation(key=\"PaymentId\", value=charge_id)\nreturn f\"dummy payment collected for charge: {charge_id}\"\n\n\n@tracer.capture_lambda_handler\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    charge_id = event.get(\"charge_id\", \"\")\n    return collect_payment(charge_id=charge_id)\n</code></pre> <p>Metadata are key-values also associated with traces but not indexed by AWS X-Ray. You can use them to add additional context for an operation using any native object.</p> Adding arbitrary metadata with put_metadata method<pre><code>from aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\n\n\ndef collect_payment(charge_id: str) -&gt; str:\n    return f\"dummy payment collected for charge: {charge_id}\"\n\n\n@tracer.capture_lambda_handler\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    payment_context = {\n        \"charge_id\": event.get(\"charge_id\", \"\"),\n        \"merchant_id\": event.get(\"merchant_id\", \"\"),\n        \"request_id\": context.aws_request_id,\n    }\n    payment_context[\"receipt_id\"] = collect_payment(charge_id=payment_context[\"charge_id\"])\ntracer.put_metadata(key=\"payment_response\", value=payment_context)\nreturn payment_context[\"receipt_id\"]\n</code></pre>"},{"location":"core/tracer/#synchronous-functions","title":"Synchronous functions","text":"<p>You can trace synchronous functions using the <code>capture_method</code> decorator.</p> Tracing an arbitrary function with capture_method<pre><code>from aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\n\n\n@tracer.capture_method\ndef collect_payment(charge_id: str) -&gt; str:\n    tracer.put_annotation(key=\"PaymentId\", value=charge_id)\n    return f\"dummy payment collected for charge: {charge_id}\"\n\n\n@tracer.capture_lambda_handler\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    charge_id = event.get(\"charge_id\", \"\")\n    return collect_payment(charge_id=charge_id)\n</code></pre> Note: Function responses are auto-captured and stored as JSON, by default. <p>Use capture_response parameter to override this behaviour.</p> <p>The serialization is performed by aws-xray-sdk via <code>jsonpickle</code> module. This can cause side effects for file-like objects like boto S3 <code>StreamingBody</code>, where its response will be read only once during serialization.</p>"},{"location":"core/tracer/#asynchronous-and-generator-functions","title":"Asynchronous and generator functions","text":"Warning <p>We do not support asynchronous Lambda handler</p> <p>You can trace asynchronous functions and generator functions (including context managers) using <code>capture_method</code>.</p> capture_method_async.pycapture_method_context_manager.pycapture_method_generators.py <pre><code>import asyncio\n\nfrom aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\n\n\n@tracer.capture_method\nasync def collect_payment(charge_id: str) -&gt; str:\n    tracer.put_annotation(key=\"PaymentId\", value=charge_id)\n    await asyncio.sleep(0.5)\n    return f\"dummy payment collected for charge: {charge_id}\"\n\n\n@tracer.capture_lambda_handler\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    charge_id = event.get(\"charge_id\", \"\")\n    return asyncio.run(collect_payment(charge_id=charge_id))\n</code></pre> <pre><code>import contextlib\nfrom collections.abc import Generator\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\n\n\n@contextlib.contextmanager\n@tracer.capture_method\ndef collect_payment(charge_id: str) -&gt; Generator[str, None, None]:\ntry:\n        yield f\"dummy payment collected for charge: {charge_id}\"\n    finally:\n        tracer.put_annotation(key=\"PaymentId\", value=charge_id)\n\n\n@tracer.capture_lambda_handler\n@logger.inject_lambda_context\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    charge_id = event.get(\"charge_id\", \"\")\n    with collect_payment(charge_id=charge_id) as receipt_id:\n        logger.info(f\"Processing payment collection for charge {charge_id} with receipt {receipt_id}\")\n\n    return receipt_id\n</code></pre> <pre><code>from collections.abc import Generator\n\nfrom aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\n\n\n@tracer.capture_method\ndef collect_payment(charge_id: str) -&gt; Generator[str, None, None]:\n    yield f\"dummy payment collected for charge: {charge_id}\"\n\n\n@tracer.capture_lambda_handler\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    charge_id = event.get(\"charge_id\", \"\")\n    return next(collect_payment(charge_id=charge_id))\n</code></pre>"},{"location":"core/tracer/#advanced","title":"Advanced","text":""},{"location":"core/tracer/#patching-modules","title":"Patching modules","text":"<p>Tracer automatically patches all supported libraries by X-Ray during initialization, by default. Underneath, AWS X-Ray SDK checks whether a supported library has been imported before patching.</p> <p>If you're looking to shave a few microseconds, or milliseconds depending on your function memory configuration, you can patch specific modules using <code>patch_modules</code> param:</p> Example of explicitly patching requests only<pre><code>import requests\n\nfrom aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nMODULES = [\"requests\"]\n\ntracer = Tracer(patch_modules=MODULES)\n@tracer.capture_lambda_handler\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    ret = requests.get(\"https://httpbin.org/get\")\n    ret.raise_for_status()\n\n    return ret.json()\n</code></pre>"},{"location":"core/tracer/#disabling-response-auto-capture","title":"Disabling response auto-capture","text":"<p>Use <code>capture_response=False</code> parameter in both <code>capture_lambda_handler</code> and <code>capture_method</code> decorators to instruct Tracer not to serialize function responses as metadata.</p> Info: This is useful in three common scenarios <ol> <li>You might return sensitive information you don't want it to be added to your traces</li> <li>You might manipulate streaming objects that can be read only once; this prevents subsequent calls from being empty</li> <li>You might return more than 64K of data e.g., <code>message too long</code> error</li> </ol> disable_capture_response.pydisable_capture_response_streaming_body.py <pre><code>from aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method(capture_response=False)\ndef collect_payment(charge_id: str) -&gt; str:\n    tracer.put_annotation(key=\"PaymentId\", value=charge_id)\n    logger.debug(\"Returning sensitive information....\")\n    return f\"dummy payment collected for charge: {charge_id}\"\n\n\n@tracer.capture_lambda_handler(capture_response=False)\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    charge_id = event.get(\"charge_id\", \"\")\n    return collect_payment(charge_id=charge_id)\n</code></pre> <pre><code>import os\n\nimport boto3\nfrom botocore.response import StreamingBody\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nBUCKET = os.getenv(\"BUCKET_NAME\", \"\")\nREPORT_KEY = os.getenv(\"REPORT_KEY\", \"\")\n\ntracer = Tracer()\nlogger = Logger()\n\nsession = boto3.Session()\ns3 = session.client(\"s3\")\n\n\n@tracer.capture_method(capture_response=False)\ndef fetch_payment_report(payment_id: str) -&gt; StreamingBody:\n    ret = s3.get_object(Bucket=BUCKET, Key=f\"{REPORT_KEY}/{payment_id}\")\n    logger.debug(\"Returning streaming body from S3 object....\")\n    return ret[\"body\"]\n\n\n@tracer.capture_lambda_handler(capture_response=False)\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    payment_id = event.get(\"payment_id\", \"\")\n    report = fetch_payment_report(payment_id=payment_id)\n    return report.read().decode()\n</code></pre>"},{"location":"core/tracer/#disabling-exception-auto-capture","title":"Disabling exception auto-capture","text":"<p>Use <code>capture_error=False</code> parameter in both <code>capture_lambda_handler</code> and <code>capture_method</code> decorators to instruct Tracer not to serialize exceptions as metadata.</p> Info <p>Useful when returning sensitive information in exceptions/stack traces you don't control</p> Disabling exception auto-capture for tracing metadata<pre><code>import os\n\nimport requests\n\nfrom aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nENDPOINT = os.getenv(\"PAYMENT_API\", \"\")\n\n\nclass PaymentError(Exception):\n    ...\n\n\n@tracer.capture_method(capture_error=False)\ndef collect_payment(charge_id: str) -&gt; dict:\n    try:\n        ret = requests.post(url=f\"{ENDPOINT}/collect\", data={\"charge_id\": charge_id})\n        ret.raise_for_status()\n        return ret.json()\n    except requests.HTTPError as e:\n        raise PaymentError(f\"Unable to collect payment for charge {charge_id}\") from e\n\n\n@tracer.capture_lambda_handler(capture_error=False)\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    charge_id = event.get(\"charge_id\", \"\")\n    ret = collect_payment(charge_id=charge_id)\n\n    return ret.get(\"receipt_id\", \"\")\n</code></pre>"},{"location":"core/tracer/#ignoring-certain-http-endpoints","title":"Ignoring certain HTTP endpoints","text":"<p>You might have endpoints you don't want requests to be traced, perhaps due to the volume of calls or sensitive URLs.</p> <p>You can use <code>ignore_endpoint</code> method with the hostname and/or URLs you'd like it to be ignored - globs (<code>*</code>) are allowed.</p> Ignoring certain HTTP endpoints from being traced<pre><code>import os\n\nimport requests\n\nfrom aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nENDPOINT = os.getenv(\"PAYMENT_API\", \"\")\nIGNORE_URLS = [\"/collect\", \"/refund\"]\n\ntracer = Tracer()\ntracer.ignore_endpoint(hostname=ENDPOINT, urls=IGNORE_URLS)\ntracer.ignore_endpoint(hostname=f\"*.{ENDPOINT}\", urls=IGNORE_URLS)  # `&lt;stage&gt;.ENDPOINT`\nclass PaymentError(Exception):\n    ...\n\n\n@tracer.capture_method(capture_error=False)\ndef collect_payment(charge_id: str) -&gt; dict:\n    try:\n        ret = requests.post(url=f\"{ENDPOINT}/collect\", data={\"charge_id\": charge_id})\n        ret.raise_for_status()\n        return ret.json()\n    except requests.HTTPError as e:\n        raise PaymentError(f\"Unable to collect payment for charge {charge_id}\") from e\n\n\n@tracer.capture_lambda_handler(capture_error=False)\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    charge_id = event.get(\"charge_id\", \"\")\n    ret = collect_payment(charge_id=charge_id)\n\n    return ret.get(\"receipt_id\", \"\")\n</code></pre>"},{"location":"core/tracer/#tracing-aiohttp-requests","title":"Tracing aiohttp requests","text":"Info <p>This snippet assumes you have aiohttp as a dependency</p> <p>You can use <code>aiohttp_trace_config</code> function to create a valid aiohttp trace_config object. This is necessary since X-Ray utilizes aiohttp trace hooks to capture requests end-to-end.</p> Tracing aiohttp requests<pre><code>import asyncio\nimport os\n\nimport aiohttp\n\nfrom aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.tracing import aiohttp_trace_config\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nENDPOINT = os.getenv(\"PAYMENT_API\", \"\")\n\ntracer = Tracer()\n\n\n@tracer.capture_method\nasync def collect_payment(charge_id: str) -&gt; dict:\nasync with aiohttp.ClientSession(trace_configs=[aiohttp_trace_config()]) as session:\nasync with session.get(f\"{ENDPOINT}/collect\") as resp:\n            return await resp.json()\n\n\n@tracer.capture_lambda_handler\ndef handler(event: dict, context: LambdaContext) -&gt; dict:\n    charge_id = event.get(\"charge_id\", \"\")\n    return asyncio.run(collect_payment(charge_id=charge_id))\n</code></pre>"},{"location":"core/tracer/#escape-hatch-mechanism","title":"Escape hatch mechanism","text":"<p>You can use <code>tracer.provider</code> attribute to access all methods provided by AWS X-Ray <code>xray_recorder</code> object.</p> <p>This is useful when you need a feature available in X-Ray that is not available in the Tracer utility, for example thread-safe, or context managers.</p> Tracing a code block with in_subsegment escape hatch<pre><code>from aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\n\n\ndef collect_payment(charge_id: str) -&gt; str:\n    return f\"dummy payment collected for charge: {charge_id}\"\n\n\n@tracer.capture_lambda_handler\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    charge_id = event.get(\"charge_id\", \"\")\nwith tracer.provider.in_subsegment(\"## collect_payment\") as subsegment:\nsubsegment.put_annotation(key=\"PaymentId\", value=charge_id)\n        ret = collect_payment(charge_id=charge_id)\n        subsegment.put_metadata(key=\"payment_response\", value=ret)\n\n    return ret\n</code></pre>"},{"location":"core/tracer/#concurrent-asynchronous-functions","title":"Concurrent asynchronous functions","text":"Warning <p>X-Ray SDK will raise an exception when async functions are run and traced concurrently</p> <p>A safe workaround mechanism is to use <code>in_subsegment_async</code> available via Tracer escape hatch (<code>tracer.provider</code>).</p> Workaround to safely trace async concurrent functions<pre><code>import asyncio\n\nfrom aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\n\n\nasync def another_async_task():\nasync with tracer.provider.in_subsegment_async(\"## another_async_task\") as subsegment:\nsubsegment.put_annotation(key=\"key\", value=\"value\")\n        subsegment.put_metadata(key=\"key\", value=\"value\", namespace=\"namespace\")\n        ...\n\n\nasync def another_async_task_2():\nasync with tracer.provider.in_subsegment_async(\"## another_async_task_2\") as subsegment:\nsubsegment.put_annotation(key=\"key\", value=\"value\")\n        subsegment.put_metadata(key=\"key\", value=\"value\", namespace=\"namespace\")\n        ...\n\n\nasync def collect_payment(charge_id: str) -&gt; str:\nawait asyncio.gather(another_async_task(), another_async_task_2())\nreturn f\"dummy payment collected for charge: {charge_id}\"\n\n\n@tracer.capture_lambda_handler\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    charge_id = event.get(\"charge_id\", \"\")\n    return asyncio.run(collect_payment(charge_id=charge_id))\n</code></pre>"},{"location":"core/tracer/#reusing-tracer-across-your-code","title":"Reusing Tracer across your code","text":"<p>Tracer keeps a copy of its configuration after the first initialization. This is useful for scenarios where you want to use Tracer in more than one location across your code base.</p> Warning: Import order matters when using Lambda Layers or multiple modules <p>Do not set <code>auto_patch=False</code> when reusing Tracer in Lambda Layers, or in multiple modules.</p> <p>This can result in the first Tracer config being inherited by new instances, and their modules not being patched.</p> <p>Tracer will automatically ignore imported modules that have been patched.</p> tracer_reuse.pytracer_reuse_module.py <pre><code>from tracer_reuse_module import collect_payment\nfrom aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\n@tracer.capture_lambda_handler\ndef handler(event: dict, context: LambdaContext) -&gt; str:\n    charge_id = event.get(\"charge_id\", \"\")\n    return collect_payment(charge_id=charge_id)\n</code></pre> <p>A new instance of Tracer will be created but will reuse the previous Tracer instance configuration, similar to a Singleton.</p> <pre><code>from aws_lambda_powertools import Tracer\n\ntracer = Tracer()\n@tracer.capture_method\ndef collect_payment(charge_id: str) -&gt; str:\n    return f\"dummy payment collected for charge: {charge_id}\"\n</code></pre>"},{"location":"core/tracer/#testing-your-code","title":"Testing your code","text":"<p>Tracer is disabled by default when not running in the AWS Lambda environment - This means no code changes or environment variables to be set.</p>"},{"location":"core/tracer/#tips","title":"Tips","text":"<ul> <li>Use annotations on key operations to slice and dice traces, create unique views, and create metrics from it via Trace Groups</li> <li>Use a namespace when adding metadata to group data more easily</li> <li>Annotations and metadata are added to the current subsegment opened. If you want them in a specific subsegment, use a context manager via the escape hatch mechanism</li> </ul>"},{"location":"core/event_handler/api_gateway/","title":"REST API","text":"<p>Event handler for Amazon API Gateway REST and HTTP APIs, Application Loader Balancer (ALB), and Lambda Function URLs.</p>"},{"location":"core/event_handler/api_gateway/#key-features","title":"Key Features","text":"<ul> <li>Lightweight routing to reduce boilerplate for API Gateway REST/HTTP API, ALB and Lambda Function URLs.</li> <li>Support for CORS, binary and Gzip compression, Decimals JSON encoding and bring your own JSON serializer</li> <li>Built-in integration with Event Source Data Classes utilities for self-documented event schema</li> </ul>"},{"location":"core/event_handler/api_gateway/#getting-started","title":"Getting started","text":"Tip <p>All examples shared in this documentation are available within the project repository.</p>"},{"location":"core/event_handler/api_gateway/#required-resources","title":"Required resources","text":"<p>If you're using any API Gateway integration, you must have an existing API Gateway Proxy integration or ALB configured to invoke your Lambda function.</p> <p>This is the sample infrastructure for API Gateway and Lambda Function URLs we are using for the examples in this documentation.</p> There is no additional permissions or dependencies required to use this utility. API Gateway SAM TemplateLambda Function URL SAM Template AWS Serverless Application Model (SAM) example<pre><code>AWSTemplateFormatVersion: \"2010-09-09\"\nTransform: AWS::Serverless-2016-10-31\nDescription: Hello world event handler API Gateway\n\nGlobals:\nApi:\nTracingEnabled: true\nCors: # see CORS section\nAllowOrigin: \"'https://example.com'\"\nAllowHeaders: \"'Content-Type,Authorization,X-Amz-Date'\"\nMaxAge: \"'300'\"\nBinaryMediaTypes: # see Binary responses section\n- \"*~1*\" # converts to */* for any binary type\nFunction:\nTimeout: 5\nRuntime: python3.9\nTracing: Active\nEnvironment:\nVariables:\nLOG_LEVEL: INFO\nPOWERTOOLS_LOGGER_SAMPLE_RATE: 0.1\nPOWERTOOLS_LOGGER_LOG_EVENT: true\nPOWERTOOLS_SERVICE_NAME: example\n\nResources:\nApiFunction:\nType: AWS::Serverless::Function\nProperties:\nHandler: getting_started_rest_api_resolver.lambda_handler\nCodeUri: ../src\nDescription: API handler function\nEvents:\nAnyApiEvent:\nType: Api\nProperties:\n# NOTE: this is a catch-all rule to simplify the documentation.\n# explicit routes and methods are recommended for prod instead (see below)\nPath: /{proxy+} # Send requests on any path to the lambda function\nMethod: ANY # Send requests using any http method to the lambda function\n\n\n# GetAllTodos:\n#   Type: Api\n#   Properties:\n#     Path: /todos\n#     Method: GET\n# GetTodoById:\n#   Type: Api\n#   Properties:\n#     Path: /todos/{todo_id}\n#     Method: GET\n# CreateTodo:\n#   Type: Api\n#   Properties:\n#     Path: /todos\n#     Method: POST\n</code></pre> AWS Serverless Application Model (SAM) example<pre><code>AWSTemplateFormatVersion: \"2010-09-09\"\nTransform: AWS::Serverless-2016-10-31\nDescription: Hello world event handler Lambda Function URL\n\nGlobals:\nFunction:\nTimeout: 5\nRuntime: python3.9\nTracing: Active\nEnvironment:\nVariables:\nLOG_LEVEL: INFO\nPOWERTOOLS_LOGGER_SAMPLE_RATE: 0.1\nPOWERTOOLS_LOGGER_LOG_EVENT: true\nPOWERTOOLS_SERVICE_NAME: example\nFunctionUrlConfig:\nCors: # see CORS section\n# Notice that values here are Lists of Strings, vs comma-separated values on API Gateway\nAllowOrigins: [\"https://example.com\"]\nAllowHeaders: [\"Content-Type\", \"Authorization\", \"X-Amz-Date\"]\nMaxAge: 300\n\nResources:\nApiFunction:\nType: AWS::Serverless::Function\nProperties:\nHandler: getting_started_lambda_function_url_resolver.lambda_handler\nCodeUri: ../src\nDescription: API handler function\nFunctionUrlConfig:\nAuthType: NONE  # AWS_IAM for added security beyond sample documentation\n</code></pre>"},{"location":"core/event_handler/api_gateway/#event-resolvers","title":"Event Resolvers","text":"<p>Before you decorate your functions to handle a given path and HTTP method(s), you need to initialize a resolver.</p> <p>A resolver will handle request resolution, including one or more routers, and give you access to the current event via typed properties.</p> <p>For resolvers, we provide: <code>APIGatewayRestResolver</code>, <code>APIGatewayHttpResolver</code>, <code>ALBResolver</code>, and <code>LambdaFunctionUrlResolver</code>. From here on, we will default to <code>APIGatewayRestResolver</code> across examples.</p> Auto-serialization <p>We serialize <code>Dict</code> responses as JSON, trim whitespace for compact responses, set content-type to <code>application/json</code>, and return a 200 OK HTTP status. You can optionally set a different HTTP status code as the second argument of the tuple:</p> <pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools.event_handler import ALBResolver\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\napp = ALBResolver()\n\n\n@app.post(\"/todo\")\ndef create_todo():\n    data: dict = app.current_event.json_body\n    todo: Response = requests.post(\"https://jsonplaceholder.typicode.com/todos\", data=data)\n\n# Returns the created todo object, with a HTTP 201 Created status\nreturn {\"todo\": todo.json()}, 201\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre>"},{"location":"core/event_handler/api_gateway/#api-gateway-rest-api","title":"API Gateway REST API","text":"<p>When using Amazon API Gateway REST API to front your Lambda functions, you can use <code>APIGatewayRestResolver</code>.</p> <p>Here's an example on how we can handle the <code>/todos</code> path.</p> Trailing slash in routes <p>For <code>APIGatewayRestResolver</code>, we seamless handle routes with a trailing slash (<code>/todos/</code>).</p> getting_started_rest_api_resolver.pygetting_started_rest_api_resolver.jsongetting_started_rest_api_resolver_output.json <pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    todos: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    # for brevity, we'll limit to the first 10 only\n    return {\"todos\": todos.json()[:10]}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\nreturn app.resolve(event, context)\n</code></pre> <p>This utility uses <code>path</code> and <code>httpMethod</code> to route to the right function. This helps make unit tests and local invocation easier too.</p> <pre><code>{\n\"body\": \"\",\n\"resource\": \"/todos\",\n\"path\": \"/todos\",\n\"httpMethod\": \"GET\",\n\"isBase64Encoded\": false,\n\"queryStringParameters\": {},\n\"multiValueQueryStringParameters\": {},\n\"pathParameters\": {},\n\"stageVariables\": {},\n\"headers\": {\n\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n\"Accept-Encoding\": \"gzip, deflate, sdch\",\n\"Accept-Language\": \"en-US,en;q=0.8\",\n\"Cache-Control\": \"max-age=0\",\n\"CloudFront-Forwarded-Proto\": \"https\",\n\"CloudFront-Is-Desktop-Viewer\": \"true\",\n\"CloudFront-Is-Mobile-Viewer\": \"false\",\n\"CloudFront-Is-SmartTV-Viewer\": \"false\",\n\"CloudFront-Is-Tablet-Viewer\": \"false\",\n\"CloudFront-Viewer-Country\": \"US\",\n\"Host\": \"1234567890.execute-api.us-east-1.amazonaws.com\",\n\"Upgrade-Insecure-Requests\": \"1\",\n\"User-Agent\": \"Custom User Agent String\",\n\"Via\": \"1.1 08f323deadbeefa7af34d5feb414ce27.cloudfront.net (CloudFront)\",\n\"X-Amz-Cf-Id\": \"cDehVQoZnx43VYQb9j2-nvCh-9z396Uhbp027Y2JvkCPNLmGJHqlaA==\",\n\"X-Forwarded-For\": \"127.0.0.1, 127.0.0.2\",\n\"X-Forwarded-Port\": \"443\",\n\"X-Forwarded-Proto\": \"https\"\n},\n\"multiValueHeaders\": {},\n\"requestContext\": {\n\"accountId\": \"123456789012\",\n\"resourceId\": \"123456\",\n\"stage\": \"Prod\",\n\"requestId\": \"c6af9ac6-7b61-11e6-9a41-93e8deadbeef\",\n\"requestTime\": \"25/Jul/2020:12:34:56 +0000\",\n\"requestTimeEpoch\": 1428582896000,\n\"identity\": {\n\"cognitoIdentityPoolId\": null,\n\"accountId\": null,\n\"cognitoIdentityId\": null,\n\"caller\": null,\n\"accessKey\": null,\n\"sourceIp\": \"127.0.0.1\",\n\"cognitoAuthenticationType\": null,\n\"cognitoAuthenticationProvider\": null,\n\"userArn\": null,\n\"userAgent\": \"Custom User Agent String\",\n\"user\": null\n},\n\"path\": \"/Prod/todos\",\n\"resourcePath\": \"/todos\",\n\"httpMethod\": \"GET\",\n\"apiId\": \"1234567890\",\n\"protocol\": \"HTTP/1.1\"\n}\n}\n</code></pre> <pre><code>{\n\"statusCode\": 200,\n\"multiValueHeaders\": {\n\"Content-Type\": [\"application/json\"]\n},\n\"body\": \"{\\\"todos\\\":[{\\\"userId\\\":1,\\\"id\\\":1,\\\"title\\\":\\\"delectus aut autem\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":2,\\\"title\\\":\\\"quis ut nam facilis et officia qui\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":3,\\\"title\\\":\\\"fugiat veniam minus\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":4,\\\"title\\\":\\\"et porro tempora\\\",\\\"completed\\\":true},{\\\"userId\\\":1,\\\"id\\\":5,\\\"title\\\":\\\"laboriosam mollitia et enim quasi adipisci quia provident illum\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":6,\\\"title\\\":\\\"qui ullam ratione quibusdam voluptatem quia omnis\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":7,\\\"title\\\":\\\"illo expedita consequatur quia in\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":8,\\\"title\\\":\\\"quo adipisci enim quam ut ab\\\",\\\"completed\\\":true},{\\\"userId\\\":1,\\\"id\\\":9,\\\"title\\\":\\\"molestiae perspiciatis ipsa\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":10,\\\"title\\\":\\\"illo est ratione doloremque quia maiores aut\\\",\\\"completed\\\":true}]}\",\n\"isBase64Encoded\": false\n}\n</code></pre>"},{"location":"core/event_handler/api_gateway/#api-gateway-http-api","title":"API Gateway HTTP API","text":"<p>When using Amazon API Gateway HTTP API to front your Lambda functions, you can use <code>APIGatewayHttpResolver</code>.</p> Note <p>Using HTTP API v1 payload? Use <code>APIGatewayRestResolver</code> instead. <code>APIGatewayHttpResolver</code> defaults to v2 payload.</p> Using HTTP API resolver<pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayHttpResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayHttpResolver()\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    todos: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    # for brevity, we'll limit to the first 10 only\n    return {\"todos\": todos.json()[:10]}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_HTTP)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre>"},{"location":"core/event_handler/api_gateway/#application-load-balancer","title":"Application Load Balancer","text":"<p>When using Amazon Application Load Balancer (ALB) to front your Lambda functions, you can use <code>ALBResolver</code>.</p> Using ALB resolver<pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import ALBResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = ALBResolver()\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    todos: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    # for brevity, we'll limit to the first 10 only\n    return {\"todos\": todos.json()[:10]}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPLICATION_LOAD_BALANCER)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre>"},{"location":"core/event_handler/api_gateway/#lambda-function-url","title":"Lambda Function URL","text":"<p>When using AWS Lambda Function URL, you can use <code>LambdaFunctionUrlResolver</code>.</p> getting_started_lambda_function_url_resolver.pygetting_started_lambda_function_url_resolver.json Using Lambda Function URL resolver<pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import LambdaFunctionUrlResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = LambdaFunctionUrlResolver()\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    todos: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    # for brevity, we'll limit to the first 10 only\n    return {\"todos\": todos.json()[:10]}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.LAMBDA_FUNCTION_URL)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> Example payload delivered to the handler<pre><code>{\n\"version\": \"2.0\",\n\"routeKey\": \"$default\",\n\"rawPath\": \"/todos\",\n\"rawQueryString\": \"\",\n\"headers\": {\n\"x-amz-content-sha256\": \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\",\n\"x-amzn-tls-version\": \"TLSv1.2\",\n\"x-amz-date\": \"20220803T092917Z\",\n\"x-forwarded-proto\": \"https\",\n\"x-forwarded-port\": \"443\",\n\"x-forwarded-for\": \"123.123.123.123\",\n\"accept\": \"application/xml\",\n\"x-amzn-tls-cipher-suite\": \"ECDHE-RSA-AES128-GCM-SHA256\",\n\"x-amzn-trace-id\": \"Root=1-63ea3fee-51ba94542feafa3928745ba3\",\n\"host\": \"xxxxxxxxxxxxx.lambda-url.eu-central-1.on.aws\",\n\"content-type\": \"application/json\",\n\"accept-encoding\": \"gzip, deflate\",\n\"user-agent\": \"Custom User Agent\"\n},\n\"requestContext\": {\n\"accountId\": \"123457890\",\n\"apiId\": \"xxxxxxxxxxxxxxxxxxxx\",\n\"authorizer\": {\n\"iam\": {\n\"accessKey\": \"AAAAAAAAAAAAAAAAAA\",\n\"accountId\": \"123457890\",\n\"callerId\": \"AAAAAAAAAAAAAAAAAA\",\n\"cognitoIdentity\": null,\n\"principalOrgId\": \"o-xxxxxxxxxxxx\",\n\"userArn\": \"arn:aws:iam::AAAAAAAAAAAAAAAAAA:user/user\",\n\"userId\": \"AAAAAAAAAAAAAAAAAA\"\n}\n},\n\"domainName\": \"xxxxxxxxxxxxx.lambda-url.eu-central-1.on.aws\",\n\"domainPrefix\": \"xxxxxxxxxxxxx\",\n\"http\": {\n\"method\": \"GET\",\n\"path\": \"/todos\",\n\"protocol\": \"HTTP/1.1\",\n\"sourceIp\": \"123.123.123.123\",\n\"userAgent\": \"Custom User Agent\"\n},\n\"requestId\": \"24f9ef37-8eb7-45fe-9dbc-a504169fd2f8\",\n\"routeKey\": \"$default\",\n\"stage\": \"$default\",\n\"time\": \"03/Aug/2022:09:29:18 +0000\",\n\"timeEpoch\": 1659518958068\n},\n\"isBase64Encoded\": false\n}\n</code></pre>"},{"location":"core/event_handler/api_gateway/#dynamic-routes","title":"Dynamic routes","text":"<p>You can use <code>/todos/&lt;todo_id&gt;</code> to configure dynamic URL paths, where <code>&lt;todo_id&gt;</code> will be resolved at runtime.</p> <p>Each dynamic route you set must be part of your function signature. This allows us to call your function using keyword arguments when matching your dynamic route.</p> Note <p>For brevity, we will only include the necessary keys for each sample request for the example to work.</p> dynamic_routes.pydynamic_routes.json <pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n\n\n@app.get(\"/todos/&lt;todo_id&gt;\")\n@tracer.capture_method\ndef get_todo_by_id(todo_id: str):  # value come as str\ntodos: Response = requests.get(f\"https://jsonplaceholder.typicode.com/todos/{todo_id}\")\n    todos.raise_for_status()\n\n    return {\"todos\": todos.json()}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> <pre><code>{\n\"resource\": \"/todos/{id}\",\n\"path\": \"/todos/1\",\n\"httpMethod\": \"GET\"\n}\n</code></pre> Tip <p>You can also nest dynamic paths, for example <code>/todos/&lt;todo_id&gt;/&lt;todo_status&gt;</code>.</p>"},{"location":"core/event_handler/api_gateway/#catch-all-routes","title":"Catch-all routes","text":"Note <p>We recommend having explicit routes whenever possible; use catch-all routes sparingly.</p> <p>You can use a regex string to handle an arbitrary number of paths within a request, for example <code>.+</code>.</p> <p>You can also combine nested paths with greedy regex to catch in between routes.</p> Warning <p>We choose the most explicit registered route that matches an incoming event.</p> dynamic_routes_catch_all.pydynamic_routes_catch_all.json <pre><code>from aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n\n\n@app.get(\".+\")\n@tracer.capture_method\ndef catch_any_route_get_method():\n    return {\"path_received\": app.current_event.path}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> <pre><code>{\n\"resource\": \"/{proxy+}\",\n\"path\": \"/any/route/should/work\",\n\"httpMethod\": \"GET\"\n}\n</code></pre>"},{"location":"core/event_handler/api_gateway/#http-methods","title":"HTTP Methods","text":"<p>You can use named decorators to specify the HTTP method that should be handled in your functions. That is, <code>app.&lt;http_method&gt;</code>, where the HTTP method could be <code>get</code>, <code>post</code>, <code>put</code>, <code>patch</code> and <code>delete</code>.</p> http_methods.pyhttp_methods.json <pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n\n\n@app.post(\"/todos\")\n@tracer.capture_method\ndef create_todo():\ntodo_data: dict = app.current_event.json_body  # deserialize json str to dict\ntodo: Response = requests.post(\"https://jsonplaceholder.typicode.com/todos\", data=todo_data)\n    todo.raise_for_status()\n\n    return {\"todo\": todo.json()}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> <pre><code>{\n\"resource\": \"/todos\",\n\"path\": \"/todos\",\n\"httpMethod\": \"POST\",\n\"body\": \"{\\\"title\\\": \\\"foo\\\", \\\"userId\\\": 1, \\\"completed\\\": false}\"\n}\n</code></pre> <p>If you need to accept multiple HTTP methods in a single function, you can use the <code>route</code> method and pass a list of HTTP methods.</p> Handling multiple HTTP Methods<pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n\n\n# PUT and POST HTTP requests to the path /hello will route to this function\n@app.route(\"/todos\", method=[\"PUT\", \"POST\"])\n@tracer.capture_method\ndef create_todo():\n    todo_data: dict = app.current_event.json_body  # deserialize json str to dict\n    todo: Response = requests.post(\"https://jsonplaceholder.typicode.com/todos\", data=todo_data)\n    todo.raise_for_status()\n\n    return {\"todo\": todo.json()}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> Note <p>It is generally better to have separate functions for each HTTP method, as the functionality tends to differ depending on which method is used.</p>"},{"location":"core/event_handler/api_gateway/#accessing-request-details","title":"Accessing request details","text":"<p>Event Handler integrates with Event Source Data Classes utilities, and it exposes their respective resolver request details and convenient methods under <code>app.current_event</code>.</p> <p>That is why you see <code>app.resolve(event, context)</code> in every example. This allows Event Handler to resolve requests, and expose data like <code>app.lambda_context</code> and  <code>app.current_event</code>.</p>"},{"location":"core/event_handler/api_gateway/#query-strings-and-payload","title":"Query strings and payload","text":"<p>Within <code>app.current_event</code> property, you can access all available query strings as a dictionary via <code>query_string_parameters</code>, or a specific one via  <code>get_query_string_value</code> method.</p> <p>You can access the raw payload via <code>body</code> property, or if it's a JSON string you can quickly deserialize it via <code>json_body</code> property - like the earlier example in the HTTP Methods section.</p> Accessing query strings and raw payload<pre><code>from typing import Optional\n\nimport requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n\n\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\ntodo_id: str = app.current_event.get_query_string_value(name=\"id\", default_value=\"\")\n# alternatively\n    _: Optional[str] = app.current_event.query_string_parameters.get(\"id\")\n\n    # Payload\n_: Optional[str] = app.current_event.body  # raw str | None\nendpoint = \"https://jsonplaceholder.typicode.com/todos\"\n    if todo_id:\n        endpoint = f\"{endpoint}/{todo_id}\"\n\n    todos: Response = requests.get(endpoint)\n    todos.raise_for_status()\n\n    return {\"todos\": todos.json()}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre>"},{"location":"core/event_handler/api_gateway/#headers","title":"Headers","text":"<p>Similarly to Query strings, you can access headers as dictionary via <code>app.current_event.headers</code>, or by name via <code>get_header_value</code>.</p> Accessing HTTP Headers<pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n\n\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    endpoint = \"https://jsonplaceholder.typicode.com/todos\"\n\napi_key: str = app.current_event.get_header_value(name=\"X-Api-Key\", case_sensitive=True, default_value=\"\")\ntodos: Response = requests.get(endpoint, headers={\"X-Api-Key\": api_key})\n    todos.raise_for_status()\n\n    return {\"todos\": todos.json()}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre>"},{"location":"core/event_handler/api_gateway/#handling-not-found-routes","title":"Handling not found routes","text":"<p>By default, we return <code>404</code> for any unmatched route.</p> <p>You can use <code>not_found</code> decorator to override this behavior, and return a custom <code>Response</code>.</p> Handling not found<pre><code>import requests\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import (\n    APIGatewayRestResolver,\n    Response,\n    content_types,\n)\nfrom aws_lambda_powertools.event_handler.exceptions import NotFoundError\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n\n\n@app.not_found\n@tracer.capture_method\ndef handle_not_found_errors(exc: NotFoundError) -&gt; Response:\n    logger.info(f\"Not found route: {app.current_event.path}\")\nreturn Response(status_code=418, content_type=content_types.TEXT_PLAIN, body=\"I'm a teapot!\")\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    todos: requests.Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    # for brevity, we'll limit to the first 10 only\n    return {\"todos\": todos.json()[:10]}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre>"},{"location":"core/event_handler/api_gateway/#exception-handling","title":"Exception handling","text":"<p>You can use <code>exception_handler</code> decorator with any Python exception. This allows you to handle a common exception outside your route, for example validation errors.</p> Exception handling<pre><code>import requests\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import (\n    APIGatewayRestResolver,\n    Response,\n    content_types,\n)\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n\n\n@app.exception_handler(ValueError)\ndef handle_invalid_limit_qs(ex: ValueError):  # receives exception raised\nmetadata = {\"path\": app.current_event.path, \"query_strings\": app.current_event.query_string_parameters}\n    logger.error(f\"Malformed request: {ex}\", extra=metadata)\n\n    return Response(\n        status_code=400,\n        content_type=content_types.TEXT_PLAIN,\n        body=\"Invalid request parameters.\",\n    )\n\n\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    # educational purpose only: we should receive a `ValueError`\n    # if a query string value for `limit` cannot be coerced to int\n    max_results: int = int(app.current_event.get_query_string_value(name=\"limit\", default_value=0))\n\n    todos: requests.Response = requests.get(f\"https://jsonplaceholder.typicode.com/todos?limit={max_results}\")\n    todos.raise_for_status()\n\n    return {\"todos\": todos.json()}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> Info <p>The <code>exception_handler</code> also supports passing a list of exception types you wish to handle with one handler.</p>"},{"location":"core/event_handler/api_gateway/#raising-http-errors","title":"Raising HTTP errors","text":"<p>You can easily raise any HTTP Error back to the client using <code>ServiceError</code> exception. This ensures your Lambda function doesn't fail but return the correct HTTP response signalling the error.</p> Info <p>If you need to send custom headers, use Response class instead.</p> <p>We provide pre-defined errors for the most popular ones such as HTTP 400, 401, 404, 500.</p> Raising common HTTP Status errors (4xx, 5xx)<pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.event_handler.exceptions import (\nBadRequestError,\nInternalServerError,\nNotFoundError,\nServiceError,\nUnauthorizedError,\n)\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n\n\n@app.get(rule=\"/bad-request-error\")\ndef bad_request_error():\nraise BadRequestError(\"Missing required parameter\")  # HTTP  400\n@app.get(rule=\"/unauthorized-error\")\ndef unauthorized_error():\nraise UnauthorizedError(\"Unauthorized\")  # HTTP 401\n@app.get(rule=\"/not-found-error\")\ndef not_found_error():\nraise NotFoundError  # HTTP 404\n@app.get(rule=\"/internal-server-error\")\ndef internal_server_error():\nraise InternalServerError(\"Internal server error\")  # HTTP 500\n@app.get(rule=\"/service-error\", cors=True)\ndef service_error():\nraise ServiceError(502, \"Something went wrong!\")\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    todos: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    return {\"todos\": todos.json()[:10]}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre>"},{"location":"core/event_handler/api_gateway/#custom-domain-api-mappings","title":"Custom Domain API Mappings","text":"<p>When using Custom Domain API Mappings feature, you must use <code>strip_prefixes</code> param in the <code>APIGatewayRestResolver</code> constructor.</p> <p>Scenario: You have a custom domain <code>api.mydomain.dev</code>. Then you set <code>/payment</code> API Mapping to forward any payment requests to your Payments API.</p> <p>Challenge: This means your <code>path</code> value for any API requests will always contain <code>/payment/&lt;actual_request&gt;</code>, leading to HTTP 404 as Event Handler is trying to match what's after <code>payment/</code>. This gets further complicated with an arbitrary level of nesting.</p> <p>To address this API Gateway behavior, we use <code>strip_prefixes</code> parameter to account for these prefixes that are now injected into the path regardless of which type of API Gateway you're using.</p> custom_api_mapping.pycustom_api_mapping.json <pre><code>from aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver(strip_prefixes=[\"/payment\"])\n@app.get(\"/subscriptions/&lt;subscription&gt;\")\n@tracer.capture_method\ndef get_subscription(subscription):\n    return {\"subscription_id\": subscription}\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> <pre><code>{\n\"resource\": \"/subscriptions/{subscription}\",\n\"path\": \"/payment/subscriptions/123\",\n\"httpMethod\": \"GET\"\n}\n</code></pre> Note <p>After removing a path prefix with <code>strip_prefixes</code>, the new root path will automatically be mapped to the path argument of <code>/</code>.</p> <p>For example, when using <code>strip_prefixes</code> value of <code>/pay</code>, there is no difference between a request path of <code>/pay</code> and <code>/pay/</code>; and the path argument would be defined as <code>/</code>.</p>"},{"location":"core/event_handler/api_gateway/#advanced","title":"Advanced","text":""},{"location":"core/event_handler/api_gateway/#cors","title":"CORS","text":"<p>You can configure CORS at the <code>APIGatewayRestResolver</code> constructor via <code>cors</code> parameter using the <code>CORSConfig</code> class.</p> <p>This will ensure that CORS headers are always returned as part of the response when your functions match the path invoked.</p> Tip <p>Optionally disable CORS on a per path basis with <code>cors=False</code> parameter.</p> setting_cors.pysetting_cors_output.json <pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver, CORSConfig\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\ncors_config = CORSConfig(allow_origin=\"https://example.com\", max_age=300)\napp = APIGatewayRestResolver(cors=cors_config)\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    todos: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    # for brevity, we'll limit to the first 10 only\n    return {\"todos\": todos.json()[:10]}\n\n\n@app.get(\"/todos/&lt;todo_id&gt;\")\n@tracer.capture_method\ndef get_todo_by_id(todo_id: str):  # value come as str\n    todos: Response = requests.get(f\"https://jsonplaceholder.typicode.com/todos/{todo_id}\")\n    todos.raise_for_status()\n\n    return {\"todos\": todos.json()}\n\n\n@app.get(\"/healthcheck\", cors=False)  # optionally removes CORS for a given route\n@tracer.capture_method\ndef am_i_alive():\n    return {\"am_i_alive\": \"yes\"}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> <pre><code>{\n\"statusCode\": 200,\n\"multiValueHeaders\": {\n\"Content-Type\": [\"application/json\"],\n\"Access-Control-Allow-Origin\": [\"https://www.example.com\"],\n\"Access-Control-Allow-Headers\": [\"Authorization,Content-Type,X-Amz-Date,X-Amz-Security-Token,X-Api-Key\"]\n},\n\"body\": \"{\\\"todos\\\":[{\\\"userId\\\":1,\\\"id\\\":1,\\\"title\\\":\\\"delectus aut autem\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":2,\\\"title\\\":\\\"quis ut nam facilis et officia qui\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":3,\\\"title\\\":\\\"fugiat veniam minus\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":4,\\\"title\\\":\\\"et porro tempora\\\",\\\"completed\\\":true},{\\\"userId\\\":1,\\\"id\\\":5,\\\"title\\\":\\\"laboriosam mollitia et enim quasi adipisci quia provident illum\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":6,\\\"title\\\":\\\"qui ullam ratione quibusdam voluptatem quia omnis\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":7,\\\"title\\\":\\\"illo expedita consequatur quia in\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":8,\\\"title\\\":\\\"quo adipisci enim quam ut ab\\\",\\\"completed\\\":true},{\\\"userId\\\":1,\\\"id\\\":9,\\\"title\\\":\\\"molestiae perspiciatis ipsa\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":10,\\\"title\\\":\\\"illo est ratione doloremque quia maiores aut\\\",\\\"completed\\\":true}]}\",\n\"isBase64Encoded\": false\n}\n</code></pre>"},{"location":"core/event_handler/api_gateway/#pre-flight","title":"Pre-flight","text":"<p>Pre-flight (OPTIONS) calls are typically handled at the API Gateway or Lambda Function URL level as per our sample infrastructure, no Lambda integration is necessary. However, ALB expects you to handle pre-flight requests.</p> <p>For convenience, we automatically handle that for you as long as you setup CORS in the constructor level.</p>"},{"location":"core/event_handler/api_gateway/#defaults","title":"Defaults","text":"<p>For convenience, these are the default values when using <code>CORSConfig</code> to enable CORS:</p> Warning <p>Always configure <code>allow_origin</code> when using in production.</p> Key Value Note allow_origin: <code>str</code> <code>*</code> Only use the default value for development. Never use <code>*</code> for production unless your use case requires it allow_headers: <code>List[str]</code> <code>[Authorization, Content-Type, X-Amz-Date, X-Api-Key, X-Amz-Security-Token]</code> Additional headers will be appended to the default list for your convenience expose_headers: <code>List[str]</code> <code>[]</code> Any additional header beyond the safe listed by CORS specification. max_age: <code>int</code> `` Only for pre-flight requests if you choose to have your function to handle it instead of API Gateway allow_credentials: <code>bool</code> <code>False</code> Only necessary when you need to expose cookies, authorization headers or TLS client certificates."},{"location":"core/event_handler/api_gateway/#fine-grained-responses","title":"Fine grained responses","text":"<p>You can use the <code>Response</code> class to have full control over the response. For example, you might want to add additional headers, cookies, or set a custom Content-type.</p> Info <p>Powertools serializes headers and cookies according to the type of input event. Some event sources require headers and cookies to be encoded as <code>multiValueHeaders</code>.</p> Using multiple values for HTTP headers in ALB? <p>Make sure you enable the multi value headers feature to serialize response headers correctly.</p> fine_grained_responses.pyfine_grained_responses_output.json <pre><code>from http import HTTPStatus\nfrom uuid import uuid4\n\nimport requests\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import (\n    APIGatewayRestResolver,\nResponse,\ncontent_types,\n)\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.shared.cookies import Cookie\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n\n\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    todos: requests.Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    custom_headers = {\"X-Transaction-Id\": [f\"{uuid4()}\"]}\nreturn Response(\nstatus_code=HTTPStatus.OK.value,  # 200\ncontent_type=content_types.APPLICATION_JSON,\nbody=todos.json()[:10],\nheaders=custom_headers,\n        cookies=[Cookie(name=\"session_id\", value=\"12345\")],\n    )\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> <pre><code>{\n\"statusCode\": 200,\n\"multiValueHeaders\": {\n\"Content-Type\": [\"application/json\"],\n\"X-Transaction-Id\": [\"3490eea9-791b-47a0-91a4-326317db61a9\"],\n\"Set-Cookie\": [\"session_id=12345; Secure\"]\n},\n\"body\": \"{\\\"todos\\\":[{\\\"userId\\\":1,\\\"id\\\":1,\\\"title\\\":\\\"delectus aut autem\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":2,\\\"title\\\":\\\"quis ut nam facilis et officia qui\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":3,\\\"title\\\":\\\"fugiat veniam minus\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":4,\\\"title\\\":\\\"et porro tempora\\\",\\\"completed\\\":true},{\\\"userId\\\":1,\\\"id\\\":5,\\\"title\\\":\\\"laboriosam mollitia et enim quasi adipisci quia provident illum\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":6,\\\"title\\\":\\\"qui ullam ratione quibusdam voluptatem quia omnis\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":7,\\\"title\\\":\\\"illo expedita consequatur quia in\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":8,\\\"title\\\":\\\"quo adipisci enim quam ut ab\\\",\\\"completed\\\":true},{\\\"userId\\\":1,\\\"id\\\":9,\\\"title\\\":\\\"molestiae perspiciatis ipsa\\\",\\\"completed\\\":false},{\\\"userId\\\":1,\\\"id\\\":10,\\\"title\\\":\\\"illo est ratione doloremque quia maiores aut\\\",\\\"completed\\\":true}]}\",\n\"isBase64Encoded\": false\n}\n</code></pre>"},{"location":"core/event_handler/api_gateway/#compress","title":"Compress","text":"<p>You can compress with gzip and base64 encode your responses via <code>compress</code> parameter.</p> Warning <p>The client must send the <code>Accept-Encoding</code> header, otherwise a normal response will be sent.</p> compressing_responses.pycompressing_responses.jsoncompressing_responses_output.json <pre><code> import requests\n from requests import Response\n\n from aws_lambda_powertools import Logger, Tracer\n from aws_lambda_powertools.event_handler import APIGatewayRestResolver\n from aws_lambda_powertools.logging import correlation_paths\n from aws_lambda_powertools.utilities.typing import LambdaContext\n\n tracer = Tracer()\n logger = Logger()\n app = APIGatewayRestResolver()\n\n\n@app.get(\"/todos\", compress=True)\n@tracer.capture_method\n def get_todos():\n     todos: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n     todos.raise_for_status()\n\n     # for brevity, we'll limit to the first 10 only\n     return {\"todos\": todos.json()[:10]}\n\n\n # You can continue to use other utilities just as before\n @logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n @tracer.capture_lambda_handler\n def lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n     return app.resolve(event, context)\n</code></pre> <pre><code>{\n\"headers\": {\n\"Accept-Encoding\": \"gzip\"\n},\n\"resource\": \"/todos\",\n\"path\": \"/todos\",\n\"httpMethod\": \"GET\"\n}\n</code></pre> <pre><code>{\n\"statusCode\": 200,\n\"multiValueHeaders\": {\n\"Content-Type\": [\"application/json\"],\n\"Content-Encoding\": [\"gzip\"]\n},\n\"body\": \"H4sIAAAAAAACE42STU4DMQyFrxJl3QXln96AMyAW7sSDLCVxiJ0Kqerd8TCCUOgii1EmP/783pOPXjmw+N3L0TfB+hz8brvxtC5KGtHvfMCIkzZx0HT5MPmNnziViIr2dIYoeNr8Q1x3xHsjcVadIbkZJoq2RXU8zzQROLseQ9505NzeCNQdMJNBE+UmY4zbzjAJhWtlZ57sB84BWtul+rteH2HPlVgWARwjqXkxpklK5gmEHAQqJBMtFsGVygcKmNVRjG0wxvuzGF2L0dpVUOKMC3bfJNjJgWMrCuZk7cUp02AiD72D6WKHHwUDKbiJs6AZ0VZXKOUx4uNvzdxT+E4mLcMA+6G8nzrLQkaxkNEVrFKW2VGbJCoCY7q2V3+tiv5kGThyxfTecDWbgGz/NfYXhL6ePgF9PnFdPgMAAA==\",\n\"isBase64Encoded\": true\n}\n</code></pre>"},{"location":"core/event_handler/api_gateway/#binary-responses","title":"Binary responses","text":"<p>For convenience, we automatically base64 encode binary responses. You can also use in combination with <code>compress</code> parameter if your client supports gzip.</p> <p>Like <code>compress</code> feature, the client must send the <code>Accept</code> header with the correct media type.</p> Warning <p>This feature requires API Gateway to configure binary media types, see our sample infrastructure for reference.</p> Note <p>Lambda Function URLs handle binary media types automatically.</p> binary_responses.pybinary_responses_logo.svgbinary_responses.jsonbinary_responses_output.json <pre><code>import os\nfrom pathlib import Path\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler.api_gateway import (\n    APIGatewayRestResolver,\n    Response,\n)\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\n\n\napp = APIGatewayRestResolver()\nlogo_file: bytes = Path(f\"{os.getenv('LAMBDA_TASK_ROOT')}/logo.svg\").read_bytes()\n@app.get(\"/logo\")\n@tracer.capture_method\ndef get_logo():\nreturn Response(status_code=200, content_type=\"image/svg+xml\", body=logo_file)\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;svg width=\"256px\" height=\"256px\" viewBox=\"0 0 256 256\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" preserveAspectRatio=\"xMidYMid\"&gt;\n&lt;title&gt;AWS Lambda&lt;/title&gt;\n&lt;defs&gt;\n&lt;linearGradient x1=\"0%\" y1=\"100%\" x2=\"100%\" y2=\"0%\" id=\"linearGradient-1\"&gt;\n&lt;stop stop-color=\"#C8511B\" offset=\"0%\"&gt;&lt;/stop&gt;\n&lt;stop stop-color=\"#FF9900\" offset=\"100%\"&gt;&lt;/stop&gt;\n&lt;/linearGradient&gt;\n&lt;/defs&gt;\n&lt;g&gt;\n&lt;rect fill=\"url(#linearGradient-1)\" x=\"0\" y=\"0\" width=\"256\" height=\"256\"&gt;&lt;/rect&gt;\n&lt;path d=\"M89.6241126,211.2 L49.8903277,211.2 L93.8354832,119.3472 L113.74728,160.3392 L89.6241126,211.2 Z M96.7029357,110.5696 C96.1640858,109.4656 95.0414813,108.7648 93.8162384,108.7648 L93.8066163,108.7648 C92.5717514,108.768 91.4491466,109.4752 90.9199187,110.5856 L41.9134208,213.0208 C41.4387197,214.0128 41.5060758,215.1776 42.0962451,216.1088 C42.6799994,217.0368 43.7063805,217.6 44.8065331,217.6 L91.654423,217.6 C92.8957027,217.6 94.0215149,216.8864 94.5539501,215.7696 L120.203859,161.6896 C120.617619,160.8128 120.614412,159.7984 120.187822,158.928 L96.7029357,110.5696 Z M207.985117,211.2 L168.507928,211.2 L105.173789,78.624 C104.644561,77.5104 103.515541,76.8 102.277469,76.8 L76.447943,76.8 L76.4768099,44.8 L127.103066,44.8 L190.145328,177.3728 C190.674556,178.4864 191.803575,179.2 193.041647,179.2 L207.985117,179.2 L207.985117,211.2 Z M211.192558,172.8 L195.071958,172.8 L132.029696,40.2272 C131.500468,39.1136 130.371449,38.4 129.130169,38.4 L73.272576,38.4 C71.5052758,38.4 70.0683421,39.8304 70.0651344,41.5968 L70.0298528,79.9968 C70.0298528,80.848 70.3634266,81.6608 70.969633,82.2624 C71.5694246,82.864 72.3841146,83.2 73.2372941,83.2 L100.253573,83.2 L163.59092,215.776 C164.123355,216.8896 165.24596,217.6 166.484032,217.6 L211.192558,217.6 C212.966274,217.6 214.4,216.1664 214.4,214.4 L214.4,176 C214.4,174.2336 212.966274,172.8 211.192558,172.8 L211.192558,172.8 Z\" fill=\"#FFFFFF\"&gt;&lt;/path&gt;\n&lt;/g&gt;\n&lt;/svg&gt;\n</code></pre> <pre><code>{\n\"headers\": {\n\"Accept\": \"image/svg+xml\"\n},\n\"resource\": \"/logo\",\n\"path\": \"/logo\",\n\"httpMethod\": \"GET\"\n}\n</code></pre> <pre><code>{\n\"body\": \"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\",\n\"multiValueHeaders\": {\n\"Content-Type\": [\"image/svg+xml\"]\n},\n\"isBase64Encoded\": true,\n\"statusCode\": 200\n}\n</code></pre>"},{"location":"core/event_handler/api_gateway/#debug-mode","title":"Debug mode","text":"<p>You can enable debug mode via <code>debug</code> param, or via <code>POWERTOOLS_DEV</code> environment variable.</p> <p>This will enable full tracebacks errors in the response, print request and responses, and set CORS in development mode.</p> Danger <p>This might reveal sensitive information in your logs and relax CORS restrictions, use it sparingly.</p> <p>It's best to use for local development only!</p> Enabling debug mode<pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver(debug=True)\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    todos: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    # for brevity, we'll limit to the first 10 only\n    return {\"todos\": todos.json()[:10]}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre>"},{"location":"core/event_handler/api_gateway/#custom-serializer","title":"Custom serializer","text":"<p>You can instruct event handler to use a custom serializer to best suit your needs, for example take into account Enums when serializing.</p> Using a custom JSON serializer for responses<pre><code>import json\nfrom dataclasses import asdict, dataclass, is_dataclass\nfrom json import JSONEncoder\n\nimport requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n\n\n@dataclass\nclass Todo:\n    userId: str\n    id: str  # noqa: A003 VNE003 \"id\" field is reserved\n    title: str\n    completed: bool\n\n\nclass DataclassCustomEncoder(JSONEncoder):\n\"\"\"A custom JSON encoder to serialize dataclass obj\"\"\"\n\n    def default(self, obj):\n        # Only called for values that aren't JSON serializable\n        # where `obj` will be an instance of Todo in this example\n        return asdict(obj) if is_dataclass(obj) else super().default(obj)\n\n\ndef custom_serializer(obj) -&gt; str:\n\"\"\"Your custom serializer function APIGatewayRestResolver will use\"\"\"\n    return json.dumps(obj, separators=(\",\", \":\"), cls=DataclassCustomEncoder)\n\n\napp = APIGatewayRestResolver(serializer=custom_serializer)\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    ret: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    ret.raise_for_status()\n    todos = [Todo(**todo) for todo in ret.json()]\n\n    # for brevity, we'll limit to the first 10 only\n    return {\"todos\": todos[:10]}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre>"},{"location":"core/event_handler/api_gateway/#split-routes-with-router","title":"Split routes with Router","text":"<p>As you grow the number of routes a given Lambda function should handle, it is natural to split routes into separate files to ease maintenance - That's where the <code>Router</code> feature is useful.</p> <p>Let's assume you have <code>split_route.py</code> as your Lambda function entrypoint and routes in <code>split_route_module.py</code>. This is how you'd use the <code>Router</code> feature.</p> split_route_module.pysplit_route.py <p>We import Router instead of APIGatewayRestResolver; syntax wise is exactly the same.</p> <pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.event_handler.api_gateway import Router\ntracer = Tracer()\nrouter = Router()\n\nendpoint = \"https://jsonplaceholder.typicode.com/todos\"\n\n\n@router.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\napi_key: str = router.current_event.get_header_value(name=\"X-Api-Key\", case_sensitive=True, default_value=\"\")\ntodos: Response = requests.get(endpoint, headers={\"X-Api-Key\": api_key})\n    todos.raise_for_status()\n\n    # for brevity, we'll limit to the first 10 only\n    return {\"todos\": todos.json()[:10]}\n\n\n@router.get(\"/todos/&lt;todo_id&gt;\")\n@tracer.capture_method\ndef get_todo_by_id(todo_id: str):  # value come as str\napi_key: str = router.current_event.get_header_value(name=\"X-Api-Key\", case_sensitive=True, default_value=\"\")  # type: ignore[assignment] # sentinel typing # noqa: E501\ntodos: Response = requests.get(f\"{endpoint}/{todo_id}\", headers={\"X-Api-Key\": api_key})\n    todos.raise_for_status()\n\n    return {\"todos\": todos.json()}\n</code></pre> <p>We use <code>include_router</code> method and include all user routers registered in the <code>router</code> global object.</p> <pre><code>import split_route_module\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\napp.include_router(split_route_module.router)\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre>"},{"location":"core/event_handler/api_gateway/#route-prefix","title":"Route prefix","text":"<p>In the previous example, <code>split_route_module.py</code> routes had a <code>/todos</code> prefix. This might grow over time and become repetitive.</p> <p>When necessary, you can set a prefix when including a router object. This means you could remove <code>/todos</code> prefix altogether.</p> split_route_prefix.pysplit_route_prefix_module.py <pre><code>import split_route_module\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n# prefix '/todos' to any route in `split_route_module.router`\napp.include_router(split_route_module.router, prefix=\"/todos\")\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> <pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.event_handler.api_gateway import Router\n\ntracer = Tracer()\nrouter = Router()\n\nendpoint = \"https://jsonplaceholder.typicode.com/todos\"\n\n\n@router.get(\"/\")\n@tracer.capture_method\ndef get_todos():\n    api_key: str = router.current_event.get_header_value(name=\"X-Api-Key\", case_sensitive=True, default_value=\"\")\n\n    todos: Response = requests.get(endpoint, headers={\"X-Api-Key\": api_key})\n    todos.raise_for_status()\n\n    # for brevity, we'll limit to the first 10 only\n    return {\"todos\": todos.json()[:10]}\n\n\n@router.get(\"/&lt;todo_id&gt;\")\n@tracer.capture_method\ndef get_todo_by_id(todo_id: str):  # value come as str\n    api_key: str = router.current_event.get_header_value(name=\"X-Api-Key\", case_sensitive=True, default_value=\"\")  # type: ignore[assignment] # sentinel typing # noqa: E501\n\n    todos: Response = requests.get(f\"{endpoint}/{todo_id}\", headers={\"X-Api-Key\": api_key})\n    todos.raise_for_status()\n\n    return {\"todos\": todos.json()}\n\n\n# many more routes\n</code></pre>"},{"location":"core/event_handler/api_gateway/#specialized-router-types","title":"Specialized router types","text":"<p>You can use specialized router classes according to the type of event that you are resolving. This way you'll get type hints from your IDE as you access the <code>current_event</code> property.</p> Router Resolver <code>current_event</code> type APIGatewayRouter APIGatewayRestResolver APIGatewayProxyEvent APIGatewayHttpRouter APIGatewayHttpResolver APIGatewayProxyEventV2 ALBRouter ALBResolver ALBEvent LambdaFunctionUrlRouter LambdaFunctionUrlResolver LambdaFunctionUrlEvent <pre><code>from aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.event_handler.router import APIGatewayRouter\n\napp = APIGatewayRestResolver()\nrouter = APIGatewayRouter()\n@router.get(\"/me\")\ndef get_self():\n# router.current_event is a APIGatewayProxyEvent\n    account_id = router.current_event.request_context.account_id\n\n    return {\"account_id\": account_id}\n\n\napp.include_router(router)\n\n\ndef lambda_handler(event, context):\n    return app.resolve(event, context)\n</code></pre>"},{"location":"core/event_handler/api_gateway/#sharing-contextual-data","title":"Sharing contextual data","text":"<p>You can use <code>append_context</code> when you want to share data between your App and Router instances. Any data you share will be available via the <code>context</code> dictionary available in your App or Router context.</p> Info <p>For safety, we always clear any data available in the <code>context</code> dictionary after each invocation.</p> Tip <p>This can also be useful for middlewares injecting contextual information before a request is processed.</p> split_route_append_context.pysplit_route_append_context_module.py <pre><code>import split_route_append_context_module\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\napp.include_router(split_route_append_context_module.router)\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\napp.append_context(is_admin=True)  # arbitrary number of key=value data\nreturn app.resolve(event, context)\n</code></pre> <pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.event_handler.api_gateway import Router\n\ntracer = Tracer()\nrouter = Router()\n\nendpoint = \"https://jsonplaceholder.typicode.com/todos\"\n\n\n@router.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\nis_admin: bool = router.context.get(\"is_admin\", False)\ntodos = {}\n\n    if is_admin:\n        todos: Response = requests.get(endpoint)\n        todos.raise_for_status()\n        todos = todos.json()[:10]\n\n    # for brevity, we'll limit to the first 10 only\n    return {\"todos\": todos}\n</code></pre>"},{"location":"core/event_handler/api_gateway/#sample-layout","title":"Sample layout","text":"<p>This is a sample project layout for a monolithic function with routes split in different files (<code>/todos</code>, <code>/health</code>).</p> Sample project layout<pre><code>.\n\u251c\u2500\u2500 pyproject.toml            # project app &amp; dev dependencies; poetry, pipenv, etc.\n\u251c\u2500\u2500 poetry.lock\n\u251c\u2500\u2500 src\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 requirements.txt  # sam build detect it automatically due to CodeUri: src. poetry export --format src/requirements.txt\n\u2502       \u2514\u2500\u2500 todos\n\u2502           \u251c\u2500\u2500 __init__.py\n\u2502           \u251c\u2500\u2500 main.py       # this will be our todos Lambda fn; it could be split in folders if we want separate fns same code base\n\u2502           \u2514\u2500\u2500 routers       # routers module\n\u2502               \u251c\u2500\u2500 __init__.py\n\u2502               \u251c\u2500\u2500 health.py # /health routes. from routers import todos; health.router\n\u2502               \u2514\u2500\u2500 todos.py  # /todos routes. from .routers import todos; todos.router\n\u251c\u2500\u2500 template.yml              # SAM. CodeUri: src, Handler: todos.main.lambda_handler\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 unit\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2514\u2500\u2500 test_todos.py     # unit tests for the todos router\n\u2502   \u2514\u2500\u2500 test_health.py    # unit tests for the health router\n\u2514\u2500\u2500 functional\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 conftest.py       # pytest fixtures for the functional tests\n\u2514\u2500\u2500 test_main.py      # functional tests for the main lambda handler\n</code></pre>"},{"location":"core/event_handler/api_gateway/#considerations","title":"Considerations","text":"<p>This utility is optimized for fast startup, minimal feature set, and to quickly on-board customers familiar with frameworks like Flask \u2014 it's not meant to be a fully fledged framework.</p> <p>Event Handler naturally leads to a single Lambda function handling multiple routes for a given service, which can be eventually broken into multiple functions.</p> <p>Both single (monolithic) and multiple functions (micro) offer different set of trade-offs worth knowing.</p> Tip <p>TL;DR. Start with a monolithic function, add additional functions with new handlers, and possibly break into micro functions if necessary.</p>"},{"location":"core/event_handler/api_gateway/#monolithic-function","title":"Monolithic function","text":"<p>A monolithic function means that your final code artifact will be deployed to a single function. This is generally the best approach to start.</p> <p>Benefits</p> <ul> <li>Code reuse. It's easier to reason about your service, modularize it and reuse code as it grows. Eventually, it can be turned into a standalone library.</li> <li>No custom tooling. Monolithic functions are treated just like normal Python packages; no upfront investment in tooling.</li> <li>Faster deployment and debugging. Whether you use all-at-once, linear, or canary deployments, a monolithic function is a single deployable unit. IDEs like PyCharm and VSCode have tooling to quickly profile, visualize, and step through debug any Python package.</li> </ul> <p>Downsides</p> <ul> <li>Cold starts. Frequent deployments and/or high load can diminish the benefit of monolithic functions depending on your latency requirements, due to Lambda scaling model. Always load test to pragmatically balance between your customer experience and development cognitive load.</li> <li>Granular security permissions. The micro function approach enables you to use fine-grained permissions &amp; access controls, separate external dependencies &amp; code signing at the function level. Conversely, you could have multiple functions while duplicating the final code artifact in a monolithic approach.<ul> <li>Regardless, least privilege can be applied to either approaches.</li> </ul> </li> <li>Higher risk per deployment. A misconfiguration or invalid import can cause disruption if not caught earlier in automated testing. Multiple functions can mitigate misconfigurations but they would still share the same code artifact. You can further minimize risks with multiple environments in your CI/CD pipeline.</li> </ul>"},{"location":"core/event_handler/api_gateway/#micro-function","title":"Micro function","text":"<p>A micro function means that your final code artifact will be different to each function deployed. This is generally the approach to start if you're looking for fine-grain control and/or high load on certain parts of your service.</p> <p>Benefits</p> <ul> <li>Granular scaling. A micro function can benefit from the Lambda scaling model to scale differently depending on each part of your application. Concurrency controls and provisioned concurrency can also be used at a granular level for capacity management.</li> <li>Discoverability. Micro functions are easier do visualize when using distributed tracing. Their high-level architectures can be self-explanatory, and complexity is highly visible \u2014 assuming each function is named to the business purpose it serves.</li> <li>Package size. An independent function can be significant smaller (KB vs MB) depending on external dependencies it require to perform its purpose. Conversely, a monolithic approach can benefit from Lambda Layers to optimize builds for external dependencies.</li> </ul> <p>Downsides</p> <ul> <li>Upfront investment. You need custom build tooling to bundle assets, including C bindings for runtime compatibility. Operations become more elaborate \u2014 you need to standardize tracing labels/annotations, structured logging, and metrics to pinpoint root causes.<ul> <li>Engineering discipline is necessary for both approaches. Micro-function approach however requires further attention in consistency as the number of functions grow, just like any distributed system.</li> </ul> </li> <li>Harder to share code. Shared code must be carefully evaluated to avoid unnecessary deployments when that changes. Equally, if shared code isn't a library, your development, building, deployment tooling need to accommodate the distinct layout.</li> <li>Slower safe deployments. Safely deploying multiple functions require coordination \u2014 AWS CodeDeploy deploys and verifies each function sequentially. This increases lead time substantially (minutes to hours) depending on the deployment strategy you choose. You can mitigate it by selectively enabling it in prod-like environments only, and where the risk profile is applicable.<ul> <li>Automated testing, operational and security reviews are essential to stability in either approaches.</li> </ul> </li> </ul>"},{"location":"core/event_handler/api_gateway/#testing-your-code","title":"Testing your code","text":"<p>You can test your routes by passing a proxy event request with required params.</p> API Gateway REST APIAPI Gateway HTTP APIApplication Load BalancerLambda Function URL assert_rest_api_resolver_response.pyassert_rest_api_response_module.py <pre><code>from dataclasses import dataclass\n\nimport assert_rest_api_resolver_response\nimport pytest\n\n\n@pytest.fixture\ndef lambda_context():\n    @dataclass\n    class LambdaContext:\n        function_name: str = \"test\"\n        memory_limit_in_mb: int = 128\n        invoked_function_arn: str = \"arn:aws:lambda:eu-west-1:123456789012:function:test\"\n        aws_request_id: str = \"da658bd3-2d6f-4e7b-8ec2-937234644fdc\"\n\n    return LambdaContext()\n\n\ndef test_lambda_handler(lambda_context):\n    minimal_event = {\n\"path\": \"/todos\",\n\"httpMethod\": \"GET\",\n\"requestContext\": {\"requestId\": \"227b78aa-779d-47d4-a48e-ce62120393b8\"},  # correlation ID\n}\n# Example of API Gateway REST API request event:\n    # https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html#apigateway-example-event\n    ret = assert_rest_api_resolver_response.lambda_handler(minimal_event, lambda_context)\n    assert ret[\"statusCode\"] == 200\n    assert ret[\"body\"] != \"\"\n</code></pre> <pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayRestResolver()\n\n\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    todos: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    return {\"todos\": todos.json()[:10]}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> assert_http_api_resolver_response.pyassert_http_api_response_module.py <pre><code>from dataclasses import dataclass\n\nimport assert_http_api_response_module\nimport pytest\n\n\n@pytest.fixture\ndef lambda_context():\n    @dataclass\n    class LambdaContext:\n        function_name: str = \"test\"\n        memory_limit_in_mb: int = 128\n        invoked_function_arn: str = \"arn:aws:lambda:eu-west-1:123456789012:function:test\"\n        aws_request_id: str = \"da658bd3-2d6f-4e7b-8ec2-937234644fdc\"\n\n    return LambdaContext()\n\n\ndef test_lambda_handler(lambda_context):\n    minimal_event = {\n\"rawPath\": \"/todos\",\n\"requestContext\": {\n\"requestContext\": {\"requestId\": \"227b78aa-779d-47d4-a48e-ce62120393b8\"},  # correlation ID\n\"http\": {\n\"method\": \"GET\",\n},\n\"stage\": \"$default\",\n},\n}\n# Example of API Gateway HTTP API request event:\n    # https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-develop-integrations-lambda.html\n\n    ret = assert_http_api_response_module.lambda_handler(minimal_event, lambda_context)\n    assert ret[\"statusCode\"] == 200\n    assert ret[\"body\"] != \"\"\n</code></pre> <pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayHttpResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = APIGatewayHttpResolver()\n\n\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    todos: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    return {\"todos\": todos.json()[:10]}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_HTTP)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> assert_alb_api_resolver_response.pyassert_alb_api_response_module.py <pre><code>from dataclasses import dataclass\n\nimport assert_alb_api_response_module\nimport pytest\n\n\n@pytest.fixture\ndef lambda_context():\n    @dataclass\n    class LambdaContext:\n        function_name: str = \"test\"\n        memory_limit_in_mb: int = 128\n        invoked_function_arn: str = \"arn:aws:lambda:eu-west-1:123456789012:function:test\"\n        aws_request_id: str = \"da658bd3-2d6f-4e7b-8ec2-937234644fdc\"\n\n    return LambdaContext()\n\n\ndef test_lambda_handler(lambda_context):\n    minimal_event = {\n\"path\": \"/todos\",\n\"httpMethod\": \"GET\",\n\"headers\": {\"x-amzn-trace-id\": \"b25827e5-0e30-4d52-85a8-4df449ee4c5a\"},\n}\n# Example of Application Load Balancer request event:\n    # https://docs.aws.amazon.com/lambda/latest/dg/services-alb.html\n\n    ret = assert_alb_api_response_module.lambda_handler(minimal_event, lambda_context)\n    assert ret[\"statusCode\"] == 200\n    assert ret[\"body\"] != \"\"\n</code></pre> <pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import ALBResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = ALBResolver()\n\n\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    todos: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    return {\"todos\": todos.json()[:10]}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPLICATION_LOAD_BALANCER)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> assert_function_url_api_resolver_response.pyassert_function_url_api_response_module.py <pre><code>from dataclasses import dataclass\n\nimport assert_function_url_api_response_module\nimport pytest\n\n\n@pytest.fixture\ndef lambda_context():\n    @dataclass\n    class LambdaContext:\n        function_name: str = \"test\"\n        memory_limit_in_mb: int = 128\n        invoked_function_arn: str = \"arn:aws:lambda:eu-west-1:123456789012:function:test\"\n        aws_request_id: str = \"da658bd3-2d6f-4e7b-8ec2-937234644fdc\"\n\n    return LambdaContext()\n\n\ndef test_lambda_handler(lambda_context):\n    minimal_event = {\n\"rawPath\": \"/todos\",\n\"requestContext\": {\n\"requestContext\": {\"requestId\": \"227b78aa-779d-47d4-a48e-ce62120393b8\"},  # correlation ID\n\"http\": {\n\"method\": \"GET\",\n},\n\"stage\": \"$default\",\n},\n}\n# Example of Lambda Function URL request event:\n    # https://docs.aws.amazon.com/lambda/latest/dg/urls-invocation.html#urls-payloads\n\n    ret = assert_function_url_api_response_module.lambda_handler(minimal_event, lambda_context)\n    assert ret[\"statusCode\"] == 200\n    assert ret[\"body\"] != \"\"\n</code></pre> <pre><code>import requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import LambdaFunctionUrlResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = LambdaFunctionUrlResolver()\n\n\n@app.get(\"/todos\")\n@tracer.capture_method\ndef get_todos():\n    todos: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    return {\"todos\": todos.json()[:10]}\n\n\n# You can continue to use other utilities just as before\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.LAMBDA_FUNCTION_URL)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre>"},{"location":"core/event_handler/api_gateway/#faq","title":"FAQ","text":"<p>What's the difference between this utility and frameworks like Chalice?</p> <p>Chalice is a full featured microframework that manages application and infrastructure. This utility, however, is largely focused on routing to reduce boilerplate and expects you to setup and manage infrastructure with your framework of choice.</p> <p>That said, Chalice has native integration with Lambda Powertools if you're looking for a more opinionated and web framework feature set.</p> <p>What happened to <code>ApiGatewayResolver</code>?</p> <p>It's been superseded by more explicit resolvers like <code>APIGatewayRestResolver</code>, <code>APIGatewayHttpResolver</code>, and <code>ALBResolver</code>.</p> <p><code>ApiGatewayResolver</code> handled multiple types of event resolvers for convenience via <code>proxy_type</code> param. However, it made it impossible for static checkers like Mypy and IDEs IntelliSense to know what properties a <code>current_event</code> would have due to late bound resolution.</p> <p>This provided a suboptimal experience for customers not being able to find all properties available besides common ones between API Gateway REST, HTTP, and ALB - while manually annotating <code>app.current_event</code> would work it is not the experience we want to provide to customers.</p> <p><code>ApiGatewayResolver</code> will be deprecated in v2 and have appropriate warnings as soon as we have a v2 draft.</p>"},{"location":"core/event_handler/appsync/","title":"GraphQL API","text":"<p>Event handler for AWS AppSync Direct Lambda Resolver and Amplify GraphQL Transformer.</p>"},{"location":"core/event_handler/appsync/#key-features","title":"Key Features","text":"<ul> <li>Automatically parse API arguments to function arguments</li> <li>Choose between strictly match a GraphQL field name or all of them to a function</li> <li>Integrates with Data classes utilities to access resolver and identity information</li> <li>Works with both Direct Lambda Resolver and Amplify GraphQL Transformer <code>@function</code> directive</li> <li>Support async Python 3.8+ functions, and generators</li> </ul>"},{"location":"core/event_handler/appsync/#terminology","title":"Terminology","text":"<p>Direct Lambda Resolver. A custom AppSync Resolver to bypass the use of Apache Velocity Template (VTL) and automatically map your function's response to a GraphQL field.</p> <p>Amplify GraphQL Transformer. Custom GraphQL directives to define your application's data model using Schema Definition Language (SDL). Amplify CLI uses these directives to convert GraphQL SDL into full descriptive AWS CloudFormation templates.</p>"},{"location":"core/event_handler/appsync/#getting-started","title":"Getting started","text":""},{"location":"core/event_handler/appsync/#required-resources","title":"Required resources","text":"<p>You must have an existing AppSync GraphQL API and IAM permissions to invoke your Lambda function. That said, there is no additional permissions to use this utility.</p> <p>This is the sample infrastructure we are using for the initial examples with a AppSync Direct Lambda Resolver.</p> Tip: Designing GraphQL Schemas for the first time? <p>Visit AWS AppSync schema documentation for understanding how to define types, nesting, and pagination.</p> getting_started_schema.graphqltemplate.yml <pre><code>schema {\nquery: Query\nmutation: Mutation\n}\n\ntype Query {\n# these are fields you can attach resolvers to (field: Query, field: getTodo)\ngetTodo(id: ID!): Todo\nlistTodos: [Todo]\n}\n\ntype Mutation {\ncreateTodo(title: String!): Todo\n}\n\ntype Todo {\nid: ID!\nuserId: String\ntitle: String\ncompleted: Boolean\n}\n</code></pre> <pre><code>AWSTemplateFormatVersion: \"2010-09-09\"\nTransform: AWS::Serverless-2016-10-31\nDescription: Hello world Direct Lambda Resolver\n\nGlobals:\nFunction:\nTimeout: 5\nRuntime: python3.9\nTracing: Active\nEnvironment:\nVariables:\n# Powertools env vars: https://awslabs.github.io/aws-lambda-powertools-python/latest/#environment-variables\nLOG_LEVEL: INFO\nPOWERTOOLS_LOGGER_SAMPLE_RATE: 0.1\nPOWERTOOLS_LOGGER_LOG_EVENT: true\nPOWERTOOLS_SERVICE_NAME: example\n\nResources:\nTodosFunction:\nType: AWS::Serverless::Function\nProperties:\nHandler: getting_started_graphql_api_resolver.lambda_handler\nCodeUri: ../src\nDescription: Sample Direct Lambda Resolver\n\n# IAM Permissions and Roles\n\nAppSyncServiceRole:\nType: \"AWS::IAM::Role\"\nProperties:\nAssumeRolePolicyDocument:\nVersion: \"2012-10-17\"\nStatement:\n- Effect: \"Allow\"\nPrincipal:\nService:\n- \"appsync.amazonaws.com\"\nAction:\n- \"sts:AssumeRole\"\n\nInvokeLambdaResolverPolicy:\nType: \"AWS::IAM::Policy\"\nProperties:\nPolicyName: \"DirectAppSyncLambda\"\nPolicyDocument:\nVersion: \"2012-10-17\"\nStatement:\n- Effect: \"Allow\"\nAction: \"lambda:invokeFunction\"\nResource:\n- !GetAtt TodosFunction.Arn\nRoles:\n- !Ref AppSyncServiceRole\n\n# GraphQL API\n\nTodosApi:\nType: \"AWS::AppSync::GraphQLApi\"\nProperties:\nName: TodosApi\nAuthenticationType: \"API_KEY\"\nXrayEnabled: true\n\nTodosApiKey:\nType: AWS::AppSync::ApiKey\nProperties:\nApiId: !GetAtt TodosApi.ApiId\n\nTodosApiSchema:\nType: \"AWS::AppSync::GraphQLSchema\"\nProperties:\nApiId: !GetAtt TodosApi.ApiId\nDefinitionS3Location: ../src/getting_started_schema.graphql\nMetadata:\ncfn-lint:\nconfig:\nignore_checks:\n- W3002  # allow relative path in DefinitionS3Location\n\n# Lambda Direct Data Source and Resolver\n\nTodosFunctionDataSource:\nType: \"AWS::AppSync::DataSource\"\nProperties:\nApiId: !GetAtt TodosApi.ApiId\nName: \"HelloWorldLambdaDirectResolver\"\nType: \"AWS_LAMBDA\"\nServiceRoleArn: !GetAtt AppSyncServiceRole.Arn\nLambdaConfig:\nLambdaFunctionArn: !GetAtt TodosFunction.Arn\n\nListTodosResolver:\nType: \"AWS::AppSync::Resolver\"\nProperties:\nApiId: !GetAtt TodosApi.ApiId\nTypeName: \"Query\"\nFieldName: \"listTodos\"\nDataSourceName: !GetAtt TodosFunctionDataSource.Name\n\nGetTodoResolver:\nType: \"AWS::AppSync::Resolver\"\nProperties:\nApiId: !GetAtt TodosApi.ApiId\nTypeName: \"Query\"\nFieldName: \"getTodo\"\nDataSourceName: !GetAtt TodosFunctionDataSource.Name\n\nCreateTodoResolver:\nType: \"AWS::AppSync::Resolver\"\nProperties:\nApiId: !GetAtt TodosApi.ApiId\nTypeName: \"Mutation\"\nFieldName: \"createTodo\"\nDataSourceName: !GetAtt TodosFunctionDataSource.Name\n\nOutputs:\nTodosFunction:\nDescription: \"Hello World Lambda Function ARN\"\nValue: !GetAtt TodosFunction.Arn\n\nTodosApi:\nValue: !GetAtt TodosApi.GraphQLUrl\n</code></pre>"},{"location":"core/event_handler/appsync/#resolver-decorator","title":"Resolver decorator","text":"<p>You can define your functions to match GraphQL types and fields with the <code>app.resolver()</code> decorator.</p> What is a type and field? <p>A type would be a top-level GraphQL Type like <code>Query</code>, <code>Mutation</code>, <code>Todo</code>. A GraphQL Field would be <code>listTodos</code> under <code>Query</code>, <code>createTodo</code> under <code>Mutation</code>, etc.</p> <p>Here's an example with two separate functions to resolve <code>getTodo</code> and <code>listTodos</code> fields within the <code>Query</code> type. For completion, we use Scalar type utilities to generate the right output based on our schema definition.</p> Important <p>GraphQL arguments are passed as function keyword arguments.</p> <p>Example</p> <p>The GraphQL Query <code>getTodo(id: \"todo_id_value\")</code> will call <code>get_todo</code> as <code>get_todo(id=\"todo_id_value\")</code>.</p> getting_started_graphql_api_resolver.pygetting_started_schema.graphqlsample events <pre><code>import sys\n\nif sys.version_info &gt;= (3, 8):\n    from typing import TypedDict\nelse:\n    from typing_extensions import TypedDict\n\nfrom typing import List\n\nimport requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import AppSyncResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.data_classes.appsync import scalar_types_utils\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = AppSyncResolver()\nclass Todo(TypedDict, total=False):\n    id: str  # noqa AA03 VNE003, required due to GraphQL Schema\n    userId: str\n    title: str\n    completed: bool\n\n\n@app.resolver(type_name=\"Query\", field_name=\"getTodo\")\n@tracer.capture_method\ndef get_todo(\nid: str = \"\",  # noqa AA03 VNE003 shadows built-in id to match query argument, e.g., getTodo(id: \"some_id\")\n) -&gt; Todo:\n    logger.info(f\"Fetching Todo {id}\")\n    todos: Response = requests.get(f\"https://jsonplaceholder.typicode.com/todos/{id}\")\n    todos.raise_for_status()\n\n    return todos.json()\n\n\n@app.resolver(type_name=\"Query\", field_name=\"listTodos\")\n@tracer.capture_method\ndef list_todos() -&gt; List[Todo]:\ntodos: Response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n    todos.raise_for_status()\n\n    # for brevity, we'll limit to the first 10 only\n    return todos.json()[:10]\n\n\n@app.resolver(type_name=\"Mutation\", field_name=\"createTodo\")\n@tracer.capture_method\ndef create_todo(title: str) -&gt; Todo:\npayload = {\"userId\": scalar_types_utils.make_id(), \"title\": title, \"completed\": False}  # dummy UUID str\n    todo: Response = requests.post(\"https://jsonplaceholder.typicode.com/todos\", json=payload)\n    todo.raise_for_status()\n\n    return todo.json()\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPSYNC_RESOLVER)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\nreturn app.resolve(event, context)\n</code></pre> <pre><code>schema {\nquery: Query\nmutation: Mutation\n}\n\ntype Query {\n# these are fields you can attach resolvers to (field: Query, field: getTodo)\ngetTodo(id: ID!): Todo\nlistTodos: [Todo]\n}\n\ntype Mutation {\ncreateTodo(title: String!): Todo\n}\n\ntype Todo {\nid: ID!\nuserId: String\ntitle: String\ncompleted: Boolean\n}\n</code></pre> getting_started_get_todo.jsongetting_started_list_todos.jsongetting_started_create_todo.json <pre><code>{\n\"arguments\": {\n\"id\": \"7e362732-c8cd-4405-b090-144ac9b38960\"\n},\n\"identity\": null,\n\"source\": null,\n\"request\": {\n\"headers\": {\n\"x-forwarded-for\": \"1.2.3.4, 5.6.7.8\",\n\"accept-encoding\": \"gzip, deflate, br\",\n\"cloudfront-viewer-country\": \"NL\",\n\"cloudfront-is-tablet-viewer\": \"false\",\n\"referer\": \"https://eu-west-1.console.aws.amazon.com/appsync/home?region=eu-west-1\",\n\"via\": \"2.0 9fce949f3749407c8e6a75087e168b47.cloudfront.net (CloudFront)\",\n\"cloudfront-forwarded-proto\": \"https\",\n\"origin\": \"https://eu-west-1.console.aws.amazon.com\",\n\"x-api-key\": \"da1-c33ullkbkze3jg5hf5ddgcs4fq\",\n\"content-type\": \"application/json\",\n\"x-amzn-trace-id\": \"Root=1-606eb2f2-1babc433453a332c43fb4494\",\n\"x-amz-cf-id\": \"SJw16ZOPuMZMINx5Xcxa9pB84oMPSGCzNOfrbJLvd80sPa0waCXzYQ==\",\n\"content-length\": \"114\",\n\"x-amz-user-agent\": \"AWS-Console-AppSync/\",\n\"x-forwarded-proto\": \"https\",\n\"host\": \"ldcvmkdnd5az3lm3gnf5ixvcyy.appsync-api.eu-west-1.amazonaws.com\",\n\"accept-language\": \"en-US,en;q=0.5\",\n\"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n\"cloudfront-is-desktop-viewer\": \"true\",\n\"cloudfront-is-mobile-viewer\": \"false\",\n\"accept\": \"*/*\",\n\"x-forwarded-port\": \"443\",\n\"cloudfront-is-smarttv-viewer\": \"false\"\n}\n},\n\"prev\": null,\n\"info\": {\n\"parentTypeName\": \"Query\",\n\"selectionSetList\": [\n\"title\",\n\"id\"\n],\n\"selectionSetGraphQL\": \"{\\n  title\\n  id\\n}\",\n\"fieldName\": \"getTodo\",\n\"variables\": {}\n},\n\"stash\": {}\n}\n</code></pre> <pre><code>{\n\"arguments\": {},\n\"identity\": null,\n\"source\": null,\n\"request\": {\n\"headers\": {\n\"x-forwarded-for\": \"1.2.3.4, 5.6.7.8\",\n\"accept-encoding\": \"gzip, deflate, br\",\n\"cloudfront-viewer-country\": \"NL\",\n\"cloudfront-is-tablet-viewer\": \"false\",\n\"referer\": \"https://eu-west-1.console.aws.amazon.com/appsync/home?region=eu-west-1\",\n\"via\": \"2.0 9fce949f3749407c8e6a75087e168b47.cloudfront.net (CloudFront)\",\n\"cloudfront-forwarded-proto\": \"https\",\n\"origin\": \"https://eu-west-1.console.aws.amazon.com\",\n\"x-api-key\": \"da1-c33ullkbkze3jg5hf5ddgcs4fq\",\n\"content-type\": \"application/json\",\n\"x-amzn-trace-id\": \"Root=1-606eb2f2-1babc433453a332c43fb4494\",\n\"x-amz-cf-id\": \"SJw16ZOPuMZMINx5Xcxa9pB84oMPSGCzNOfrbJLvd80sPa0waCXzYQ==\",\n\"content-length\": \"114\",\n\"x-amz-user-agent\": \"AWS-Console-AppSync/\",\n\"x-forwarded-proto\": \"https\",\n\"host\": \"ldcvmkdnd5az3lm3gnf5ixvcyy.appsync-api.eu-west-1.amazonaws.com\",\n\"accept-language\": \"en-US,en;q=0.5\",\n\"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0\",\n\"cloudfront-is-desktop-viewer\": \"true\",\n\"cloudfront-is-mobile-viewer\": \"false\",\n\"accept\": \"*/*\",\n\"x-forwarded-port\": \"443\",\n\"cloudfront-is-smarttv-viewer\": \"false\"\n}\n},\n\"prev\": null,\n\"info\": {\n\"parentTypeName\": \"Query\",\n\"selectionSetList\": [\n\"id\",\n\"title\"\n],\n\"selectionSetGraphQL\": \"{\\n  id\\n  title\\n}\",\n\"fieldName\": \"listTodos\",\n\"variables\": {}\n},\n\"stash\": {}\n}\n</code></pre> <pre><code> {\n\"arguments\": {\n\"title\": \"Sample todo mutation\"\n},\n\"identity\": null,\n\"source\": null,\n\"request\": {\n\"headers\": {\n\"x-forwarded-for\": \"203.0.113.1, 203.0.113.18\",\n\"cloudfront-viewer-country\": \"NL\",\n\"cloudfront-is-tablet-viewer\": \"false\",\n\"x-amzn-requestid\": \"fdc4f30b-44c2-475d-b2f9-9da0778d5275\",\n\"via\": \"2.0 f655cacd0d6f7c5dc935ea687af6f3c0.cloudfront.net (CloudFront)\",\n\"cloudfront-forwarded-proto\": \"https\",\n\"origin\": \"https://eu-west-1.console.aws.amazon.com\",\n\"content-length\": \"166\",\n\"x-forwarded-proto\": \"https\",\n\"accept-language\": \"en-US,en;q=0.5\",\n\"host\": \"kiuqayvn4jhhzio6whpnk7xj3a.appsync-api.eu-west-1.amazonaws.com\",\n\"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:102.0) Gecko/20100101 Firefox/102.0\",\n\"cloudfront-is-mobile-viewer\": \"false\",\n\"accept\": \"application/json, text/plain, */*\",\n\"cloudfront-viewer-asn\": \"1136\",\n\"cloudfront-is-smarttv-viewer\": \"false\",\n\"accept-encoding\": \"gzip, deflate, br\",\n\"referer\": \"https://eu-west-1.console.aws.amazon.com/\",\n\"content-type\": \"application/json\",\n\"x-api-key\": \"da2-vsqnxwyzgzf4nh6kvoaidtvs7y\",\n\"sec-fetch-mode\": \"cors\",\n\"x-amz-cf-id\": \"0kxqijFPsbGSWJ1u3Z_sUS4Wu2hRoG_2T77aJPuoh_Q4bXAB3x0a3g==\",\n\"x-amzn-trace-id\": \"Root=1-63fef2cf-6d566e9f4a35b99e6212388e\",\n\"sec-fetch-dest\": \"empty\",\n\"x-amz-user-agent\": \"AWS-Console-AppSync/\",\n\"cloudfront-is-desktop-viewer\": \"true\",\n\"sec-fetch-site\": \"cross-site\",\n\"x-forwarded-port\": \"443\"\n},\n\"domainName\": null\n},\n\"prev\": null,\n\"info\": {\n\"selectionSetList\": [\n\"id\",\n\"title\",\n\"completed\"\n],\n\"selectionSetGraphQL\": \"{\\n  id\\n  title\\n  completed\\n}\",\n\"fieldName\": \"createTodo\",\n\"parentTypeName\": \"Mutation\",\n\"variables\": {}\n},\n\"stash\": {}\n}\n</code></pre>"},{"location":"core/event_handler/appsync/#scalar-functions","title":"Scalar functions","text":"<p>When working with AWS AppSync Scalar types, you might want to generate the same values for data validation purposes.</p> <p>For convenience, the most commonly used values are available as functions within <code>scalar_types_utils</code> module.</p> Creating key scalar values with scalar_types_utils<pre><code>from aws_lambda_powertools.utilities.data_classes.appsync.scalar_types_utils import (\naws_date,\naws_datetime,\naws_time,\naws_timestamp,\nmake_id,\n)\n\n# Scalars: https://docs.aws.amazon.com/appsync/latest/devguide/scalars.html\n\nmy_id: str = make_id()  # Scalar: ID!\nmy_date: str = aws_date()  # Scalar: AWSDate\nmy_timestamp: str = aws_time()  # Scalar: AWSTime\nmy_datetime: str = aws_datetime()  # Scalar: AWSDateTime\nmy_epoch_timestamp: int = aws_timestamp()  # Scalar: AWSTimestamp\n</code></pre> <p>Here's a table with their related scalar as a quick reference:</p> Scalar type Scalar function Sample value ID <code>scalar_types_utils.make_id</code> <code>e916c84d-48b6-484c-bef3-cee3e4d86ebf</code> AWSDate <code>scalar_types_utils.aws_date</code> <code>2022-07-08Z</code> AWSTime <code>scalar_types_utils.aws_time</code> <code>15:11:00.189Z</code> AWSDateTime <code>scalar_types_utils.aws_datetime</code> <code>2022-07-08T15:11:00.189Z</code> AWSTimestamp <code>scalar_types_utils.aws_timestamp</code> <code>1657293060</code>"},{"location":"core/event_handler/appsync/#advanced","title":"Advanced","text":""},{"location":"core/event_handler/appsync/#nested-mappings","title":"Nested mappings","text":"Note <p>The following examples use a more advanced schema. These schemas differ from initial sample infrastructure we used earlier.</p> <p>You can nest <code>app.resolver()</code> decorator multiple times when resolving fields with the same return value.</p> nested_mappings.pynested_mappings_schema.graphql <pre><code>import sys\n\nif sys.version_info &gt;= (3, 8):\n    from typing import TypedDict\nelse:\n    from typing_extensions import TypedDict\n\nfrom typing import List\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import AppSyncResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = AppSyncResolver()\nclass Location(TypedDict, total=False):\n    id: str  # noqa AA03 VNE003, required due to GraphQL Schema\n    name: str\n    description: str\n    address: str\n\n\n@app.resolver(field_name=\"listLocations\")\n@app.resolver(field_name=\"locations\")\n@tracer.capture_method\ndef get_locations(name: str, description: str = \"\") -&gt; List[Location]:  # match GraphQL Query arguments\nreturn [{\"name\": name, \"description\": description}]\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPSYNC_RESOLVER)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\nreturn app.resolve(event, context)\n</code></pre> <pre><code>schema {\nquery: Query\n}\n\ntype Query {\nlistLocations: [Location]\n}\n\ntype Location {\nid: ID!\nname: String!\ndescription: String\naddress: String\n}\n\ntype Merchant {\nid: String!\nname: String!\ndescription: String\nlocations: [Location]\n}\n</code></pre>"},{"location":"core/event_handler/appsync/#async-functions","title":"Async functions","text":"<p>For Lambda Python3.8+ runtime, this utility supports async functions when you use in conjunction with <code>asyncio.run</code>.</p> Resolving GraphQL resolvers async<pre><code>import asyncio\nimport sys\n\nif sys.version_info &gt;= (3, 8):\n    from typing import TypedDict\nelse:\n    from typing_extensions import TypedDict\n\nfrom typing import List\n\nimport aiohttp\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import AppSyncResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.tracing import aiohttp_trace_config\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = AppSyncResolver()\nclass Todo(TypedDict, total=False):\n    id: str  # noqa AA03 VNE003, required due to GraphQL Schema\n    userId: str\n    title: str\n    completed: bool\n\n\n@app.resolver(type_name=\"Query\", field_name=\"listTodos\")\nasync def list_todos() -&gt; List[Todo]:\nasync with aiohttp.ClientSession(trace_configs=[aiohttp_trace_config()]) as session:\n        async with session.get(\"https://jsonplaceholder.typicode.com/todos\") as resp:\n            return await resp.json()\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPSYNC_RESOLVER)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\nresult = app.resolve(event, context)\nreturn asyncio.run(result)\n</code></pre>"},{"location":"core/event_handler/appsync/#amplify-graphql-transformer","title":"Amplify GraphQL Transformer","text":"<p>Assuming you have Amplify CLI installed, create a new API using <code>amplify add api</code> and use the following GraphQL Schema.</p> Example GraphQL Schema<pre><code>@model\ntype Merchant {\nid: String!\nname: String!\ndescription: String\n# Resolves to `common_field`\ncommonField: String  @function(name: \"merchantInfo-${env}\")\n}\n\ntype Location {\nid: ID!\nname: String!\naddress: String\n# Resolves to `common_field`\ncommonField: String  @function(name: \"merchantInfo-${env}\")\n}\n\ntype Query {\n# List of locations resolves to `list_locations`\nlistLocations(page: Int, size: Int): [Location] @function(name: \"merchantInfo-${env}\")\n# List of locations resolves to `list_locations`\nfindMerchant(search: str): [Merchant] @function(name: \"searchMerchant-${env}\")\n}\n</code></pre> <p>Create two new basic Python functions via <code>amplify add function</code>.</p> Note <p>Amplify CLI generated functions use <code>Pipenv</code> as a dependency manager. Your function source code is located at <code>amplify/backend/function/your-function-name</code>.</p> <p>Within your function's folder, add Powertools as a dependency with <code>pipenv install aws-lambda-powertools</code>.</p> <p>Use the following code for <code>merchantInfo</code> and <code>searchMerchant</code> functions respectively.</p> graphql_transformer_merchant_info.pygraphql_transformer_search_merchant.pygraphql_transformer_list_locations.jsongraphql_transformer_common_field.jsongraphql_transformer_find_merchant.json <pre><code>import sys\n\nif sys.version_info &gt;= (3, 8):\n    from typing import TypedDict\nelse:\n    from typing_extensions import TypedDict\n\nfrom typing import List\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import AppSyncResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.data_classes.appsync import scalar_types_utils\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = AppSyncResolver()\n\n\nclass Location(TypedDict, total=False):\n    id: str  # noqa AA03 VNE003, required due to GraphQL Schema\n    name: str\n    description: str\n    address: str\n    commonField: str\n\n\n@app.resolver(type_name=\"Query\", field_name=\"listLocations\")\ndef list_locations(page: int = 0, size: int = 10) -&gt; List[Location]:\nreturn [{\"id\": scalar_types_utils.make_id(), \"name\": \"Smooth Grooves\"}]\n\n\n@app.resolver(field_name=\"commonField\")\ndef common_field() -&gt; str:\n# Would match all fieldNames matching 'commonField'\n    return scalar_types_utils.make_id()\n\n\n@tracer.capture_lambda_handler\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPSYNC_RESOLVER)\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\nreturn app.resolve(event, context)\n</code></pre> <pre><code>import sys\n\nif sys.version_info &gt;= (3, 8):\n    from typing import TypedDict\nelse:\n    from typing_extensions import TypedDict\n\nfrom typing import List\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import AppSyncResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.data_classes.appsync import scalar_types_utils\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\napp = AppSyncResolver()\ntracer = Tracer()\nlogger = Logger()\n\n\nclass Merchant(TypedDict, total=False):\n    id: str  # noqa AA03 VNE003, required due to GraphQL Schema\n    name: str\n    description: str\n    commonField: str\n\n\n@app.resolver(type_name=\"Query\", field_name=\"findMerchant\")\ndef find_merchant(search: str) -&gt; List[Merchant]:\nmerchants: List[Merchant] = [\n        {\n            \"id\": scalar_types_utils.make_id(),\n            \"name\": \"Parry-Wood\",\n            \"description\": \"Possimus doloremque tempora harum deleniti eum.\",\n        },\n        {\n            \"id\": scalar_types_utils.make_id(),\n            \"name\": \"Shaw, Owen and Jones\",\n            \"description\": \"Aliquam iste architecto suscipit in.\",\n        },\n    ]\n\nreturn [merchant for merchant in merchants if search == merchant[\"name\"]]\n@tracer.capture_lambda_handler\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPSYNC_RESOLVER)\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\nreturn app.resolve(event, context)\n</code></pre> <pre><code>{\n\"typeName\": \"Query\",\n\"fieldName\": \"listLocations\",\n\"arguments\": {\n\"page\": 2,\n\"size\": 1\n},\n\"identity\": {\n\"claims\": {\n\"iat\": 1615366261\n},\n\"username\": \"treid\"\n},\n\"request\": {\n\"headers\": {\n\"x-amzn-trace-id\": \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\",\n\"x-forwarded-for\": \"127.0.0.1\",\n\"cloudfront-viewer-country\": \"NL\",\n\"x-api-key\": \"da1-c33ullkbkze3jg5hf5ddgcs4fq\"\n}\n}\n}\n</code></pre> <pre><code>{\n\"typeName\": \"Merchant\",\n\"fieldName\": \"commonField\",\n\"arguments\": {},\n\"identity\": {\n\"claims\": {\n\"iat\": 1615366261\n},\n\"username\": \"marieellis\"\n},\n\"request\": {\n\"headers\": {\n\"x-amzn-trace-id\": \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\",\n\"x-forwarded-for\": \"127.0.0.1\"\n}\n},\n}\n</code></pre> <pre><code>{\n\"typeName\": \"Query\",\n\"fieldName\": \"findMerchant\",\n\"arguments\": {\n\"search\": \"Parry-Wood\"\n},\n\"identity\": {\n\"claims\": {\n\"iat\": 1615366261\n},\n\"username\": \"wwilliams\"\n},\n\"request\": {\n\"headers\": {\n\"x-amzn-trace-id\": \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\",\n\"x-forwarded-for\": \"127.0.0.1\"\n}\n},\n}\n</code></pre>"},{"location":"core/event_handler/appsync/#custom-data-models","title":"Custom data models","text":"<p>You can subclass AppSyncResolverEvent to bring your own set of methods to handle incoming events, by using <code>data_model</code> param in the <code>resolve</code> method.</p> custom_models.py.pynested_mappings_schema.graphqlgraphql_transformer_list_locations.json <pre><code>import sys\n\nif sys.version_info &gt;= (3, 8):\n    from typing import TypedDict\nelse:\n    from typing_extensions import TypedDict\n\nfrom typing import List\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import AppSyncResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.data_classes.appsync import scalar_types_utils\nfrom aws_lambda_powertools.utilities.data_classes.appsync_resolver_event import (\nAppSyncResolverEvent,\n)\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = AppSyncResolver()\n\n\nclass Location(TypedDict, total=False):\n    id: str  # noqa AA03 VNE003, required due to GraphQL Schema\n    name: str\n    description: str\n    address: str\n    commonField: str\n\n\nclass MyCustomModel(AppSyncResolverEvent):\n@property\ndef country_viewer(self) -&gt; str:\nreturn self.get_header_value(name=\"cloudfront-viewer-country\", default_value=\"\", case_sensitive=False)  # type: ignore[return-value] # sentinel typing # noqa: E501\n\n@property\ndef api_key(self) -&gt; str:\nreturn self.get_header_value(name=\"x-api-key\", default_value=\"\", case_sensitive=False)  # type: ignore[return-value] # sentinel typing # noqa: E501\n\n\n@app.resolver(type_name=\"Query\", field_name=\"listLocations\")\ndef list_locations(page: int = 0, size: int = 10) -&gt; List[Location]:\n    # additional properties/methods will now be available under current_event\nlogger.debug(f\"Request country origin: {app.current_event.country_viewer}\")  # type: ignore[attr-defined]\nreturn [{\"id\": scalar_types_utils.make_id(), \"name\": \"Perry, James and Carroll\"}]\n\n\n@tracer.capture_lambda_handler\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPSYNC_RESOLVER)\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\nreturn app.resolve(event, context, data_model=MyCustomModel)\n</code></pre> <pre><code>schema {\nquery: Query\n}\n\ntype Query {\nlistLocations: [Location]\n}\n\ntype Location {\nid: ID!\nname: String!\ndescription: String\naddress: String\n}\n\ntype Merchant {\nid: String!\nname: String!\ndescription: String\nlocations: [Location]\n}\n</code></pre> <pre><code> {\n\"typeName\": \"Query\",\n\"fieldName\": \"listLocations\",\n\"arguments\": {\n\"page\": 2,\n\"size\": 1\n},\n\"identity\": {\n\"claims\": {\n\"iat\": 1615366261\n},\n\"username\": \"treid\"\n},\n\"request\": {\n\"headers\": {\n\"x-amzn-trace-id\": \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\",\n\"x-forwarded-for\": \"127.0.0.1\",\n\"cloudfront-viewer-country\": \"NL\",\n\"x-api-key\": \"da1-c33ullkbkze3jg5hf5ddgcs4fq\"\n}\n}\n}\n</code></pre>"},{"location":"core/event_handler/appsync/#split-operations-with-router","title":"Split operations with Router","text":"Tip <p>Read the considerations section for trade-offs between monolithic and micro functions, as it's also applicable here.</p> <p>As you grow the number of related GraphQL operations a given Lambda function should handle, it is natural to split them into separate files to ease maintenance - That's when the <code>Router</code> feature comes handy.</p> <p>Let's assume you have <code>split_operation.py</code> as your Lambda function entrypoint and routes in <code>split_operation_module.py</code>. This is how you'd use the <code>Router</code> feature.</p> split_operation_module.pysplit_operation.py <p>We import Router instead of AppSyncResolver; syntax wise is exactly the same.</p> <pre><code>import sys\n\nif sys.version_info &gt;= (3, 8):\n    from typing import TypedDict\nelse:\n    from typing_extensions import TypedDict\n\nfrom typing import List\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler.appsync import Router\ntracer = Tracer()\nlogger = Logger()\nrouter = Router()\nclass Location(TypedDict, total=False):\n    id: str  # noqa AA03 VNE003, required due to GraphQL Schema\n    name: str\n    description: str\n    address: str\n\n\n@router.resolver(field_name=\"listLocations\")\n@router.resolver(field_name=\"locations\")\n@tracer.capture_method\ndef get_locations(name: str, description: str = \"\") -&gt; List[Location]:  # match GraphQL Query arguments\n    return [{\"name\": name, \"description\": description}]\n</code></pre> <p>We use <code>include_router</code> method and include all <code>location</code> operations registered in the <code>router</code> global object.</p> <pre><code>import split_operation_module\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import AppSyncResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = AppSyncResolver()\napp.include_router(split_operation_module.router)\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPSYNC_RESOLVER)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre>"},{"location":"core/event_handler/appsync/#sharing-contextual-data","title":"Sharing contextual data","text":"<p>You can use <code>append_context</code> when you want to share data between your App and Router instances. Any data you share will be available via the <code>context</code> dictionary available in your App or Router context.</p> Info <p>For safety, we always clear any data available in the <code>context</code> dictionary after each invocation.</p> Tip <p>This can also be useful for middlewares injecting contextual information before a request is processed.</p> split_route_append_context.pysplit_route_append_context_module.py <pre><code>import split_operation_append_context_module\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import AppSyncResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = AppSyncResolver()\napp.include_router(split_operation_append_context_module.router)\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPSYNC_RESOLVER)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\napp.append_context(is_admin=True)  # arbitrary number of key=value data\nreturn app.resolve(event, context)\n</code></pre> <pre><code>import sys\n\nif sys.version_info &gt;= (3, 8):\n    from typing import TypedDict\nelse:\n    from typing_extensions import TypedDict\n\nfrom typing import List\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler.appsync import Router\n\ntracer = Tracer()\nlogger = Logger()\nrouter = Router()\n\n\nclass Location(TypedDict, total=False):\n    id: str  # noqa AA03 VNE003, required due to GraphQL Schema\n    name: str\n    description: str\n    address: str\n\n\n@router.resolver(field_name=\"listLocations\")\n@router.resolver(field_name=\"locations\")\n@tracer.capture_method\ndef get_locations(name: str, description: str = \"\") -&gt; List[Location]:  # match GraphQL Query arguments\nis_admin: bool = router.context.get(\"is_admin\", False)\nreturn [{\"name\": name, \"description\": description}] if is_admin else []\n</code></pre>"},{"location":"core/event_handler/appsync/#testing-your-code","title":"Testing your code","text":"<p>You can test your resolvers by passing a mocked or actual AppSync Lambda event that you're expecting.</p> <p>You can use either <code>app.resolve(event, context)</code> or simply <code>app(event, context)</code>.</p> <p>Here's an example of how you can test your synchronous resolvers:</p> assert_graphql_response.pyassert_graphql_response_module.pyassert_graphql_response.json <pre><code>import json\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nimport pytest\nfrom assert_graphql_response_module import Location, app  # instance of AppSyncResolver\n@pytest.fixture\ndef lambda_context():\n    @dataclass\n    class LambdaContext:\n        function_name: str = \"test\"\n        memory_limit_in_mb: int = 128\n        invoked_function_arn: str = \"arn:aws:lambda:eu-west-1:123456789012:function:test\"\n        aws_request_id: str = \"da658bd3-2d6f-4e7b-8ec2-937234644fdc\"\n\n    return LambdaContext()\n\n\ndef test_direct_resolver(lambda_context):\n    # GIVEN\n    fake_event = json.loads(Path(\"assert_graphql_response.json\").read_text())\n\n    # WHEN\nresult: list[Location] = app(fake_event, lambda_context)\n# THEN\nassert result[0][\"name\"] == \"Perkins-Reed\"\n</code></pre> <pre><code>import sys\n\nif sys.version_info &gt;= (3, 8):\n    from typing import TypedDict\nelse:\n    from typing_extensions import TypedDict\n\nfrom typing import List\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import AppSyncResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = AppSyncResolver()\nclass Location(TypedDict, total=False):\n    id: str  # noqa AA03 VNE003, required due to GraphQL Schema\n    name: str\n    description: str\n    address: str\n\n\n@app.resolver(field_name=\"listLocations\")\n@app.resolver(field_name=\"locations\")\n@tracer.capture_method\ndef get_locations(name: str, description: str = \"\") -&gt; List[Location]:  # match GraphQL Query arguments\n    return [{\"name\": name, \"description\": description}]\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPSYNC_RESOLVER)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> <pre><code>{\n\"typeName\": \"Query\",\n\"fieldName\": \"listLocations\",\n\"arguments\": {\n\"name\": \"Perkins-Reed\",\n\"description\": \"Nulla sed amet. Earum libero qui sunt perspiciatis. Non aliquid accusamus.\"\n},\n\"selectionSetList\": [\n\"id\",\n\"name\"\n],\n\"identity\": {\n\"claims\": {\n\"sub\": \"192879fc-a240-4bf1-ab5a-d6a00f3063f9\",\n\"email_verified\": true,\n\"iss\": \"https://cognito-idp.us-west-2.amazonaws.com/us-west-xxxxxxxxxxx\",\n\"phone_number_verified\": false,\n\"cognito:username\": \"jdoe\",\n\"aud\": \"7471s60os7h0uu77i1tk27sp9n\",\n\"event_id\": \"bc334ed8-a938-4474-b644-9547e304e606\",\n\"token_use\": \"id\",\n\"auth_time\": 1599154213,\n\"phone_number\": \"+19999999999\",\n\"exp\": 1599157813,\n\"iat\": 1599154213,\n\"email\": \"jdoe@email.com\"\n},\n\"defaultAuthStrategy\": \"ALLOW\",\n\"groups\": null,\n\"issuer\": \"https://cognito-idp.us-west-2.amazonaws.com/us-west-xxxxxxxxxxx\",\n\"sourceIp\": [\n\"1.1.1.1\"\n],\n\"sub\": \"192879fc-a240-4bf1-ab5a-d6a00f3063f9\",\n\"username\": \"jdoe\"\n},\n\"request\": {\n\"headers\": {\n\"x-amzn-trace-id\": \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\",\n\"x-forwarded-for\": \"127.0.0.1\",\n\"cloudfront-viewer-country\": \"NL\",\n\"x-api-key\": \"da1-c33ullkbkze3jg5hf5ddgcs4fq\"\n}\n}\n}\n</code></pre> <p>And an example for testing asynchronous resolvers. Note that this requires the <code>pytest-asyncio</code> package. This tests a specific async GraphQL operation.</p> Note <p>Alternatively, you can continue call <code>lambda_handler</code> function synchronously as it'd run <code>asyncio.run</code> to await for the coroutine to complete.</p> assert_async_graphql_response.pyassert_async_graphql_response_module.pyassert_async_graphql_response.json <pre><code>import json\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import List\n\nimport pytest\nfrom assert_async_graphql_response_module import (  # instance of AppSyncResolver\n    Todo,\n    app,\n)\n\n\n@pytest.fixture\ndef lambda_context():\n    @dataclass\n    class LambdaContext:\n        function_name: str = \"test\"\n        memory_limit_in_mb: int = 128\n        invoked_function_arn: str = \"arn:aws:lambda:eu-west-1:123456789012:function:test\"\n        aws_request_id: str = \"da658bd3-2d6f-4e7b-8ec2-937234644fdc\"\n\n    return LambdaContext()\n\n\n@pytest.mark.asyncio\nasync def test_async_direct_resolver(lambda_context):\n    # GIVEN\n    fake_event = json.loads(Path(\"assert_async_graphql_response.json\").read_text())\n\n    # WHEN\nresult: List[Todo] = await app(fake_event, lambda_context)\n# alternatively, you can also run a sync test against `lambda_handler`\n    # since `lambda_handler` awaits the coroutine to complete\n\n    # THEN\n    assert result[0][\"userId\"] == 1\n    assert result[0][\"id\"] == 1\n    assert result[0][\"completed\"] is False\n</code></pre> <pre><code>import sys\n\nif sys.version_info &gt;= (3, 8):\n    from typing import TypedDict\nelse:\n    from typing_extensions import TypedDict\n\nimport asyncio\nfrom typing import List\n\nimport aiohttp\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import AppSyncResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.tracing import aiohttp_trace_config\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\nlogger = Logger()\napp = AppSyncResolver()\nclass Todo(TypedDict, total=False):\n    id: str  # noqa AA03 VNE003, required due to GraphQL Schema\n    userId: str\n    title: str\n    completed: bool\n\n\n@app.resolver(type_name=\"Query\", field_name=\"listTodos\")\nasync def list_todos() -&gt; List[Todo]:\n    async with aiohttp.ClientSession(trace_configs=[aiohttp_trace_config()]) as session:\n        async with session.get(\"https://jsonplaceholder.typicode.com/todos\") as resp:\n            result: List[Todo] = await resp.json()\n            return result[:2]  # first two results to demo assertion\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPSYNC_RESOLVER)\n@tracer.capture_lambda_handler\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    result = app.resolve(event, context)\n\n    return asyncio.run(result)\n</code></pre> <pre><code>{\n\"typeName\": \"Query\",\n\"fieldName\": \"listTodos\",\n\"arguments\": {},\n\"selectionSetList\": [\n\"id\",\n\"userId\",\n\"completed\"\n],\n\"identity\": {\n\"claims\": {\n\"sub\": \"192879fc-a240-4bf1-ab5a-d6a00f3063f9\",\n\"email_verified\": true,\n\"iss\": \"https://cognito-idp.us-west-2.amazonaws.com/us-west-xxxxxxxxxxx\",\n\"phone_number_verified\": false,\n\"cognito:username\": \"jdoe\",\n\"aud\": \"7471s60os7h0uu77i1tk27sp9n\",\n\"event_id\": \"bc334ed8-a938-4474-b644-9547e304e606\",\n\"token_use\": \"id\",\n\"auth_time\": 1599154213,\n\"phone_number\": \"+19999999999\",\n\"exp\": 1599157813,\n\"iat\": 1599154213,\n\"email\": \"jdoe@email.com\"\n},\n\"defaultAuthStrategy\": \"ALLOW\",\n\"groups\": null,\n\"issuer\": \"https://cognito-idp.us-west-2.amazonaws.com/us-west-xxxxxxxxxxx\",\n\"sourceIp\": [\n\"1.1.1.1\"\n],\n\"sub\": \"192879fc-a240-4bf1-ab5a-d6a00f3063f9\",\n\"username\": \"jdoe\"\n},\n\"request\": {\n\"headers\": {\n\"x-amzn-trace-id\": \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\",\n\"x-forwarded-for\": \"127.0.0.1\",\n\"cloudfront-viewer-country\": \"NL\",\n\"x-api-key\": \"da1-c33ullkbkze3jg5hf5ddgcs4fq\"\n}\n}\n}\n</code></pre>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This tutorial progressively introduces Lambda Powertools core utilities by using one feature at a time.</p>"},{"location":"tutorial/#requirements","title":"Requirements","text":"<ul> <li>AWS CLI and configured with your credentials.</li> <li>AWS SAM CLI installed.</li> </ul>"},{"location":"tutorial/#getting-started","title":"Getting started","text":"<p>Let's clone our sample project before we add one feature at a time.</p> Tip: Want to skip to the final project? <p>Bootstrap directly via SAM CLI:</p> <pre><code>sam init --app-template hello-world-powertools-python --name sam-app --package-type Zip --runtime python3.10 --no-tracing`\n</code></pre> Use SAM CLI to initialize the sample project<pre><code>sam init --runtime python3.10 --dependency-manager pip --app-template hello-world --name powertools-quickstart\n</code></pre>"},{"location":"tutorial/#project-structure","title":"Project structure","text":"<p>As we move forward, we will modify the following files within the <code>powertools-quickstart</code> folder:</p> <ul> <li>app.py - Application code.</li> <li>template.yaml - AWS infrastructure configuration using SAM.</li> <li>requirements.txt - List of extra Python packages needed.</li> </ul>"},{"location":"tutorial/#code-example","title":"Code example","text":"<p>Let's configure our base application to look like the following code snippet.</p> app.pytemplate.yaml <pre><code>import json\n\n\ndef hello():\n    return {\"statusCode\": 200, \"body\": json.dumps({\"message\": \"hello unknown!\"})}\n\n\ndef lambda_handler(event, context):\n    return hello()\n</code></pre> <pre><code>AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Sample SAM Template for powertools-quickstart\nGlobals:\nFunction:\nTimeout: 3\nResources:\nHelloWorldFunction:\nType: AWS::Serverless::Function\nProperties:\nCodeUri: hello_world/\nHandler: app.lambda_handler\nRuntime: python3.9\nArchitectures:\n- x86_64\nEvents:\nHelloWorld:\nType: Api\nProperties:\nPath: /hello\nMethod: get\nOutputs:\nHelloWorldApi:\nDescription: \"API Gateway endpoint URL for Prod stage for Hello World function\"\nValue: !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\"\n</code></pre> <p>Our Lambda code consists of an entry point function named <code>lambda_handler</code>, and a <code>hello</code> function.</p> <p>When API Gateway receives a HTTP GET request on <code>/hello</code> route, Lambda will call our <code>lambda_handler</code> function, subsequently calling the <code>hello</code> function. API Gateway will use this response to return the correct HTTP Status Code and payload back to the caller.</p> Warning <p>For simplicity, we do not set up authentication and authorization! You can find more information on how to implement it on AWS SAM documentation.</p>"},{"location":"tutorial/#run-your-code","title":"Run your code","text":"<p>At each point, you have two ways to run your code: locally and within your AWS account.</p>"},{"location":"tutorial/#local-test","title":"Local test","text":"<p>AWS SAM allows you to execute a serverless application locally by running <code>sam build &amp;&amp; sam local start-api</code> in your preferred shell.</p> Build and run API Gateway locally<pre><code>&gt; sam build &amp;&amp; sam local start-api\n...\n2021-11-26 17:43:08  * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit)\n</code></pre> <p>As a result, a local API endpoint will be exposed and you can invoke it using your browser, or your preferred HTTP API client e.g., Postman, httpie, etc.</p> Invoking our function locally via curl<pre><code>&gt; curl http://127.0.0.1:3000/hello\n{\"message\": \"hello unknown!\"}\n</code></pre> Info <p>To learn more about local testing, please visit the AWS SAM CLI local testing documentation.</p>"},{"location":"tutorial/#live-test","title":"Live test","text":"<p>First, you need to deploy your application into your AWS Account by issuing <code>sam build &amp;&amp; sam deploy --guided</code> command. This command builds a ZIP package of your source code, and deploy it to your AWS Account.</p> Build and deploy your serverless application<pre><code>&gt; sam build &amp;&amp; sam deploy --guided\n...\nCloudFormation outputs from deployed stack\n------------------------------------------------------------------------------------------------------------------------------------------\nOutputs\n------------------------------------------------------------------------------------------------------------------------------------------\nKey                 HelloWorldFunctionIamRole\nDescription         Implicit IAM Role created for Hello World function\nValue               arn:aws:iam::123456789012:role/sam-app-HelloWorldFunctionRole-1T2W3H9LZHGGV\n\nKey                 HelloWorldApi\nDescription         API Gateway endpoint URL for Prod stage for Hello World function\nValue               https://1234567890.execute-api.eu-central-1.amazonaws.com/Prod/hello/\n\nKey                 HelloWorldFunction\nDescription         Hello World Lambda Function ARN\nValue               arn:aws:lambda:eu-central-1:123456789012:function:sam-app-HelloWorldFunction-dOcfAtYoEiGo\n------------------------------------------------------------------------------------------------------------------------------------------\nSuccessfully created/updated stack - sam-app in eu-central-1\n</code></pre> <p>At the end of the deployment, you will find the API endpoint URL within <code>Outputs</code> section. You can use this URL to test your serverless application.</p> Invoking our application via API endpoint<pre><code>&gt; curl https://1234567890.execute-api.eu-central-1.amazonaws.com/Prod/hello\n{\"message\": \"hello unknown!\"}%\n</code></pre> Info <p>For more details on AWS SAM deployment mechanism, see SAM Deploy reference docs.</p>"},{"location":"tutorial/#routing","title":"Routing","text":""},{"location":"tutorial/#adding-a-new-route","title":"Adding a new route","text":"<p>Let's expand our application with a new route - <code>/hello/{name}</code>. It will accept an username as a path input and return it in the response.</p> <p>For this to work, we could create a new Lambda function to handle incoming requests for <code>/hello/{name}</code> - It'd look like this:</p> hello_by_name.pytemplate.yaml <pre><code>import json\n\n\ndef hello_name(name):\n    return {\"statusCode\": 200, \"body\": json.dumps({\"message\": f\"hello {name}!\"})}\n\n\ndef lambda_handler(event, context):\n    name = event[\"pathParameters\"][\"name\"]\n    return hello_name(name)\n</code></pre> <pre><code>AWSTemplateFormatVersion: \"2010-09-09\"\nTransform: AWS::Serverless-2016-10-31\nDescription: Sample SAM Template for powertools-quickstart\nGlobals:\nFunction:\nTimeout: 3\nResources:\nHelloWorldFunction:\nType: AWS::Serverless::Function\nProperties:\nCodeUri: hello_world/\nHandler: app.lambda_handler\nRuntime: python3.9\nEvents:\nHelloWorld:\nType: Api\nProperties:\nPath: /hello\nMethod: get\n\nHelloWorldByNameFunctionName:\nType: AWS::Serverless::Function\nProperties:\nCodeUri: hello_world/\nHandler: hello_by_name.lambda_handler\nRuntime: python3.9\nEvents:\nHelloWorldName:\nType: Api\nProperties:\nPath: /hello/{name}\nMethod: get\nOutputs:\nHelloWorldApi:\nDescription: \"API Gateway endpoint URL for Prod stage for Hello World function\"\nValue: !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\"\n</code></pre> Question <p>But what happens if your application gets bigger and we need to cover numerous URL paths and HTTP methods for them?</p> <p>This would quickly become non-trivial to maintain. Adding new Lambda function for each path, or multiple if/else to handle several routes &amp; HTTP Methods can be error prone.</p>"},{"location":"tutorial/#creating-our-own-router","title":"Creating our own router","text":"Question <p>What if we create a simple router to reduce boilerplate?</p> <p>We could group similar routes and intents, separate read and write operations resulting in fewer functions. It doesn't address the boilerplate routing code, but maybe it will be easier to add additional URLs.</p> Info: You might be already asking yourself about mono vs micro-functions <p>If you want a more detailed explanation of these two approaches, head over to the trade-offs on each approach later.</p> <p>A first attempt at the routing logic might look similar to the following code snippet.</p> app.pytemplate.yaml <pre><code>import json\n\n\ndef hello_name(event, **kargs):\nusername = event[\"pathParameters\"][\"name\"]\n    return {\"statusCode\": 200, \"body\": json.dumps({\"message\": f\"hello {username}!\"})}\n\n\ndef hello(**kargs):\nreturn {\"statusCode\": 200, \"body\": json.dumps({\"message\": \"hello unknown!\"})}\n\n\nclass Router:\ndef __init__(self):\n        self.routes = {}\n\n    def set(self, path, method, handler):\n        self.routes[f\"{path}-{method}\"] = handler\n\n    def get(self, path, method):\n        try:\n            route = self.routes[f\"{path}-{method}\"]\n        except KeyError:\n            raise RuntimeError(f\"Cannot route request to the correct method. path={path}, method={method}\")\n        return route\n\nrouter = Router()\nrouter.set(path=\"/hello\", method=\"GET\", handler=hello)\nrouter.set(path=\"/hello/{name}\", method=\"GET\", handler=hello_name)\ndef lambda_handler(event, context):\n    path = event[\"resource\"]\n    http_method = event[\"httpMethod\"]\nmethod = router.get(path=path, method=http_method)\nreturn method(event=event)\n</code></pre> <pre><code>AWSTemplateFormatVersion: \"2010-09-09\"\nTransform: AWS::Serverless-2016-10-31\nDescription: Sample SAM Template for powertools-quickstart\nGlobals:\nFunction:\nTimeout: 3\nResources:\nHelloWorldFunction:\nType: AWS::Serverless::Function\nProperties:\nCodeUri: hello_world/\nHandler: app.lambda_handler\nRuntime: python3.9\nEvents:\nHelloWorld:\nType: Api\nProperties:\nPath: /hello\nMethod: get\nHelloWorldName:\nType: Api\nProperties:\nPath: /hello/{name}\nMethod: get\nOutputs:\nHelloWorldApi:\nDescription: \"API Gateway endpoint URL for Prod stage for Hello World function\"\nValue: !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\"\n</code></pre> <p>Let's break this down:</p> <ul> <li>L4,9: We defined two <code>hello_name</code> and <code>hello</code> functions to handle <code>/hello/{name}</code> and <code>/hello</code> routes.</li> <li>L13: We added a <code>Router</code> class to map a path, a method, and the function to call.</li> <li>L27-29: We create a <code>Router</code> instance and map both <code>/hello</code> and <code>/hello/{name}</code>.</li> <li>L35: We use Router's <code>get</code> method to retrieve a reference to the processing method (<code>hello</code> or <code>hello_name</code>).</li> <li>L36: Finally, we run this method and send the results back to API Gateway.</li> </ul> <p>This approach simplifies the configuration of our infrastructure since we have added all API Gateway paths in the <code>HelloWorldFunction</code> event section.</p> <p>However, it forces us to understand the internal structure of the API Gateway request events, responses, and it could lead to other errors such as CORS not being handled properly, error handling, etc.</p>"},{"location":"tutorial/#simplifying-with-event-handler","title":"Simplifying with Event Handler","text":"<p>We can massively simplify cross-cutting concerns while keeping it lightweight by using Event Handler.</p> Tip <p>This is available for both REST API (API Gateway, ALB) and GraphQL API (AppSync).</p> <p>Let's include Lambda Powertools as a dependency in <code>requirement.txt</code>, and use Event Handler to refactor our previous example.</p> app.pyrequirements.txt <pre><code>from aws_lambda_powertools.event_handler import APIGatewayRestResolver\napp = APIGatewayRestResolver()\n@app.get(\"/hello/&lt;name&gt;\")\ndef hello_name(name):\n    return {\"message\": f\"hello {name}!\"}\n\n\n@app.get(\"/hello\")\ndef hello():\n    return {\"message\": \"hello unknown!\"}\n\n\ndef lambda_handler(event, context):\nreturn app.resolve(event, context)\n</code></pre> <pre><code>aws-lambda-powertools[tracer]  # Tracer requires AWS X-Ray SDK dependency\n</code></pre> <p>Use <code>sam build &amp;&amp; sam local start-api</code> and try run it locally again.</p> Note <p>If you're coming from Flask, you will be familiar with this experience already. Event Handler for API Gateway uses <code>APIGatewayRestResolver</code> to give a Flask-like experience while staying true to our tenet <code>Keep it lean</code>.</p> <p>We have added the route annotation as the decorator for our methods. It enables us to use the parameters passed in the request directly, and our responses are simply dictionaries.</p> <p>Lastly, we used <code>return app.resolve(event, context)</code> so Event Handler can resolve routes, inject the current request, handle serialization, route validation, etc.</p> <p>From here, we could handle 404 routes, error handling, access query strings, payload, etc.</p> Tip <p>If you'd like to learn how python decorators work under the hood, you can follow Real Python's article.</p>"},{"location":"tutorial/#structured-logging","title":"Structured Logging","text":"<p>Over time, you realize that searching logs as text results in poor observability, it's hard to create metrics from, enumerate common exceptions, etc.</p> <p>Then, you decided to propose production quality logging capabilities to your Lambda code. You found out that by having logs as <code>JSON</code> you can structure them, so that you can use any Log Analytics tool out there to quickly analyze them.</p> <p>This helps not only in searching, but produces consistent logs containing enough context and data to ask arbitrary questions on the status of your system. We can take advantage of CloudWatch Logs and Cloudwatch Insight for this purpose.</p>"},{"location":"tutorial/#json-as-output","title":"JSON as output","text":"<p>The first option could be to use the standard Python Logger, and use a specialized library like <code>pythonjsonlogger</code> to create a JSON Formatter.</p> app.pyrequirements.txt <pre><code>import logging\nimport os\n\nfrom pythonjsonlogger import jsonlogger\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nlogger = logging.getLogger(\"APP\")\nlogHandler = logging.StreamHandler()\nformatter = jsonlogger.JsonFormatter(fmt=\"%(asctime)s %(levelname)s %(name)s %(message)s\")\nlogHandler.setFormatter(formatter)\nlogger.addHandler(logHandler)\nlogger.setLevel(os.getenv(\"LOG_LEVEL\", \"INFO\"))\napp = APIGatewayRestResolver()\n\n\n@app.get(\"/hello/&lt;name&gt;\")\ndef hello_name(name):\nlogger.info(f\"Request from {name} received\")\nreturn {\"message\": f\"hello {name}!\"}\n\n\n@app.get(\"/hello\")\ndef hello():\nlogger.info(\"Request from unknown received\")\nreturn {\"message\": \"hello unknown!\"}\n\n\ndef lambda_handler(event, context):\nlogger.debug(event)\nreturn app.resolve(event, context)\n</code></pre> <pre><code>aws-lambda-powertools\npython-json-logger\n</code></pre> <p>With just a few lines our logs will now output to <code>JSON</code> format. We've taken the following steps to make that work:</p> <ul> <li>L7: Creates an application logger named <code>APP</code>.</li> <li>L8-11: Configures handler and formatter.</li> <li>L12: Sets the logging level set in the <code>LOG_LEVEL</code> environment variable, or <code>INFO</code> as a sentinel value.</li> </ul> <p>After that, we use this logger in our application code to record the required information. We see logs structured as follows:</p> JSON outputNormal output <pre><code>{\n\"asctime\": \"2021-11-22 15:32:02,145\",\n\"levelname\": \"INFO\",\n\"name\": \"APP\",\n\"message\": \"Request from unknown received\"\n}\n</code></pre> <pre><code>[INFO]  2021-11-22T15:32:02.145Z        ba3bea3d-fe3a-45db-a2ce-72e813d55b91    Request from unknown received\n</code></pre> <p>So far, so good! We can take a step further now by adding additional context to the logs.</p> <p>We could start by creating a dictionary with Lambda context information or something from the incoming event, which should always be logged. Additional attributes could be added on every <code>logger.info</code> using <code>extra</code> keyword like in any standard Python logger.</p>"},{"location":"tutorial/#simplifying-with-logger","title":"Simplifying with Logger","text":"Surely this could be easier, right? <p>Yes! Powertools Logger to the rescue :-)</p> <p>As we already have Lambda Powertools as a dependency, we can simply import Logger.</p> Refactoring with Lambda Powertools Logger<pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nlogger = Logger(service=\"APP\")\napp = APIGatewayRestResolver()\n\n\n@app.get(\"/hello/&lt;name&gt;\")\ndef hello_name(name):\nlogger.info(f\"Request from {name} received\")\nreturn {\"message\": f\"hello {name}!\"}\n\n\n@app.get(\"/hello\")\ndef hello():\nlogger.info(\"Request from unknown received\")\nreturn {\"message\": \"hello unknown!\"}\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST, log_event=True)\ndef lambda_handler(event, context):\n    return app.resolve(event, context)\n</code></pre> <p>Let's break this down:</p> <ul> <li>L5: We add Lambda Powertools Logger; the boilerplate is now done for you. By default, we set <code>INFO</code> as the logging level if <code>LOG_LEVEL</code> env var isn't set.</li> <li>L22: We use <code>logger.inject_lambda_context</code> decorator to inject key information from Lambda context into every log.</li> <li>L22: We also instruct Logger to use the incoming API Gateway Request ID as a correlation id automatically.</li> <li>L22: Since we're in dev, we also use <code>log_event=True</code> to automatically log each incoming request for debugging. This can be also set via environment variables.</li> </ul> <p>This is how the logs would look like now:</p> Our logs are now structured consistently<pre><code>{\n\"level\":\"INFO\",\n\"location\":\"hello:17\",\n\"message\":\"Request from unknown received\",\n\"timestamp\":\"2021-10-22 16:29:58,367+0000\",\n\"service\":\"APP\",\n\"cold_start\":true,\n\"function_name\":\"HelloWorldFunction\",\n\"function_memory_size\":\"256\",\n\"function_arn\":\"arn:aws:lambda:us-east-1:123456789012:function:HelloWorldFunction\",\n\"function_request_id\":\"d50bb07a-7712-4b2d-9f5d-c837302221a2\",\n\"correlation_id\":\"bf9b584c-e5d9-4ad5-af3d-db953f2b10dc\"\n}\n</code></pre> <p>We can now search our logs by the request ID to find a specific operation. Additionally, we can also search our logs for function name, Lambda request ID, Lambda function ARN, find out whether an operation was a cold start, etc.</p> <p>From here, we could set specific keys to add additional contextual information about a given operation, log exceptions to easily enumerate them later, sample debug logs, etc.</p> <p>By having structured logs like this, we can easily search and analyse them in CloudWatch Logs Insight.</p> CloudWatch Logs Insight Example <p></p>"},{"location":"tutorial/#tracing","title":"Tracing","text":"Note <p>You won't see any traces in AWS X-Ray when executing your function locally.</p> <p>The next improvement is to add distributed tracing to your stack. Traces help you visualize end-to-end transactions or parts of it to easily debug upstream/downstream anomalies.</p> <p>Combined with structured logs, it is an important step to be able to observe how your application runs in production.</p>"},{"location":"tutorial/#generating-traces","title":"Generating traces","text":"<p>AWS X-Ray is the distributed tracing service we're going to use. But how do we generate application traces in the first place?</p> <p>It's a two-step process:</p> <ol> <li>Enable tracing in your Lambda function.</li> <li>Instrument your application code.</li> </ol> <p>Let's explore how we can instrument our code with AWS X-Ray SDK, and then simplify it with Lambda Powertools Tracer feature.</p> app.pytemplate.yamlrequirements.txt <pre><code>from aws_xray_sdk.core import xray_recorder\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\n\nlogger = Logger(service=\"APP\")\n\napp = APIGatewayRestResolver()\n\n\n@app.get(\"/hello/&lt;name&gt;\")\n@xray_recorder.capture('hello_name')\ndef hello_name(name):\n    logger.info(f\"Request from {name} received\")\n    return {\"message\": f\"hello {name}!\"}\n\n\n@app.get(\"/hello\")\n@xray_recorder.capture('hello')\ndef hello():\n    logger.info(\"Request from unknown received\")\n    return {\"message\": \"hello unknown!\"}\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST, log_event=True)\n@xray_recorder.capture('handler')\ndef lambda_handler(event, context):\n    return app.resolve(event, context)\n</code></pre> <pre><code>AWSTemplateFormatVersion: \"2010-09-09\"\nTransform: AWS::Serverless-2016-10-31\nDescription: Sample SAM Template for powertools-quickstart\nGlobals:\nFunction:\nTimeout: 3\nApi:\nTracingEnabled: true\nResources:\nHelloWorldFunction:\nType: AWS::Serverless::Function\nProperties:\nCodeUri: hello_world/\nHandler: app.lambda_handler\nRuntime: python3.9\nTracing: Active\nEvents:\nHelloWorld:\nType: Api\nProperties:\nPath: /hello\nMethod: get\nHelloWorldName:\nType: Api\nProperties:\nPath: /hello/{name}\nMethod: get\nOutputs:\nHelloWorldApi:\nDescription: \"API Gateway endpoint URL for Prod stage for Hello World function\"\nValue: !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\"\n</code></pre> <pre><code>aws-lambda-powertools\naws-xray-sdk\n</code></pre> <p>Let's break it down:</p> <ul> <li>L1: First, we import AWS X-Ray SDK. <code>xray_recorder</code> records blocks of code being traced (subsegment). It also sends generated traces to the AWS X-Ray daemon running in the Lambda service who subsequently forwards them to AWS X-Ray service.</li> <li>L13,20,27: We decorate our function so the SDK traces the end-to-end execution, and the argument names the generated block being traced.</li> </ul> Question <p>But how do I enable tracing for the Lambda function and what permissions do I need?</p> <p>We've made the following changes in <code>template.yaml</code> for this to work seamless:</p> <ul> <li>L7-8: Enables tracing for Amazon API Gateway.</li> <li>L16: Enables tracing for our Serverless Function. This will also add a managed IAM Policy named AWSXRayDaemonWriteAccess to allow Lambda to send traces to AWS X-Ray.</li> </ul> <p>You can now build and deploy our updates with <code>sam build &amp;&amp; sam deploy</code>. Once deployed, try invoking the application via the API endpoint, and visit AWS X-Ray Console to see how much progress we've made so far!!</p> <p></p>"},{"location":"tutorial/#enriching-our-generated-traces","title":"Enriching our generated traces","text":"<p>What we've done helps bring an initial visibility, but we can do so much more.</p> Question <p>You're probably asking yourself at least the following questions:</p> <ul> <li>What if I want to search traces by customer name?</li> <li>What about grouping traces with cold starts?</li> <li>Better yet, what if we want to include the request or response of our functions as part of the trace?</li> </ul> <p>Within AWS X-Ray, we can answer these questions by using two features: tracing Annotations and Metadata.</p> <p>Annotations are simple key-value pairs that are indexed for use with filter expressions. Metadata are key-value pairs with values of any type, including objects and lists, but that are not indexed.</p> <p>Let's put them into action.</p> Enriching traces with annotations and metadata<pre><code>from aws_xray_sdk.core import patch_all, xray_recorder\n\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\n\nlogger = Logger(service=\"APP\")\n\napp = APIGatewayRestResolver()\ncold_start = True\npatch_all()\n\n\n@app.get(\"/hello/&lt;name&gt;\")\n@xray_recorder.capture('hello_name')\ndef hello_name(name):\nsubsegment = xray_recorder.current_subsegment()\nsubsegment.put_annotation(key=\"User\", value=name)\nlogger.info(f\"Request from {name} received\")\n    return {\"message\": f\"hello {name}!\"}\n\n\n@app.get(\"/hello\")\n@xray_recorder.capture('hello')\ndef hello():\nsubsegment = xray_recorder.current_subsegment()\nsubsegment.put_annotation(key=\"User\", value=\"unknown\")\nlogger.info(\"Request from unknown received\")\n    return {\"message\": \"hello unknown!\"}\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST, log_event=True)\n@xray_recorder.capture('handler')\ndef lambda_handler(event, context):\nglobal cold_start\nsubsegment = xray_recorder.current_subsegment()\nif cold_start:\nsubsegment.put_annotation(key=\"ColdStart\", value=cold_start)\ncold_start = False\nelse:\nsubsegment.put_annotation(key=\"ColdStart\", value=cold_start)\nresult = app.resolve(event, context)\nsubsegment.put_metadata(\"response\", result)\nreturn result\n</code></pre> <p>Let's break it down:</p> <ul> <li>L10: We track Lambda cold start by setting global variable outside the handler; this is executed once per sandbox Lambda creates. This information provides an overview of how often the sandbox is reused by Lambda, which directly impacts the performance of each transaction.</li> <li>L17-18: We use AWS X-Ray SDK to add <code>User</code> annotation on <code>hello_name</code> subsegment. This will allow us to filter traces using the <code>User</code> value.</li> <li>L26-27: We repeat what we did in L17-18 except we use the value <code>unknown</code> since we don't have that information.</li> <li>L35: We use <code>global</code> to modify our global variable defined in the outer scope.</li> <li>37-42: We add <code>ColdStart</code> annotation and flip the value of <code>cold_start</code> variable, so that subsequent requests annotates the value <code>false</code> when the sandbox is reused.</li> <li>L45: We include the final response under <code>response</code> key as part of the <code>handler</code> subsegment.</li> </ul> Info <p>If you want to understand how the Lambda execution environment (sandbox) works and why cold starts can occur, see this blog series on Lambda performance.</p> <p>Repeat the process of building, deploying, and invoking your application via the API endpoint.</p> <p>Within the AWS X-Ray Console, you should now be able to group traces by the <code>User</code> and <code>ColdStart</code> annotation.</p> <p></p> <p>If you choose any of the traces available, try opening the <code>handler</code> subsegment and you should see the response of your Lambda function under the <code>Metadata</code> tab.</p> <p></p>"},{"location":"tutorial/#simplifying-with-tracer","title":"Simplifying with Tracer","text":"<p>Cross-cutting concerns like filtering traces by Cold Start, including response as well as exceptions as tracing metadata can take a considerable amount of boilerplate.</p> <p>We can simplify our previous patterns by using Lambda Powertools Tracer; a thin wrapper on top of X-Ray SDK.</p> Note <p>You can now safely remove <code>aws-xray-sdk</code> from <code>requirements.txt</code>; keep <code>aws-lambda-powertools</code> only.</p> Refactoring with Lambda Powertools Tracer<pre><code>from aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\n\nlogger = Logger(service=\"APP\")\ntracer = Tracer(service=\"APP\")\napp = APIGatewayRestResolver()\n\n\n@app.get(\"/hello/&lt;name&gt;\")\n@tracer.capture_method\ndef hello_name(name):\ntracer.put_annotation(key=\"User\", value=name)\nlogger.info(f\"Request from {name} received\")\n    return {\"message\": f\"hello {name}!\"}\n\n\n@app.get(\"/hello\")\n@tracer.capture_method\ndef hello():\ntracer.put_annotation(key=\"User\", value=\"unknown\")\nlogger.info(\"Request from unknown received\")\n    return {\"message\": \"hello unknown!\"}\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST, log_event=True)\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context):\n    return app.resolve(event, context)\n</code></pre> <p>Decorators, annotations and metadata are largely the same, except we now have a much cleaner code as the boilerplate is gone. Here's what's changed compared to AWS X-Ray SDK approach:</p> <ul> <li>L6: We initialize <code>Tracer</code> and define the name of our service (<code>APP</code>). We automatically run <code>patch_all</code> from AWS X-Ray SDK on your behalf. Any previously patched or non-imported library is simply ignored.</li> <li>L11: We use <code>@tracer.capture_method</code> decorator instead of <code>xray_recorder.capture</code>. We automatically create a subsegment named after the function name (<code>## hello_name</code>), and add the response/exception as tracing metadata.</li> <li>L13: Putting annotations remain exactly the same UX.</li> <li>L27: We use <code>@tracer.lambda_handler</code> so we automatically add <code>ColdStart</code> annotation within Tracer itself. We also add a new <code>Service</code> annotation using the value of <code>Tracer(service=\"APP\")</code>, so that you can filter traces by the service your function(s) represent.</li> </ul> <p>Another subtle difference is that you can now run your Lambda functions and unit test them locally without having to explicitly disable Tracer.</p> <p>Lambda Powertools optimizes for Lambda compute environment. As such, we add these and other common approaches to accelerate your development, so you don't worry about implementing every cross-cutting concern.</p> Tip <p>You can opt-out some of these behaviours like disabling response capturing,  explicitly patching only X modules, etc.</p> <p>Repeat the process of building, deploying, and invoking your application via the API endpoint. Within the AWS X-Ray Console, you should see a similar view:</p> <p></p> Tip <p>Consider using Amazon CloudWatch ServiceLens view as it aggregates AWS X-Ray traces and CloudWatch metrics and logs in one view.</p> <p>From here, you can browse to specific logs in CloudWatch Logs Insight, Metrics Dashboard or AWS X-Ray traces.</p> <p></p> Info <p>For more information on Amazon CloudWatch ServiceLens, please visit link.</p>"},{"location":"tutorial/#custom-metrics","title":"Custom Metrics","text":""},{"location":"tutorial/#creating-metrics","title":"Creating metrics","text":"<p>Let's add custom metrics to better understand our application and business behavior (e.g. number of reservations, etc.).</p> <p>By default, AWS Lambda adds invocation and performance metrics, and Amazon API Gateway adds latency and some HTTP metrics.</p> Tip <p>You can optionally enable detailed metrics per each API route, stage, and method in API Gateway.</p> <p>Let's expand our application with custom metrics using AWS SDK to see how it works, then let's upgrade it with Lambda Powertools :-)</p> app.pytemplate.yaml <pre><code>import os\n\nimport boto3\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\n\ncold_start = True\nmetric_namespace = \"MyApp\"\nlogger = Logger(service=\"APP\")\ntracer = Tracer(service=\"APP\")\nmetrics = boto3.client(\"cloudwatch\")\napp = APIGatewayRestResolver()\n\n\n@tracer.capture_method\ndef add_greeting_metric(service: str = \"APP\"):\nfunction_name = os.getenv(\"AWS_LAMBDA_FUNCTION_NAME\", \"undefined\")\nservice_dimension = {\"Name\": \"service\", \"Value\": service}\nfunction_dimension = {\"Name\": \"function_name\", \"Value\": function_name}\nis_cold_start = True\nglobal cold_start\nif cold_start:\ncold_start = False\nelse:\nis_cold_start = False\nreturn metrics.put_metric_data(\nMetricData=[\n{\n\"MetricName\": \"SuccessfulGreetings\",\n\"Dimensions\": [service_dimension],\n\"Unit\": \"Count\",\n\"Value\": 1,\n},\n{\n\"MetricName\": \"ColdStart\",\n\"Dimensions\": [service_dimension, function_dimension],\n\"Unit\": \"Count\",\n\"Value\": int(is_cold_start)\n}\n],\nNamespace=metric_namespace,\n)\n@app.get(\"/hello/&lt;name&gt;\")\n@tracer.capture_method\ndef hello_name(name):\n    tracer.put_annotation(key=\"User\", value=name)\n    logger.info(f\"Request from {name} received\")\nadd_greeting_metric()\nreturn {\"message\": f\"hello {name}!\"}\n\n\n@app.get(\"/hello\")\n@tracer.capture_method\ndef hello():\n    tracer.put_annotation(key=\"User\", value=\"unknown\")\n    logger.info(\"Request from unknown received\")\nadd_greeting_metric()\nreturn {\"message\": \"hello unknown!\"}\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST, log_event=True)\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context):\n    return app.resolve(event, context)\n</code></pre> <pre><code>AWSTemplateFormatVersion: \"2010-09-09\"\nTransform: AWS::Serverless-2016-10-31\nDescription: Sample SAM Template for powertools-quickstart\nGlobals:\nFunction:\nTimeout: 3\nResources:\nHelloWorldFunction:\nType: AWS::Serverless::Function\nProperties:\nCodeUri: hello_world/\nHandler: app.lambda_handler\nRuntime: python3.9\nTracing: Active\nEvents:\nHelloWorld:\nType: Api\nProperties:\nPath: /hello\nMethod: get\nHelloWorldName:\nType: Api\nProperties:\nPath: /hello/{name}\nMethod: get\nPolicies:\n- CloudWatchPutMetricPolicy: {}\nOutputs:\nHelloWorldApi:\nDescription: \"API Gateway endpoint URL for Prod stage for Hello World function\"\nValue: !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/hello/\"\n</code></pre> <p>There's a lot going on, let's break this down:</p> <ul> <li>L10: We define a container where all of our application metrics will live <code>MyApp</code>, a.k.a Metrics Namespace.</li> <li>L14: We initialize a CloudWatch client to send metrics later.</li> <li>L19-47: We create a custom function to prepare and send <code>ColdStart</code> and <code>SuccessfulGreetings</code> metrics using CloudWatch expected data structure. We also set dimensions of these metrics.<ul> <li>Think of them as metadata to define to slice and dice them later; an unique metric is a combination of metric name + metric dimension(s).</li> </ul> </li> <li>L55,64: We call our custom function to create metrics for every greeting received.</li> </ul> Question <p>But what permissions do I need to send metrics to CloudWatch?</p> <p>Within <code>template.yaml</code>, we add CloudWatchPutMetricPolicy policy in SAM.</p> Adding metrics via AWS SDK gives a lot of flexibility at a cost <p><code>put_metric_data</code> is a synchronous call to CloudWatch Metrics API. This means establishing a connection to CloudWatch endpoint, sending metrics payload, and waiting from a response.</p> <p>It will be visible in your AWS X-RAY traces as additional external call. Given your architecture scale, this approach might lead to disadvantages such as increased cost of measuring data collection and increased Lambda latency.</p>"},{"location":"tutorial/#simplifying-with-metrics","title":"Simplifying with Metrics","text":"<p>Lambda Powertools Metrics uses Amazon CloudWatch Embedded Metric Format (EMF) to create custom metrics asynchronously via a native integration with Lambda.</p> <p>In general terms, EMF is a specification that expects metrics in a JSON payload within CloudWatch Logs. Lambda ingests all logs emitted by a given function into CloudWatch Logs. CloudWatch automatically looks up for log entries that follow the EMF format and transforms them into a CloudWatch metric.</p> Info <p>If you are interested in the details of the EMF mechanism, follow blog post.</p> <p>Let's implement that using Metrics:</p> Refactoring with Lambda Powertools Metrics<pre><code>from aws_lambda_powertools import Logger, Tracer, Metrics\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.metrics import MetricUnit\nlogger = Logger(service=\"APP\")\ntracer = Tracer(service=\"APP\")\nmetrics = Metrics(namespace=\"MyApp\", service=\"APP\")\napp = APIGatewayRestResolver()\n\n\n@app.get(\"/hello/&lt;name&gt;\")\n@tracer.capture_method\ndef hello_name(name):\n    tracer.put_annotation(key=\"User\", value=name)\n    logger.info(f\"Request from {name} received\")\nmetrics.add_metric(name=\"SuccessfulGreetings\", unit=MetricUnit.Count, value=1)\nreturn {\"message\": f\"hello {name}!\"}\n\n\n@app.get(\"/hello\")\n@tracer.capture_method\ndef hello():\n    tracer.put_annotation(key=\"User\", value=\"unknown\")\n    logger.info(\"Request from unknown received\")\nmetrics.add_metric(name=\"SuccessfulGreetings\", unit=MetricUnit.Count, value=1)\nreturn {\"message\": \"hello unknown!\"}\n\n\n@tracer.capture_lambda_handler\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.API_GATEWAY_REST, log_event=True)\n@metrics.log_metrics(capture_cold_start_metric=True)\ndef lambda_handler(event, context):\n    try:\n        return app.resolve(event, context)\n    except Exception as e:\n        logger.exception(e)\n        raise\n</code></pre> <p>That's a lot less boilerplate code! Let's break this down:</p> <ul> <li>L9: We initialize <code>Metrics</code> with our service name (<code>APP</code>) and metrics namespace (<code>MyApp</code>), reducing the need to add the <code>service</code> dimension for every metric and setting the namespace later</li> <li>L18, 27: We use <code>add_metric</code> similarly to our custom function, except we now have an enum <code>MetricCount</code> to help us understand which Metric Units we have at our disposal</li> <li>L33: We use <code>@metrics.log_metrics</code> decorator to ensure that our metrics are aligned with the EMF output and validated before-hand, like in case we forget to set namespace, or accidentally use a metric unit as a string that doesn't exist in CloudWatch.</li> <li>L33: We also use <code>capture_cold_start_metric=True</code> so we don't have to handle that logic either. Note that Metrics does not publish a warm invocation metric (ColdStart=0) for cost reasons. As such, treat the absence (sparse metric) as a non-cold start invocation.</li> </ul> <p>Repeat the process of building, deploying, and invoking your application via the API endpoint a few times to generate metrics - Artillery and K6.io are quick ways to generate some load.</p> <p>Within CloudWatch Metrics view, you should see <code>MyApp</code> custom namespace with your custom metrics there and <code>SuccessfulGreetings</code> available to graph.</p> <p></p> <p>If you're curious about how the EMF portion of your function logs look like, you can quickly go to CloudWatch ServiceLens view, choose your function and open logs. You will see a similar entry that looks like this:</p> <pre><code>{\n\"_aws\": {\n\"Timestamp\": 1638115724269,\n\"CloudWatchMetrics\": [\n{\n\"Namespace\": \"CustomMetrics\",\n\"Dimensions\": [\n[\n\"method\",\n\"service\"\n]\n],\n\"Metrics\": [\n{\n\"Name\": \"AppMethodsInvocations\",\n\"Unit\": \"Count\"\n}\n]\n}\n]\n},\n\"method\": \"/hello/&lt;name&gt;\",\n\"service\": \"APP\",\n\"AppMethodsInvocations\": [\n1\n]\n}\n</code></pre>"},{"location":"tutorial/#final-considerations","title":"Final considerations","text":"<p>We covered a lot of ground here and we only scratched the surface of the feature set available within Lambda Powertools.</p> <p>When it comes to the observability features (Tracer, Metrics, Logging), don't stop there! The goal here is to ensure you can ask arbitrary questions to assess your system's health; these features are only part of the wider story!</p> <p>This requires a change in mindset to ensure operational excellence is part of the software development lifecycle.</p> Tip <p>You can find more details on other leading practices described in the Well-Architected Serverless Lens.</p> <p>Lambda Powertools is largely designed to make some of these practices easier to adopt from day 1.</p> Have ideas for other tutorials? <p>You can open up a documentation issue, or via e-mail aws-lambda-powertools-feedback@amazon.com.</p>"},{"location":"utilities/batch/","title":"Batch Processing","text":"<p>The batch processing utility handles partial failures when processing batches from Amazon SQS, Amazon Kinesis Data Streams, and Amazon DynamoDB Streams.</p>"},{"location":"utilities/batch/#key-features","title":"Key Features","text":"<ul> <li>Reports batch item failures to reduce number of retries for a record upon errors</li> <li>Simple interface to process each batch record</li> <li>Integrates with Event Source Data Classes and Parser (Pydantic) for self-documenting record schema</li> <li>Build your own batch processor by extending primitives</li> </ul>"},{"location":"utilities/batch/#background","title":"Background","text":"<p>When using SQS, Kinesis Data Streams, or DynamoDB Streams as a Lambda event source, your Lambda functions are triggered with a batch of messages.</p> <p>If your function fails to process any message from the batch, the entire batch returns to your queue or stream. This same batch is then retried until either condition happens first: a) your Lambda function returns a successful response, b) record reaches maximum retry attempts, or c) when records expire.</p> <p>With this utility, batch records are processed individually \u2013 only messages that failed to be processed return to the queue or stream for a further retry. This works when two mechanisms are in place:</p> <ol> <li><code>ReportBatchItemFailures</code> is set in your SQS, Kinesis, or DynamoDB event source properties</li> <li>A specific response is returned so Lambda knows which records should not be deleted during partial responses</li> </ol> Warning: This utility lowers the chance of processing records more than once; it does not guarantee it <p>We recommend implementing processing logic in an idempotent manner wherever possible.</p> <p>You can find more details on how Lambda works with either SQS, Kinesis, or DynamoDB in the AWS Documentation.</p>"},{"location":"utilities/batch/#getting-started","title":"Getting started","text":"<p>Regardless whether you're using SQS, Kinesis Data Streams or DynamoDB Streams, you must configure your Lambda function event source to use <code>`ReportBatchItemFailures</code>.</p> <p>You do not need any additional IAM permissions to use this utility, except for what each event source requires.</p>"},{"location":"utilities/batch/#required-resources","title":"Required resources","text":"<p>The remaining sections of the documentation will rely on these samples. For completeness, this demonstrates IAM permissions and Dead Letter Queue where batch records will be sent after 2 retries were attempted.</p> SQSKinesis Data StreamsDynamoDB Streams template.yaml<pre><code>AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: partial batch response sample\n\nGlobals:\nFunction:\nTimeout: 5\nMemorySize: 256\nRuntime: python3.9\nTracing: Active\nEnvironment:\nVariables:\nLOG_LEVEL: INFO\nPOWERTOOLS_SERVICE_NAME: hello\n\nResources:\nHelloWorldFunction:\nType: AWS::Serverless::Function\nProperties:\nHandler: app.lambda_handler\nCodeUri: hello_world\nPolicies:\n- SQSPollerPolicy:\nQueueName: !GetAtt SampleQueue.QueueName\nEvents:\nBatch:\nType: SQS\nProperties:\nQueue: !GetAtt SampleQueue.Arn\nFunctionResponseTypes:\n- ReportBatchItemFailures\nSampleDLQ:\nType: AWS::SQS::Queue\n\nSampleQueue:\nType: AWS::SQS::Queue\nProperties:\nVisibilityTimeout: 30 # Fn timeout * 6\nRedrivePolicy:\nmaxReceiveCount: 2\ndeadLetterTargetArn: !GetAtt SampleDLQ.Arn\n</code></pre> template.yaml<pre><code>AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: partial batch response sample\n\nGlobals:\nFunction:\nTimeout: 5\nMemorySize: 256\nRuntime: python3.9\nTracing: Active\nEnvironment:\nVariables:\nLOG_LEVEL: INFO\nPOWERTOOLS_SERVICE_NAME: hello\n\nResources:\nHelloWorldFunction:\nType: AWS::Serverless::Function\nProperties:\nHandler: app.lambda_handler\nCodeUri: hello_world\nPolicies:\n# Lambda Destinations require additional permissions\n# to send failure records to DLQ from Kinesis/DynamoDB\n- Version: \"2012-10-17\"\nStatement:\nEffect: \"Allow\"\nAction:\n- sqs:GetQueueAttributes\n- sqs:GetQueueUrl\n- sqs:SendMessage\nResource: !GetAtt SampleDLQ.Arn\nEvents:\nKinesisStream:\nType: Kinesis\nProperties:\nStream: !GetAtt SampleStream.Arn\nBatchSize: 100\nStartingPosition: LATEST\nMaximumRetryAttempts: 2\nDestinationConfig:\nOnFailure:\nDestination: !GetAtt SampleDLQ.Arn\nFunctionResponseTypes:\n- ReportBatchItemFailures\nSampleDLQ:\nType: AWS::SQS::Queue\n\nSampleStream:\nType: AWS::Kinesis::Stream\nProperties:\nShardCount: 1\n</code></pre> template.yaml<pre><code>AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: partial batch response sample\n\nGlobals:\nFunction:\nTimeout: 5\nMemorySize: 256\nRuntime: python3.9\nTracing: Active\nEnvironment:\nVariables:\nLOG_LEVEL: INFO\nPOWERTOOLS_SERVICE_NAME: hello\n\nResources:\nHelloWorldFunction:\nType: AWS::Serverless::Function\nProperties:\nHandler: app.lambda_handler\nCodeUri: hello_world\nPolicies:\n# Lambda Destinations require additional permissions\n# to send failure records from Kinesis/DynamoDB\n- Version: \"2012-10-17\"\nStatement:\nEffect: \"Allow\"\nAction:\n- sqs:GetQueueAttributes\n- sqs:GetQueueUrl\n- sqs:SendMessage\nResource: !GetAtt SampleDLQ.Arn\nEvents:\nDynamoDBStream:\nType: DynamoDB\nProperties:\nStream: !GetAtt SampleTable.StreamArn\nStartingPosition: LATEST\nMaximumRetryAttempts: 2\nDestinationConfig:\nOnFailure:\nDestination: !GetAtt SampleDLQ.Arn\nFunctionResponseTypes:\n- ReportBatchItemFailures\nSampleDLQ:\nType: AWS::SQS::Queue\n\nSampleTable:\nType: AWS::DynamoDB::Table\nProperties:\nBillingMode: PAY_PER_REQUEST\nAttributeDefinitions:\n- AttributeName: pk\nAttributeType: S\n- AttributeName: sk\nAttributeType: S\nKeySchema:\n- AttributeName: pk\nKeyType: HASH\n- AttributeName: sk\nKeyType: RANGE\nSSESpecification:\nSSEEnabled: yes\nStreamSpecification:\nStreamViewType: NEW_AND_OLD_IMAGES\n</code></pre>"},{"location":"utilities/batch/#processing-messages-from-sqs","title":"Processing messages from SQS","text":"<p>Processing batches from SQS works in three stages:</p> <ol> <li>Instantiate <code>BatchProcessor</code> and choose <code>EventType.SQS</code> for the event type</li> <li>Define your function to handle each batch record, and use <code>SQSRecord</code> type annotation for autocompletion</li> <li>Use <code>process_partial_response</code> to kick off processing</li> </ol> Info <p>This code example optionally uses Tracer and Logger for completion.</p> RecommendedAs a context managerAs a decorator (legacy)Sample responseSample event <pre><code>import json\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import (\nBatchProcessor,\n    EventType,\n    process_partial_response,\n)\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nprocessor = BatchProcessor(event_type=EventType.SQS)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: SQSRecord):\npayload: str = record.body\n    if payload:\n        item: dict = json.loads(payload)\n        logger.info(item)\n    ...\n\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\nreturn process_partial_response(event=event, record_handler=record_handler, processor=processor, context=context)\n</code></pre> <pre><code>import json\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, EventType\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\nprocessor = BatchProcessor(event_type=EventType.SQS)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: SQSRecord):\npayload: str = record.body\n    if payload:\n        item: dict = json.loads(payload)\n    ...\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\nbatch = event[\"Records\"]\nwith processor(records=batch, handler=record_handler):\nprocessed_messages = processor.process() # kick off processing, return list[tuple]\nreturn processor.response()\n</code></pre> <pre><code>import json\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\nprocessor = BatchProcessor(event_type=EventType.SQS)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: SQSRecord):\npayload: str = record.body\n    if payload:\n        item: dict = json.loads(payload)\n    ...\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\n@batch_processor(record_handler=record_handler, processor=processor)\ndef lambda_handler(event, context: LambdaContext):\nreturn processor.response()\n</code></pre> <p>The second record failed to be processed, therefore the processor added its message ID in the response.</p> <pre><code>{\n    'batchItemFailures': [\n        {\n            'itemIdentifier': '244fc6b4-87a3-44ab-83d2-361172410c3a'\n        }\n    ]\n}\n</code></pre> <pre><code>{\n\"Records\": [\n{\n\"messageId\": \"059f36b4-87a3-44ab-83d2-661975830a7d\",\n\"receiptHandle\": \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a\",\n\"body\": \"{\\\"Message\\\": \\\"success\\\"}\",\n\"attributes\": {\n\"ApproximateReceiveCount\": \"1\",\n\"SentTimestamp\": \"1545082649183\",\n\"SenderId\": \"AIDAIENQZJOLO23YVJ4VO\",\n\"ApproximateFirstReceiveTimestamp\": \"1545082649185\"\n},\n\"messageAttributes\": {},\n\"md5OfBody\": \"e4e68fb7bd0e697a0ae8f1bb342846b3\",\n\"eventSource\": \"aws:sqs\",\n\"eventSourceARN\": \"arn:aws:sqs:us-east-2: 123456789012:my-queue\",\n\"awsRegion\": \"us-east-1\"\n},\n{\n\"messageId\": \"244fc6b4-87a3-44ab-83d2-361172410c3a\",\n\"receiptHandle\": \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a\",\n\"body\": \"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==\",\n\"attributes\": {\n\"ApproximateReceiveCount\": \"1\",\n\"SentTimestamp\": \"1545082649183\",\n\"SenderId\": \"AIDAIENQZJOLO23YVJ4VO\",\n\"ApproximateFirstReceiveTimestamp\": \"1545082649185\"\n},\n\"messageAttributes\": {},\n\"md5OfBody\": \"e4e68fb7bd0e697a0ae8f1bb342846b3\",\n\"eventSource\": \"aws:sqs\",\n\"eventSourceARN\": \"arn:aws:sqs:us-east-2: 123456789012:my-queue\",\n\"awsRegion\": \"us-east-1\"\n}\n]\n}\n</code></pre>"},{"location":"utilities/batch/#fifo-queues","title":"FIFO queues","text":"<p>When using SQS FIFO queues, we will stop processing messages after the first failure, and return all failed and unprocessed messages in <code>batchItemFailures</code>. This helps preserve the ordering of messages in your queue.</p> RecommendedAs a context managerAs a decorator (legacy) <pre><code>from aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import (\nSqsFifoPartialProcessor,\nprocess_partial_response,\n)\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nprocessor = SqsFifoPartialProcessor()\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: SQSRecord):\n    ...\n\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\n    return process_partial_response(event=event, record_handler=record_handler, processor=processor, context=context)\n</code></pre> <pre><code>from aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import SqsFifoPartialProcessor\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nprocessor = SqsFifoPartialProcessor()\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: SQSRecord):\n    ...\n\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\n    batch = event[\"Records\"]\n    with processor(records=batch, handler=record_handler):\n        processor.process()  # kick off processing, return List[Tuple]\n\n    return processor.response()\n</code></pre> <pre><code>from aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import (\nSqsFifoPartialProcessor,\nbatch_processor,\n)\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nprocessor = SqsFifoPartialProcessor()\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: SQSRecord):\n    ...\n\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\n@batch_processor(record_handler=record_handler, processor=processor)\ndef lambda_handler(event, context: LambdaContext):\n    return processor.response()\n</code></pre>"},{"location":"utilities/batch/#processing-messages-from-kinesis","title":"Processing messages from Kinesis","text":"<p>Processing batches from Kinesis works in three stages:</p> <ol> <li>Instantiate <code>BatchProcessor</code> and choose <code>EventType.KinesisDataStreams</code> for the event type</li> <li>Define your function to handle each batch record, and use <code>KinesisStreamRecord</code> type annotation for autocompletion</li> <li>Use <code>process_partial_response</code> to kick off processing</li> </ol> Info <p>This code example optionally uses Tracer and Logger for completion.</p> RecommendedAs a context managerAs a decorator (legacy)Sample responseSample event <pre><code>from aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import (\nBatchProcessor,\n    EventType,\n    process_partial_response,\n)\nfrom aws_lambda_powertools.utilities.data_classes.kinesis_stream_event import (\nKinesisStreamRecord,\n)\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nprocessor = BatchProcessor(event_type=EventType.KinesisDataStreams)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: KinesisStreamRecord):\nlogger.info(record.kinesis.data_as_text)\n    payload: dict = record.kinesis.data_as_json()\n    logger.info(payload)\n    ...\n\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\nreturn process_partial_response(event=event, record_handler=record_handler, processor=processor, context=context)\n</code></pre> <pre><code>import json\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, EventType\nfrom aws_lambda_powertools.utilities.data_classes.kinesis_stream_event import KinesisStreamRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\nprocessor = BatchProcessor(event_type=EventType.KinesisDataStreams)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: KinesisStreamRecord):\nlogger.info(record.kinesis.data_as_text)\n    payload: dict = record.kinesis.data_as_json()\n    ...\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\nbatch = event[\"Records\"]\nwith processor(records=batch, handler=record_handler):\nprocessed_messages = processor.process() # kick off processing, return list[tuple]\nreturn processor.response()\n</code></pre> <pre><code>from aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor\nfrom aws_lambda_powertools.utilities.data_classes.kinesis_stream_event import KinesisStreamRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\nprocessor = BatchProcessor(event_type=EventType.KinesisDataStreams)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: KinesisStreamRecord):\n    logger.info(record.kinesis.data_as_text)\n    payload: dict = record.kinesis.data_as_json()\n    ...\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\n@batch_processor(record_handler=record_handler, processor=processor)\ndef lambda_handler(event, context: LambdaContext):\nreturn processor.response()\n</code></pre> <p>The second record failed to be processed, therefore the processor added its sequence number in the response.</p> <pre><code>{\n    'batchItemFailures': [\n        {\n            'itemIdentifier': '6006958808509702859251049540584488075644979031228738'\n        }\n    ]\n}\n</code></pre> <pre><code>{\n\"Records\": [\n{\n\"kinesis\": {\n\"kinesisSchemaVersion\": \"1.0\",\n\"partitionKey\": \"1\",\n\"sequenceNumber\": \"4107859083838847772757075850904226111829882106684065\",\n\"data\": \"eyJNZXNzYWdlIjogInN1Y2Nlc3MifQ==\",\n\"approximateArrivalTimestamp\": 1545084650.987\n},\n\"eventSource\": \"aws:kinesis\",\n\"eventVersion\": \"1.0\",\n\"eventID\": \"shardId-000000000006:4107859083838847772757075850904226111829882106684065\",\n\"eventName\": \"aws:kinesis:record\",\n\"invokeIdentityArn\": \"arn:aws:iam::123456789012:role/lambda-role\",\n\"awsRegion\": \"us-east-2\",\n\"eventSourceARN\": \"arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream\"\n},\n{\n\"kinesis\": {\n\"kinesisSchemaVersion\": \"1.0\",\n\"partitionKey\": \"1\",\n\"sequenceNumber\": \"6006958808509702859251049540584488075644979031228738\",\n\"data\": \"c3VjY2Vzcw==\",\n\"approximateArrivalTimestamp\": 1545084650.987\n},\n\"eventSource\": \"aws:kinesis\",\n\"eventVersion\": \"1.0\",\n\"eventID\": \"shardId-000000000006:6006958808509702859251049540584488075644979031228738\",\n\"eventName\": \"aws:kinesis:record\",\n\"invokeIdentityArn\": \"arn:aws:iam::123456789012:role/lambda-role\",\n\"awsRegion\": \"us-east-2\",\n\"eventSourceARN\": \"arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream\"\n}\n]\n}\n</code></pre>"},{"location":"utilities/batch/#processing-messages-from-dynamodb","title":"Processing messages from DynamoDB","text":"<p>Processing batches from Kinesis works in three stages:</p> <ol> <li>Instantiate <code>BatchProcessor</code> and choose <code>EventType.DynamoDBStreams</code> for the event type</li> <li>Define your function to handle each batch record, and use <code>DynamoDBRecord</code> type annotation for autocompletion</li> <li>Use <code>process_partial_response</code> to kick off processing</li> </ol> Info <p>This code example optionally uses Tracer and Logger for completion.</p> RecommendedAs a context managerAs a decorator (legacy)Sample responseSample event <pre><code>import json\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import (\nBatchProcessor,\n    EventType,\n    process_partial_response,\n)\nfrom aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import (\nDynamoDBRecord,\n)\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nprocessor = BatchProcessor(event_type=EventType.DynamoDBStreams)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: DynamoDBRecord):\nlogger.info(record.dynamodb.new_image)  # type: ignore[union-attr]\n    payload: dict = json.loads(record.dynamodb.new_image.get(\"Message\"))  # type: ignore[union-attr,arg-type]\n    logger.info(payload)\n    ...\n\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\nreturn process_partial_response(event=event, record_handler=record_handler, processor=processor, context=context)\n</code></pre> <pre><code>import json\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, EventType\nfrom aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import DynamoDBRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\nprocessor = BatchProcessor(event_type=EventType.DynamoDBStreams)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: DynamoDBRecord):\nlogger.info(record.dynamodb.new_image)\n    payload: dict = json.loads(record.dynamodb.new_image.get(\"Message\"))\n    ...\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\nbatch = event[\"Records\"]\nwith processor(records=batch, handler=record_handler):\nprocessed_messages = processor.process() # kick off processing, return list[tuple]\nreturn processor.response()\n</code></pre> <pre><code>import json\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor\nfrom aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import DynamoDBRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\nprocessor = BatchProcessor(event_type=EventType.DynamoDBStreams)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: DynamoDBRecord):\nlogger.info(record.dynamodb.new_image)\n    payload: dict = json.loads(record.dynamodb.new_image.get(\"Message\"))\n    ...\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\n@batch_processor(record_handler=record_handler, processor=processor)\ndef lambda_handler(event, context: LambdaContext):\nreturn processor.response()\n</code></pre> <p>The second record failed to be processed, therefore the processor added its sequence number in the response.</p> <pre><code>{\n    'batchItemFailures': [\n        {\n            'itemIdentifier': '8640712661'\n        }\n    ]\n}\n</code></pre> <pre><code>{\n\"Records\": [\n{\n\"eventID\": \"1\",\n\"eventVersion\": \"1.0\",\n\"dynamodb\": {\n\"Keys\": {\n\"Id\": {\n\"N\": \"101\"\n}\n},\n\"NewImage\": {\n\"Message\": {\n\"S\": \"failure\"\n}\n},\n\"StreamViewType\": \"NEW_AND_OLD_IMAGES\",\n\"SequenceNumber\": \"3275880929\",\n\"SizeBytes\": 26\n},\n\"awsRegion\": \"us-west-2\",\n\"eventName\": \"INSERT\",\n\"eventSourceARN\": \"eventsource_arn\",\n\"eventSource\": \"aws:dynamodb\"\n},\n{\n\"eventID\": \"1\",\n\"eventVersion\": \"1.0\",\n\"dynamodb\": {\n\"Keys\": {\n\"Id\": {\n\"N\": \"101\"\n}\n},\n\"NewImage\": {\n\"SomethingElse\": {\n\"S\": \"success\"\n}\n},\n\"StreamViewType\": \"NEW_AND_OLD_IMAGES\",\n\"SequenceNumber\": \"8640712661\",\n\"SizeBytes\": 26\n},\n\"awsRegion\": \"us-west-2\",\n\"eventName\": \"INSERT\",\n\"eventSourceARN\": \"eventsource_arn\",\n\"eventSource\": \"aws:dynamodb\"\n}\n]\n}\n</code></pre>"},{"location":"utilities/batch/#partial-failure-mechanics","title":"Partial failure mechanics","text":"<p>All records in the batch will be passed to this handler for processing, even if exceptions are thrown - Here's the behaviour after completing the batch:</p> <ul> <li>All records successfully processed. We will return an empty list of item failures <code>{'batchItemFailures': []}</code></li> <li>Partial success with some exceptions. We will return a list of all item IDs/sequence numbers that failed processing</li> <li>All records failed to be processed. We will raise <code>BatchProcessingError</code> exception with a list of all exceptions raised when processing</li> </ul>"},{"location":"utilities/batch/#processing-messages-asynchronously","title":"Processing messages asynchronously","text":"<p>New to AsyncIO? Read this comprehensive guide first.</p> <p>You can use <code>AsyncBatchProcessor</code> class and <code>async_process_partial_response</code> function to process messages concurrently.</p> When is this useful? <p>Your use case might be able to process multiple records at the same time without conflicting with one another.</p> <p>For example, imagine you need to process multiple loyalty points and incrementally save in a database. While you await the database to confirm your records are saved, you could start processing another request concurrently.</p> <p>The reason this is not the default behaviour is that not all use cases can handle concurrency safely (e.g., loyalty points must be updated in order).</p> High-concurrency with AsyncBatchProcessor<pre><code>import httpx  # external dependency\n\nfrom aws_lambda_powertools.utilities.batch import (\nAsyncBatchProcessor,\n    EventType,\n    async_process_partial_response,\n)\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nprocessor = AsyncBatchProcessor(event_type=EventType.SQS)\nasync def async_record_handler(record: SQSRecord):\n# Yield control back to the event loop to schedule other tasks\n    # while you await from a response from httpbin.org\n    async with httpx.AsyncClient() as client:\n        ret = await client.get(\"https://httpbin.org/get\")\n\n    return ret.status_code\n\n\ndef lambda_handler(event, context: LambdaContext):\nreturn async_process_partial_response(\nevent=event, record_handler=async_record_handler, processor=processor, context=context\n    )\n</code></pre> Using tracer? <p><code>AsyncBatchProcessor</code> uses <code>asyncio.gather</code> which can cause side effects and reach trace limits at high concurrency.</p> <p>See Tracing concurrent asynchronous functions.</p>"},{"location":"utilities/batch/#advanced","title":"Advanced","text":""},{"location":"utilities/batch/#pydantic-integration","title":"Pydantic integration","text":"<p>You can bring your own Pydantic models via <code>model</code> parameter when inheriting from <code>SqsRecordModel</code>, <code>KinesisDataStreamRecord</code>, or <code>DynamoDBStreamRecordModel</code></p> <p>Inheritance is importance because we need to access message IDs and sequence numbers from these records in the event of failure. Mypy is fully integrated with this utility, so it should identify whether you're passing the incorrect Model.</p> SQSKinesis Data StreamsDynamoDB Streams <pre><code>import json\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, process_partial_response\nfrom aws_lambda_powertools.utilities.parser.models import SqsRecordModel\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nfrom aws_lambda_powertools.utilities.parser import BaseModel\nfrom aws_lambda_powertools.utilities.parser.types import Json\n\n\nclass Order(BaseModel):\n    item: dict\n\nclass OrderSqsRecord(SqsRecordModel):\nbody: Json[Order]  # deserialize order data from JSON string\n\nprocessor = BatchProcessor(event_type=EventType.SQS, model=OrderSqsRecord)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: OrderSqsRecord):\nreturn record.body.item\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\nreturn process_partial_response(event=event, record_handler=record_handler, processor=processor, context=context)\n</code></pre> <pre><code>import json\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, process_partial_response\nfrom aws_lambda_powertools.utilities.parser.models import KinesisDataStreamRecordPayload, KinesisDataStreamRecord\nfrom aws_lambda_powertools.utilities.parser import BaseModel, validator\nfrom aws_lambda_powertools.utilities.parser.types import Json\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\nclass Order(BaseModel):\n    item: dict\n\n\nclass OrderKinesisPayloadRecord(KinesisDataStreamRecordPayload):\ndata: Json[Order]\n\n\nclass OrderKinesisRecord(KinesisDataStreamRecord):\nkinesis: OrderKinesisPayloadRecord\n\n\nprocessor = BatchProcessor(event_type=EventType.KinesisDataStreams, model=OrderKinesisRecord)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: OrderKinesisRecord):\nreturn record.kinesis.data.item\n\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\nreturn process_partial_response(event=event, record_handler=record_handler, processor=processor, context=context)\n</code></pre> <pre><code>import json\n\nfrom typing import Dict, Literal, Optional\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, process_partial_response\nfrom aws_lambda_powertools.utilities.parser.models import DynamoDBStreamChangedRecordModel, DynamoDBStreamRecordModel\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nfrom aws_lambda_powertools.utilities.parser import BaseModel, validator\n\n\nclass Order(BaseModel):\n    item: dict\n\n\nclass OrderDynamoDB(BaseModel):\nMessage: Order\n\n    # auto transform json string\n    # so Pydantic can auto-initialize nested Order model\n    @validator(\"Message\", pre=True)\n    def transform_message_to_dict(cls, value: Dict[Literal[\"S\"], str]):\n        return json.loads(value[\"S\"])\n\n\nclass OrderDynamoDBChangeRecord(DynamoDBStreamChangedRecordModel):\nNewImage: Optional[OrderDynamoDB]\n    OldImage: Optional[OrderDynamoDB]\n\n\nclass OrderDynamoDBRecord(DynamoDBStreamRecordModel):\ndynamodb: OrderDynamoDBChangeRecord\n\n\nprocessor = BatchProcessor(event_type=EventType.DynamoDBStreams, model=OrderDynamoDBRecord)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: OrderDynamoDBRecord):\nreturn record.dynamodb.NewImage.Message.item\n\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\n    return process_partial_response(event=event, record_handler=record_handler, processor=processor, context=context)\n</code></pre>"},{"location":"utilities/batch/#accessing-processed-messages","title":"Accessing processed messages","text":"<p>Use the context manager to access a list of all returned values from your <code>record_handler</code> function.</p> <ul> <li>When successful. We will include a tuple with <code>success</code>, the result of <code>record_handler</code>, and the batch record</li> <li>When failed. We will include a tuple with <code>fail</code>, exception as a string, and the batch record</li> </ul> Accessing processed messages via context manager<pre><code>import json\n\nfrom typing import Any, List, Literal, Union\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import (BatchProcessor,\n                                                   EventType,\n                                                   FailureResponse,\n                                                   SuccessResponse)\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\nprocessor = BatchProcessor(event_type=EventType.SQS)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: SQSRecord):\n    payload: str = record.body\n    if payload:\n        item: dict = json.loads(payload)\n    ...\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\n    batch = event[\"Records\"]\nwith processor(records=batch, handler=record_handler):\nprocessed_messages: List[Union[SuccessResponse, FailureResponse]] = processor.process()\nfor message in processed_messages:\nstatus: Union[Literal[\"success\"], Literal[\"fail\"]] = message[0]\nresult: Any = message[1]\nrecord: SQSRecord = message[2]\nreturn processor.response()\n</code></pre>"},{"location":"utilities/batch/#accessing-lambda-context","title":"Accessing Lambda Context","text":"<p>Within your <code>record_handler</code> function, you might need access to the Lambda context to determine how much time you have left before your function times out.</p> <p>We can automatically inject the Lambda context into your <code>record_handler</code> if your function signature has a parameter named <code>lambda_context</code>. When using a context manager, you also need to pass the Lambda context object like in the example below.</p> RecommendedAs a decorator (legacy)As a context manager <pre><code>import json\nfrom typing import Optional\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import (\n    BatchProcessor,\n    EventType,\n    process_partial_response,\n)\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nprocessor = BatchProcessor(event_type=EventType.SQS)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: SQSRecord, lambda_context: Optional[LambdaContext] = None):\npayload: str = record.body\n    if payload:\n        item: dict = json.loads(payload)\n        logger.info(item)\n    ...\n\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\n    return process_partial_response(event=event, record_handler=record_handler, processor=processor, context=context)\n</code></pre> <pre><code>from typing import Optional\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import (BatchProcessor, EventType,\n                                                   batch_processor)\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nprocessor = BatchProcessor(event_type=EventType.SQS)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: SQSRecord, lambda_context: Optional[LambdaContext] = None):\nif lambda_context is not None:\n        remaining_time = lambda_context.get_remaining_time_in_millis()\n    ...\n\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\n@batch_processor(record_handler=record_handler, processor=processor)\ndef lambda_handler(event, context: LambdaContext):\n    return processor.response()\n</code></pre> <pre><code>from typing import Optional\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, EventType\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nprocessor = BatchProcessor(event_type=EventType.SQS)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: SQSRecord, lambda_context: Optional[LambdaContext] = None):\nif lambda_context is not None:\n        remaining_time = lambda_context.get_remaining_time_in_millis()\n    ...\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\n    batch = event[\"Records\"]\nwith processor(records=batch, handler=record_handler, lambda_context=context):\nresult = processor.process()\n\n    return result\n</code></pre>"},{"location":"utilities/batch/#extending-batchprocessor","title":"Extending BatchProcessor","text":"<p>You might want to bring custom logic to the existing <code>BatchProcessor</code> to slightly override how we handle successes and failures.</p> <p>For these scenarios, you can subclass <code>BatchProcessor</code> and quickly override <code>success_handler</code> and <code>failure_handler</code> methods:</p> <ul> <li><code>success_handler()</code> \u2013 Keeps track of successful batch records</li> <li><code>failure_handler()</code> \u2013 Keeps track of failed batch records</li> </ul> Example <p>Let's suppose you'd like to add a metric named <code>BatchRecordFailures</code> for each batch record that failed processing</p> Extending failure handling mechanism in BatchProcessor<pre><code>from typing import Tuple\n\nfrom aws_lambda_powertools import Metrics\nfrom aws_lambda_powertools.metrics import MetricUnit\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, ExceptionInfo, EventType, FailureResponse, process_partial_response\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\n\n\nclass MyProcessor(BatchProcessor):\n    def failure_handler(self, record: SQSRecord, exception: ExceptionInfo) -&gt; FailureResponse:\n        metrics.add_metric(name=\"BatchRecordFailures\", unit=MetricUnit.Count, value=1)\n        return super().failure_handler(record, exception)\n\nprocessor = MyProcessor(event_type=EventType.SQS)\nmetrics = Metrics(namespace=\"test\")\n\n\n@tracer.capture_method\ndef record_handler(record: SQSRecord):\n    payload: str = record.body\n    if payload:\n        item: dict = json.loads(payload)\n    ...\n\n@metrics.log_metrics(capture_cold_start_metric=True)\ndef lambda_handler(event, context: LambdaContext):\n    return process_partial_response(event=event, record_handler=record_handler, processor=processor, context=context)\n</code></pre>"},{"location":"utilities/batch/#create-your-own-partial-processor","title":"Create your own partial processor","text":"<p>You can create your own partial batch processor from scratch by inheriting the <code>BasePartialProcessor</code> class, and implementing <code>_prepare()</code>, <code>_clean()</code> and <code>_process_record()</code>.</p> <ul> <li><code>_process_record()</code> \u2013 handles all processing logic for each individual message of a batch, including calling the <code>record_handler</code> (self.handler)</li> <li><code>_prepare()</code> \u2013 called once as part of the processor initialization</li> <li><code>clean()</code> \u2013 teardown logic called once after <code>_process_record</code> completes</li> </ul> <p>You can then use this class as a context manager, or pass it to <code>batch_processor</code> to use as a decorator on your Lambda handler function.</p> Creating a custom batch processor<pre><code>from random import randint\n\nfrom aws_lambda_powertools.utilities.batch import BasePartialProcessor, batch_processor\nimport boto3\nimport os\n\ntable_name = os.getenv(\"TABLE_NAME\", \"table_not_found\")\n\nclass MyPartialProcessor(BasePartialProcessor):\n\"\"\"\n    Process a record and stores successful results at a Amazon DynamoDB Table\n\n    Parameters\n    ----------\n    table_name: str\n        DynamoDB table name to write results to\n    \"\"\"\n\n    def __init__(self, table_name: str):\n        self.table_name = table_name\n\n        super().__init__()\n\ndef _prepare(self):\n# It's called once, *before* processing\n        # Creates table resource and clean previous results\n        self.ddb_table = boto3.resource(\"dynamodb\").Table(self.table_name)\n        self.success_messages.clear()\n\ndef _clean(self):\n# It's called once, *after* closing processing all records (closing the context manager)\n        # Here we're sending, at once, all successful messages to a ddb table\n        with self.ddb_table.batch_writer() as batch:\n            for result in self.success_messages:\n                batch.put_item(Item=result)\n\ndef _process_record(self, record):\n# It handles how your record is processed\n        # Here we're keeping the status of each run\n        # where self.handler is the record_handler function passed as an argument\n        try:\n            result = self.handler(record) # record_handler passed to decorator/context manager\n            return self.success_handler(record, result)\n        except Exception as exc:\n            return self.failure_handler(record, exc)\n\n    def success_handler(self, record):\n        entry = (\"success\", result, record)\n        message = {\"age\": result}\n        self.success_messages.append(message)\n        return entry\n\n\ndef record_handler(record):\n    return randint(0, 100)\n\n@batch_processor(record_handler=record_handler, processor=MyPartialProcessor(table_name))\ndef lambda_handler(event, context):\n    return {\"statusCode\": 200}\n</code></pre>"},{"location":"utilities/batch/#caveats","title":"Caveats","text":""},{"location":"utilities/batch/#tracer-response-auto-capture-for-large-batch-sizes","title":"Tracer response auto-capture for large batch sizes","text":"<p>When using Tracer to capture responses for each batch record processing, you might exceed 64K of tracing data depending on what you return from your <code>record_handler</code> function, or how big is your batch size.</p> <p>If that's the case, you can configure Tracer to disable response auto-capturing.</p> Disabling Tracer response auto-capturing<pre><code>import json\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, batch_processor\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\nprocessor = BatchProcessor(event_type=EventType.SQS)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method(capture_response=False)\ndef record_handler(record: SQSRecord):\n    payload: str = record.body\n    if payload:\n        item: dict = json.loads(payload)\n    ...\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\n@batch_processor(record_handler=record_handler, processor=processor)\ndef lambda_handler(event, context: LambdaContext):\n    return processor.response()\n</code></pre>"},{"location":"utilities/batch/#testing-your-code","title":"Testing your code","text":"<p>As there is no external calls, you can unit test your code with <code>BatchProcessor</code> quite easily.</p> <p>Example:</p> <p>Given a SQS batch where the first batch record succeeds and the second fails processing, we should have a single item reported in the function response.</p> test_app.pysrc/app.pySample SQS event <pre><code>import json\n\nfrom pathlib import Path\nfrom dataclasses import dataclass\n\nimport pytest\nfrom src.app import lambda_handler, processor\n\n\ndef load_event(path: Path):\n    with path.open() as f:\n        return json.load(f)\n\n\n@pytest.fixture\ndef lambda_context():\n    @dataclass\n    class LambdaContext:\n        function_name: str = \"test\"\n        memory_limit_in_mb: int = 128\n        invoked_function_arn: str = \"arn:aws:lambda:eu-west-1:809313241:function:test\"\n        aws_request_id: str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\"\n\n    return LambdaContext()\n\n@pytest.fixture()\ndef sqs_event():\n\"\"\"Generates API GW Event\"\"\"\n    return load_event(path=Path(\"events/sqs_event.json\"))\n\n\ndef test_app_batch_partial_response(sqs_event, lambda_context):\n    # GIVEN\n    processor = app.processor  # access processor for additional assertions\n    successful_record = sqs_event[\"Records\"][0]\n    failed_record = sqs_event[\"Records\"][1]\n    expected_response = {\n        \"batchItemFailures: [\n            {\n                \"itemIdentifier\": failed_record[\"messageId\"]\n            }\n        ]\n    }\n\n    # WHEN\n    ret = app.lambda_handler(sqs_event, lambda_context)\n\n    # THEN\n    assert ret == expected_response\n    assert len(processor.fail_messages) == 1\n    assert processor.success_messages[0] == successful_record\n</code></pre> <pre><code>import json\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, EventType, process_partial_response\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\nprocessor = BatchProcessor(event_type=EventType.SQS)\ntracer = Tracer()\nlogger = Logger()\n\n\n@tracer.capture_method\ndef record_handler(record: SQSRecord):\n    payload: str = record.body\n    if payload:\n        item: dict = json.loads(payload)\n    ...\n\n@logger.inject_lambda_context\n@tracer.capture_lambda_handler\ndef lambda_handler(event, context: LambdaContext):\n    return process_partial_response(event=event, record_handler=record_handler, processor=processor, context=context)\n</code></pre> events/sqs_sample.json<pre><code>{\n\"Records\": [\n{\n\"messageId\": \"059f36b4-87a3-44ab-83d2-661975830a7d\",\n\"receiptHandle\": \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a\",\n\"body\": \"{\\\"Message\\\": \\\"success\\\"}\",\n\"attributes\": {\n\"ApproximateReceiveCount\": \"1\",\n\"SentTimestamp\": \"1545082649183\",\n\"SenderId\": \"AIDAIENQZJOLO23YVJ4VO\",\n\"ApproximateFirstReceiveTimestamp\": \"1545082649185\"\n},\n\"messageAttributes\": {},\n\"md5OfBody\": \"e4e68fb7bd0e697a0ae8f1bb342846b3\",\n\"eventSource\": \"aws:sqs\",\n\"eventSourceARN\": \"arn:aws:sqs:us-east-2: 123456789012:my-queue\",\n\"awsRegion\": \"us-east-1\"\n},\n{\n\"messageId\": \"244fc6b4-87a3-44ab-83d2-361172410c3a\",\n\"receiptHandle\": \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a\",\n\"body\": \"SGVsbG8sIHRoaXMgaXMgYSB0ZXN0Lg==\",\n\"attributes\": {\n\"ApproximateReceiveCount\": \"1\",\n\"SentTimestamp\": \"1545082649183\",\n\"SenderId\": \"AIDAIENQZJOLO23YVJ4VO\",\n\"ApproximateFirstReceiveTimestamp\": \"1545082649185\"\n},\n\"messageAttributes\": {},\n\"md5OfBody\": \"e4e68fb7bd0e697a0ae8f1bb342846b3\",\n\"eventSource\": \"aws:sqs\",\n\"eventSourceARN\": \"arn:aws:sqs:us-east-2: 123456789012:my-queue\",\n\"awsRegion\": \"us-east-1\"\n}\n]\n}\n</code></pre>"},{"location":"utilities/batch/#faq","title":"FAQ","text":""},{"location":"utilities/batch/#choosing-between-decorator-and-context-manager","title":"Choosing between decorator and context manager","text":"<p>Use context manager when you want access to the processed messages or handle <code>BatchProcessingError</code> exception when all records within the batch fail to be processed.</p>"},{"location":"utilities/batch/#whats-the-difference-between-the-decorator-and-process_partial_response-functions","title":"What's the difference between the decorator and process_partial_response functions?","text":"<p><code>batch_processor</code> and <code>async_batch_processor</code> decorators are now considered legacy. Historically, they were kept due to backwards compatibility and to minimize code changes between V1 and V2.</p> <p>As 2.12.0, <code>process_partial_response</code> and <code>async_process_partial_response</code> are the recommended instead. It reduces boilerplate, smaller memory/CPU cycles, and it makes it less error prone - e.g., decorators required an additional return.</p>"},{"location":"utilities/batch/#integrating-exception-handling-with-sentryio","title":"Integrating exception handling with Sentry.io","text":"<p>When using Sentry.io for error monitoring, you can override <code>failure_handler</code> to capture each processing exception with Sentry SDK:</p> <p>Credits to Charles-Axel Dein</p> Integrating error tracking with Sentry.io<pre><code>from typing import Tuple\n\nfrom aws_lambda_powertools.utilities.batch import BatchProcessor, FailureResponse\nfrom sentry_sdk import capture_exception\nclass MyProcessor(BatchProcessor):\ndef failure_handler(self, record, exception) -&gt; FailureResponse:\ncapture_exception()  # send exception to Sentry\n        return super().failure_handler(record, exception)\n</code></pre>"},{"location":"utilities/data_classes/","title":"Event Source Data Classes","text":"<p>Event Source Data Classes utility provides classes self-describing Lambda event sources.</p>"},{"location":"utilities/data_classes/#key-features","title":"Key Features","text":"<ul> <li>Type hinting and code completion for common event types</li> <li>Helper functions for decoding/deserializing nested fields</li> <li>Docstrings for fields contained in event schemas</li> </ul> <p>Background</p> <p>When authoring Lambda functions, you often need to understand the schema of the event dictionary which is passed to the handler. There are several common event types which follow a specific schema, depending on the service triggering the Lambda function.</p>"},{"location":"utilities/data_classes/#getting-started","title":"Getting started","text":""},{"location":"utilities/data_classes/#utilizing-the-data-classes","title":"Utilizing the data classes","text":"<p>The classes are initialized by passing in the Lambda event object into the constructor of the appropriate data class or by using the <code>event_source</code> decorator.</p> <p>For example, if your Lambda function is being triggered by an API Gateway proxy integration, you can use the <code>APIGatewayProxyEvent</code> class.</p> app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent\ndef lambda_handler(event: dict, context):\nevent = APIGatewayProxyEvent(event)\nif 'helloworld' in event.path and event.http_method == 'GET':\n        do_something_with(event.body, user)\n</code></pre> <p>Same example as above, but using the <code>event_source</code> decorator</p> app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, APIGatewayProxyEvent\n@event_source(data_class=APIGatewayProxyEvent)\ndef lambda_handler(event: APIGatewayProxyEvent, context):\n    if 'helloworld' in event.path and event.http_method == 'GET':\n        do_something_with(event.body, user)\n</code></pre> <p>Log Data Event for Troubleshooting</p> app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, APIGatewayProxyEvent\nfrom aws_lambda_powertools.logging.logger import Logger\n\nlogger = Logger(service=\"hello_logs\", level=\"DEBUG\")\n@event_source(data_class=APIGatewayProxyEvent)\ndef lambda_handler(event: APIGatewayProxyEvent, context):\nlogger.debug(event)\n</code></pre> <p>Autocomplete with self-documented properties and methods</p> <p></p>"},{"location":"utilities/data_classes/#supported-event-sources","title":"Supported event sources","text":"Event Source Data_class Active MQ <code>ActiveMQEvent</code> API Gateway Authorizer <code>APIGatewayAuthorizerRequestEvent</code> API Gateway Authorizer V2 <code>APIGatewayAuthorizerEventV2</code> API Gateway Proxy <code>APIGatewayProxyEvent</code> API Gateway Proxy V2 <code>APIGatewayProxyEventV2</code> Application Load Balancer <code>ALBEvent</code> AppSync Authorizer <code>AppSyncAuthorizerEvent</code> AppSync Resolver <code>AppSyncResolverEvent</code> CloudWatch Dashboard Custom Widget <code>CloudWatchDashboardCustomWidgetEvent</code> CloudWatch Logs <code>CloudWatchLogsEvent</code> CodePipeline Job Event <code>CodePipelineJobEvent</code> Cognito User Pool Multiple available under <code>cognito_user_pool_event</code> Connect Contact Flow <code>ConnectContactFlowEvent</code> DynamoDB streams <code>DynamoDBStreamEvent</code>, <code>DynamoDBRecordEventName</code> EventBridge <code>EventBridgeEvent</code> Kafka <code>KafkaEvent</code> Kinesis Data Stream <code>KinesisStreamEvent</code> Kinesis Firehose Delivery Stream <code>KinesisFirehoseEvent</code> Lambda Function URL <code>LambdaFunctionUrlEvent</code> Rabbit MQ <code>RabbitMQEvent</code> S3 <code>S3Event</code> S3 Object Lambda <code>S3ObjectLambdaEvent</code> S3 EventBridge Notification <code>S3EventBridgeNotificationEvent</code> SES <code>SESEvent</code> SNS <code>SNSEvent</code> SQS <code>SQSEvent</code> Info <p>The examples provided below are far from exhaustive - the data classes themselves are designed to provide a form of documentation inherently (via autocompletion, types and docstrings).</p>"},{"location":"utilities/data_classes/#active-mq","title":"Active MQ","text":"<p>It is used for Active MQ payloads, also see the AWS blog post for more details.</p> app.py <pre><code>from typing import Dict\n\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.data_classes import event_source\nfrom aws_lambda_powertools.utilities.data_classes.active_mq_event import ActiveMQEvent\nlogger = Logger()\n\n@event_source(data_class=ActiveMQEvent)\ndef lambda_handler(event: ActiveMQEvent, context):\nfor message in event.messages:\n        logger.debug(f\"MessageID: {message.message_id}\")\n        data: Dict = message.json_data\n        logger.debug(\"Process json in base64 encoded data str\", data)\n</code></pre>"},{"location":"utilities/data_classes/#api-gateway-authorizer","title":"API Gateway Authorizer","text":"<p>New in 1.20.0</p> <p>It is used for API Gateway Rest API Lambda Authorizer payload.</p> <p>Use <code>APIGatewayAuthorizerRequestEvent</code> for type <code>REQUEST</code> and <code>APIGatewayAuthorizerTokenEvent</code> for type <code>TOKEN</code>.</p> app_type_request.pyapp_type_token.py <p>This example uses the <code>APIGatewayAuthorizerResponse</code> to decline a given request if the user is not found.</p> <p>When the user is found, it includes the user details in the request context that will be available to the back-end, and returns a full access policy for admin users.</p> <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source\nfrom aws_lambda_powertools.utilities.data_classes.api_gateway_authorizer_event import (\nDENY_ALL_RESPONSE,\nAPIGatewayAuthorizerRequestEvent,\nAPIGatewayAuthorizerResponse,\nHttpVerb,\n)\nfrom secrets import compare_digest\n\n\ndef get_user_by_token(token):\n    if compare_digest(token, \"admin-foo\"):\n        return {\"id\": 0, \"name\": \"Admin\", \"isAdmin\": True}\n    elif compare_digest(token, \"regular-foo\"):\n        return {\"id\": 1, \"name\": \"Joe\"}\n    else:\n        return None\n\n\n@event_source(data_class=APIGatewayAuthorizerRequestEvent)\ndef handler(event: APIGatewayAuthorizerRequestEvent, context):\n    user = get_user_by_token(event.get_header_value(\"Authorization\"))\n\n    if user is None:\n        # No user was found\n        # to return 401 - `{\"message\":\"Unauthorized\"}`, but pollutes lambda error count metrics\n        # raise Exception(\"Unauthorized\")\n        # to return 403 - `{\"message\":\"Forbidden\"}`\nreturn DENY_ALL_RESPONSE\n# parse the `methodArn` as an `APIGatewayRouteArn`\n    arn = event.parsed_arn\n\n    # Create the response builder from parts of the `methodArn`\n    # and set the logged in user id and context\npolicy = APIGatewayAuthorizerResponse(\nprincipal_id=user[\"id\"],\ncontext=user,\nregion=arn.region,\naws_account_id=arn.aws_account_id,\napi_id=arn.api_id,\nstage=arn.stage,\n)\n\n    # Conditional IAM Policy\n    if user.get(\"isAdmin\", False):\npolicy.allow_all_routes()\nelse:\npolicy.allow_route(HttpVerb.GET, \"/user-profile\")\nreturn policy.asdict()\n</code></pre> <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source\nfrom aws_lambda_powertools.utilities.data_classes.api_gateway_authorizer_event import (\nAPIGatewayAuthorizerTokenEvent,\nAPIGatewayAuthorizerResponse,\n)\n@event_source(data_class=APIGatewayAuthorizerTokenEvent)\ndef handler(event: APIGatewayAuthorizerTokenEvent, context):\n    arn = event.parsed_arn\n\npolicy = APIGatewayAuthorizerResponse(\nprincipal_id=\"user\",\nregion=arn.region,\naws_account_id=arn.aws_account_id,\napi_id=arn.api_id,\nstage=arn.stage\n)\nif event.authorization_token == \"42\":\npolicy.allow_all_routes()\nelse:\npolicy.deny_all_routes()\nreturn policy.asdict()\n</code></pre>"},{"location":"utilities/data_classes/#api-gateway-authorizer-v2","title":"API Gateway Authorizer V2","text":"<p>New in 1.20.0</p> <p>It is used for API Gateway HTTP API Lambda Authorizer payload version 2. See also this blog post for more details.</p> app.py <p>This example looks up user details via <code>x-token</code> header. It uses <code>APIGatewayAuthorizerResponseV2</code> to return a deny policy when user is not found or authorized.</p> <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source\nfrom aws_lambda_powertools.utilities.data_classes.api_gateway_authorizer_event import (\nAPIGatewayAuthorizerEventV2,\nAPIGatewayAuthorizerResponseV2,\n)\nfrom secrets import compare_digest\n\n\ndef get_user_by_token(token):\n    if compare_digest(token, \"Foo\"):\n        return {\"name\": \"Foo\"}\n    return None\n\n\n@event_source(data_class=APIGatewayAuthorizerEventV2)\ndef handler(event: APIGatewayAuthorizerEventV2, context):\n    user = get_user_by_token(event.get_header_value(\"x-token\"))\n\n    if user is None:\n        # No user was found, so we return not authorized\nreturn APIGatewayAuthorizerResponseV2().asdict()\n# Found the user and setting the details in the context\nreturn APIGatewayAuthorizerResponseV2(authorize=True, context=user).asdict()\n</code></pre>"},{"location":"utilities/data_classes/#api-gateway-proxy","title":"API Gateway Proxy","text":"<p>It is used for either API Gateway REST API or HTTP API using v1 proxy event.</p> app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, APIGatewayProxyEvent\n\n@event_source(data_class=APIGatewayProxyEvent)\ndef lambda_handler(event: APIGatewayProxyEvent, context):\n    if \"helloworld\" in event.path and event.http_method == \"GET\":\n        request_context = event.request_context\n        identity = request_context.identity\n        user = identity.user\n        do_something_with(event.json_body, user)\n</code></pre>"},{"location":"utilities/data_classes/#api-gateway-proxy-v2","title":"API Gateway Proxy V2","text":"<p>It is used for HTTP API using v2 proxy event.</p> app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, APIGatewayProxyEventV2\n\n@event_source(data_class=APIGatewayProxyEventV2)\ndef lambda_handler(event: APIGatewayProxyEventV2, context):\n    if \"helloworld\" in event.path and event.http_method == \"POST\":\n        do_something_with(event.json_body, event.query_string_parameters)\n</code></pre>"},{"location":"utilities/data_classes/#application-load-balancer","title":"Application Load Balancer","text":"<p>Is it used for Application load balancer event.</p> app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, ALBEvent\n\n@event_source(data_class=ALBEvent)\ndef lambda_handler(event: ALBEvent, context):\n    if \"helloworld\" in event.path and event.http_method == \"POST\":\n        do_something_with(event.json_body, event.query_string_parameters)\n</code></pre>"},{"location":"utilities/data_classes/#appsync-authorizer","title":"AppSync Authorizer","text":"<p>New in 1.20.0</p> <p>Used when building an AWS_LAMBDA Authorization with AppSync. See blog post Introducing Lambda authorization for AWS AppSync GraphQL APIs or read the Amplify documentation on using AWS Lambda for authorization with AppSync.</p> <p>In this example extract the <code>requestId</code> as the <code>correlation_id</code> for logging, used <code>@event_source</code> decorator and builds the AppSync authorizer using the <code>AppSyncAuthorizerResponse</code> helper.</p> app.py <pre><code>from typing import Dict\n\nfrom aws_lambda_powertools.logging import correlation_paths\nfrom aws_lambda_powertools.logging.logger import Logger\nfrom aws_lambda_powertools.utilities.data_classes.appsync_authorizer_event import (\n    AppSyncAuthorizerEvent,\n    AppSyncAuthorizerResponse,\n)\nfrom aws_lambda_powertools.utilities.data_classes.event_source import event_source\n\nlogger = Logger()\n\n\ndef get_user_by_token(token: str):\n\"\"\"Look a user by token\"\"\"\n    ...\n\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPSYNC_AUTHORIZER)\n@event_source(data_class=AppSyncAuthorizerEvent)\ndef lambda_handler(event: AppSyncAuthorizerEvent, context) -&gt; Dict:\n    user = get_user_by_token(event.authorization_token)\n\n    if not user:\n        # No user found, return not authorized\n        return AppSyncAuthorizerResponse().asdict()\n\n    return AppSyncAuthorizerResponse(\n        authorize=True,\n        resolver_context={\"id\": user.id},\n        # Only allow admins to delete events\n        deny_fields=None if user.is_admin else [\"Mutation.deleteEvent\"],\n    ).asdict()\n</code></pre>"},{"location":"utilities/data_classes/#appsync-resolver","title":"AppSync Resolver","text":"<p>New in 1.12.0</p> <p>Used when building Lambda GraphQL Resolvers with Amplify GraphQL Transform Library (<code>@function</code>), and AppSync Direct Lambda Resolvers.</p> <p>In this example, we also use the new Logger <code>correlation_id</code> and built-in <code>correlation_paths</code> to extract, if available, X-Ray Trace ID in AppSync request headers:</p> app.pyExample AppSync EventExample CloudWatch Log <pre><code>from aws_lambda_powertools.logging import Logger, correlation_paths\nfrom aws_lambda_powertools.utilities.data_classes.appsync_resolver_event import (\nAppSyncResolverEvent,\nAppSyncIdentityCognito\n)\nlogger = Logger()\n\ndef get_locations(name: str = None, size: int = 0, page: int = 0):\n\"\"\"Your resolver logic here\"\"\"\n\n@logger.inject_lambda_context(correlation_id_path=correlation_paths.APPSYNC_RESOLVER)\ndef lambda_handler(event, context):\nevent: AppSyncResolverEvent = AppSyncResolverEvent(event)\n# Case insensitive look up of request headers\n    x_forwarded_for = event.get_header_value(\"x-forwarded-for\")\n\n# Support for AppSyncIdentityCognito or AppSyncIdentityIAM identity types\nassert isinstance(event.identity, AppSyncIdentityCognito)\nidentity: AppSyncIdentityCognito = event.identity\n# Logging with correlation_id\n    logger.debug({\n        \"x-forwarded-for\": x_forwarded_for,\n        \"username\": identity.username\n    })\n\nif event.type_name == \"Merchant\" and event.field_name == \"locations\":\nreturn get_locations(**event.arguments)\nraise ValueError(f\"Unsupported field resolver: {event.field_name}\")\n</code></pre> <pre><code>{\n\"typeName\": \"Merchant\",\n\"fieldName\": \"locations\",\n\"arguments\": {\n\"page\": 2,\n\"size\": 1,\n\"name\": \"value\"\n},\n\"identity\": {\n\"claims\": {\n\"iat\": 1615366261\n...\n},\n\"username\": \"mike\",\n...\n},\n\"request\": {\n\"headers\": {\n\"x-amzn-trace-id\": \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\",\n\"x-forwarded-for\": \"127.0.0.1\"\n...\n}\n},\n...\n}\n</code></pre> <pre><code>{\n\"level\":\"DEBUG\",\n\"location\":\"lambda_handler:22\",\n\"message\":{\n\"x-forwarded-for\":\"127.0.0.1\",\n\"username\":\"mike\"\n},\n\"timestamp\":\"2021-03-10 12:38:40,062\",\n\"service\":\"service_undefined\",\n\"sampling_rate\":0.0,\n\"cold_start\":true,\n\"function_name\":\"func_name\",\n\"function_memory_size\":512,\n\"function_arn\":\"func_arn\",\n\"function_request_id\":\"6735a29c-c000-4ae3-94e6-1f1c934f7f94\",\n\"correlation_id\":\"Root=1-60488877-0b0c4e6727ab2a1c545babd0\"\n}\n</code></pre>"},{"location":"utilities/data_classes/#cloudwatch-dashboard-custom-widget","title":"CloudWatch Dashboard Custom Widget","text":"app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, CloudWatchDashboardCustomWidgetEvent\n\nconst DOCS = `\n## Echo\nA simple echo script. Anything passed in \\`\\`\\`echo\\`\\`\\` parameter is returned as the content of custom widget.\n\n### Widget parameters\n| Param    | Description              |\n| -------- | ------------------------ |\n| **echo** | The content to echo back |\n\n### Example parameters\n\\`\\`\\` yaml\necho: &lt;h1&gt;Hello world&lt;/h1&gt;\n\\`\\`\\`\n`\n\n@event_source(data_class=CloudWatchDashboardCustomWidgetEvent)\ndef lambda_handler(event: CloudWatchDashboardCustomWidgetEvent, context):\n\n    if event.describe:\n        return DOCS\n\n    # You can directly return HTML or JSON content\n    # Alternatively, you can return markdown that will be rendered by CloudWatch\n    echo = event.widget_context.params[\"echo\"]\n    return { \"markdown\": f\"# {echo}\" }\n</code></pre>"},{"location":"utilities/data_classes/#cloudwatch-logs","title":"CloudWatch Logs","text":"<p>CloudWatch Logs events by default are compressed and base64 encoded. You can use the helper function provided to decode, decompress and parse json data from the event.</p> app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, CloudWatchLogsEvent\nfrom aws_lambda_powertools.utilities.data_classes.cloud_watch_logs_event import CloudWatchLogsDecodedData\n\n@event_source(data_class=CloudWatchLogsEvent)\ndef lambda_handler(event: CloudWatchLogsEvent, context):\n    decompressed_log: CloudWatchLogsDecodedData = event.parse_logs_data()\n    log_events = decompressed_log.log_events\n    for event in log_events:\n        do_something_with(event.timestamp, event.message)\n</code></pre>"},{"location":"utilities/data_classes/#kinesis-integration","title":"Kinesis integration","text":"<p>When streaming CloudWatch Logs to a Kinesis Data Stream (cross-account or not), you can use <code>extract_cloudwatch_logs_from_event</code> to decode, decompress and extract logs as <code>CloudWatchLogsDecodedData</code> to ease log processing.</p> app.py <pre><code>from typing import List\n\nfrom aws_lambda_powertools.utilities.data_classes import event_source\nfrom aws_lambda_powertools.utilities.data_classes.cloud_watch_logs_event import CloudWatchLogsDecodedData\nfrom aws_lambda_powertools.utilities.data_classes.kinesis_stream_event import (\nKinesisStreamEvent, extract_cloudwatch_logs_from_event)\n@event_source(data_class=KinesisStreamEvent)\ndef simple_handler(event: KinesisStreamEvent, context):\nlogs: List[CloudWatchLogsDecodedData] = extract_cloudwatch_logs_from_event(event)\nfor log in logs:\n        if log.message_type == \"DATA_MESSAGE\":\n            return \"success\"\n    return \"nothing to be processed\"\n</code></pre> <p>Alternatively, you can use <code>extract_cloudwatch_logs_from_record</code> to seamless integrate with the Batch utility for more robust log processing.</p> app.py <pre><code>from aws_lambda_powertools.utilities.batch import (BatchProcessor, EventType,\n                                                   batch_processor)\nfrom aws_lambda_powertools.utilities.data_classes.kinesis_stream_event import (\nKinesisStreamRecord, extract_cloudwatch_logs_from_record)\nprocessor = BatchProcessor(event_type=EventType.KinesisDataStreams)\n\n\ndef record_handler(record: KinesisStreamRecord):\nlog = extract_cloudwatch_logs_from_record(record)\nreturn log.message_type == \"DATA_MESSAGE\"\n\n\n@batch_processor(record_handler=record_handler, processor=processor)\ndef lambda_handler(event, context):\n    return processor.response()\n</code></pre>"},{"location":"utilities/data_classes/#codepipeline-job","title":"CodePipeline Job","text":"<p>Data classes and utility functions to help create continuous delivery pipelines tasks with AWS Lambda</p> app.py <pre><code>from aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.data_classes import event_source, CodePipelineJobEvent\n\nlogger = Logger()\n\n@event_source(data_class=CodePipelineJobEvent)\ndef lambda_handler(event, context):\n\"\"\"The Lambda function handler\n\n    If a continuing job then checks the CloudFormation stack status\n    and updates the job accordingly.\n\n    If a new job then kick of an update or creation of the target\n    CloudFormation stack.\n    \"\"\"\n\n    # Extract the Job ID\n    job_id = event.get_id\n\n    # Extract the params\n    params: dict = event.decoded_user_parameters\n    stack = params[\"stack\"]\n    artifact_name = params[\"artifact\"]\n    template_file = params[\"file\"]\n\n    try:\n        if event.data.continuation_token:\n            # If we're continuing then the create/update has already been triggered\n            # we just need to check if it has finished.\n            check_stack_update_status(job_id, stack)\n        else:\n            template = event.get_artifact(artifact_name, template_file)\n            # Kick off a stack update or create\n            start_update_or_create(job_id, stack, template)\n    except Exception as e:\n        # If any other exceptions which we didn't expect are raised\n        # then fail the job and log the exception message.\n        logger.exception(\"Function failed due to exception.\")\n        put_job_failure(job_id, \"Function exception: \" + str(e))\n\n    logger.debug(\"Function complete.\")\n    return \"Complete.\"\n</code></pre>"},{"location":"utilities/data_classes/#cognito-user-pool","title":"Cognito User Pool","text":"<p>Cognito User Pools have several different Lambda trigger sources, all of which map to a different data class, which can be imported from <code>aws_lambda_powertools.data_classes.cognito_user_pool_event</code>:</p> Trigger/Event Source Data Class Custom message event <code>data_classes.cognito_user_pool_event.CustomMessageTriggerEvent</code> Post authentication <code>data_classes.cognito_user_pool_event.PostAuthenticationTriggerEvent</code> Post confirmation <code>data_classes.cognito_user_pool_event.PostConfirmationTriggerEvent</code> Pre authentication <code>data_classes.cognito_user_pool_event.PreAuthenticationTriggerEvent</code> Pre sign-up <code>data_classes.cognito_user_pool_event.PreSignUpTriggerEvent</code> Pre token generation <code>data_classes.cognito_user_pool_event.PreTokenGenerationTriggerEvent</code> User migration <code>data_classes.cognito_user_pool_event.UserMigrationTriggerEvent</code> Define Auth Challenge <code>data_classes.cognito_user_pool_event.DefineAuthChallengeTriggerEvent</code> Create Auth Challenge <code>data_classes.cognito_user_pool_event.CreateAuthChallengeTriggerEvent</code> Verify Auth Challenge <code>data_classes.cognito_user_pool_event.VerifyAuthChallengeResponseTriggerEvent</code>"},{"location":"utilities/data_classes/#post-confirmation-example","title":"Post Confirmation Example","text":"app.py <pre><code>from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import PostConfirmationTriggerEvent\n\ndef lambda_handler(event, context):\n    event: PostConfirmationTriggerEvent = PostConfirmationTriggerEvent(event)\n\n    user_attributes = event.request.user_attributes\n    do_something_with(user_attributes)\n</code></pre>"},{"location":"utilities/data_classes/#define-auth-challenge-example","title":"Define Auth Challenge Example","text":"Note <p>In this example we are modifying the wrapped dict response fields, so we need to return the json serializable wrapped event in <code>event.raw_event</code>.</p> <p>This example is based on the AWS Cognito docs for Define Auth Challenge Lambda Trigger.</p> app.pySPR_A responsePASSWORD_VERIFIER success responseCUSTOM_CHALLENGE success response <pre><code>from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import DefineAuthChallengeTriggerEvent\n\ndef handler(event: dict, context) -&gt; dict:\n    event: DefineAuthChallengeTriggerEvent = DefineAuthChallengeTriggerEvent(event)\n    if (\n        len(event.request.session) == 1\n        and event.request.session[0].challenge_name == \"SRP_A\"\n    ):\n        event.response.issue_tokens = False\n        event.response.fail_authentication = False\n        event.response.challenge_name = \"PASSWORD_VERIFIER\"\n    elif (\n        len(event.request.session) == 2\n        and event.request.session[1].challenge_name == \"PASSWORD_VERIFIER\"\n        and event.request.session[1].challenge_result\n    ):\n        event.response.issue_tokens = False\n        event.response.fail_authentication = False\n        event.response.challenge_name = \"CUSTOM_CHALLENGE\"\n    elif (\n        len(event.request.session) == 3\n        and event.request.session[2].challenge_name == \"CUSTOM_CHALLENGE\"\n        and event.request.session[2].challenge_result\n    ):\n        event.response.issue_tokens = True\n        event.response.fail_authentication = False\n    else:\n        event.response.issue_tokens = False\n        event.response.fail_authentication = True\n\n    return event.raw_event\n</code></pre> <pre><code>{\n\"version\": \"1\",\n\"region\": \"us-east-1\",\n\"userPoolId\": \"us-east-1_example\",\n\"userName\": \"UserName\",\n\"callerContext\": {\n\"awsSdkVersion\": \"awsSdkVersion\",\n\"clientId\": \"clientId\"\n},\n\"triggerSource\": \"DefineAuthChallenge_Authentication\",\n\"request\": {\n\"userAttributes\": {\n\"sub\": \"4A709A36-7D63-4785-829D-4198EF10EBDA\",\n\"email_verified\": \"true\",\n\"name\": \"First Last\",\n\"email\": \"define-auth@mail.com\"\n},\n\"session\": [\n{\n\"challengeName\": \"SRP_A\",\n\"challengeResult\": true\n}\n]\n},\n\"response\": {\n\"issueTokens\": false,\n\"failAuthentication\": false,\n\"challengeName\": \"PASSWORD_VERIFIER\"\n}\n}\n</code></pre> <pre><code>{\n\"version\": \"1\",\n\"region\": \"us-east-1\",\n\"userPoolId\": \"us-east-1_example\",\n\"userName\": \"UserName\",\n\"callerContext\": {\n\"awsSdkVersion\": \"awsSdkVersion\",\n\"clientId\": \"clientId\"\n},\n\"triggerSource\": \"DefineAuthChallenge_Authentication\",\n\"request\": {\n\"userAttributes\": {\n\"sub\": \"4A709A36-7D63-4785-829D-4198EF10EBDA\",\n\"email_verified\": \"true\",\n\"name\": \"First Last\",\n\"email\": \"define-auth@mail.com\"\n},\n\"session\": [\n{\n\"challengeName\": \"SRP_A\",\n\"challengeResult\": true\n},\n{\n\"challengeName\": \"PASSWORD_VERIFIER\",\n\"challengeResult\": true\n}\n]\n},\n\"response\": {\n\"issueTokens\": false,\n\"failAuthentication\": false,\n\"challengeName\": \"CUSTOM_CHALLENGE\"\n}\n}\n</code></pre> <pre><code>{\n\"version\": \"1\",\n\"region\": \"us-east-1\",\n\"userPoolId\": \"us-east-1_example\",\n\"userName\": \"UserName\",\n\"callerContext\": {\n\"awsSdkVersion\": \"awsSdkVersion\",\n\"clientId\": \"clientId\"\n},\n\"triggerSource\": \"DefineAuthChallenge_Authentication\",\n\"request\": {\n\"userAttributes\": {\n\"sub\": \"4A709A36-7D63-4785-829D-4198EF10EBDA\",\n\"email_verified\": \"true\",\n\"name\": \"First Last\",\n\"email\": \"define-auth@mail.com\"\n},\n\"session\": [\n{\n\"challengeName\": \"SRP_A\",\n\"challengeResult\": true\n},\n{\n\"challengeName\": \"PASSWORD_VERIFIER\",\n\"challengeResult\": true\n},\n{\n\"challengeName\": \"CUSTOM_CHALLENGE\",\n\"challengeResult\": true\n}\n]\n},\n\"response\": {\n\"issueTokens\": true,\n\"failAuthentication\": false\n}\n}\n</code></pre>"},{"location":"utilities/data_classes/#create-auth-challenge-example","title":"Create Auth Challenge Example","text":"<p>This example is based on the AWS Cognito docs for Create Auth Challenge Lambda Trigger.</p> app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source\nfrom aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import CreateAuthChallengeTriggerEvent\n\n@event_source(data_class=CreateAuthChallengeTriggerEvent)\ndef handler(event: CreateAuthChallengeTriggerEvent, context) -&gt; dict:\n    if event.request.challenge_name == \"CUSTOM_CHALLENGE\":\n        event.response.public_challenge_parameters = {\"captchaUrl\": \"url/123.jpg\"}\n        event.response.private_challenge_parameters = {\"answer\": \"5\"}\n        event.response.challenge_metadata = \"CAPTCHA_CHALLENGE\"\n    return event.raw_event\n</code></pre>"},{"location":"utilities/data_classes/#verify-auth-challenge-response-example","title":"Verify Auth Challenge Response Example","text":"<p>This example is based on the AWS Cognito docs for Verify Auth Challenge Response Lambda Trigger.</p> app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source\nfrom aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import VerifyAuthChallengeResponseTriggerEvent\n\n@event_source(data_class=VerifyAuthChallengeResponseTriggerEvent)\ndef handler(event: VerifyAuthChallengeResponseTriggerEvent, context) -&gt; dict:\n    event.response.answer_correct = (\n        event.request.private_challenge_parameters.get(\"answer\") == event.request.challenge_answer\n    )\n    return event.raw_event\n</code></pre>"},{"location":"utilities/data_classes/#connect-contact-flow","title":"Connect Contact Flow","text":"<p>New in 1.11.0</p> app.py <pre><code>from aws_lambda_powertools.utilities.data_classes.connect_contact_flow_event import (\n    ConnectContactFlowChannel,\n    ConnectContactFlowEndpointType,\n    ConnectContactFlowEvent,\n    ConnectContactFlowInitiationMethod,\n)\n\ndef lambda_handler(event, context):\n    event: ConnectContactFlowEvent = ConnectContactFlowEvent(event)\n    assert event.contact_data.attributes == {\"Language\": \"en-US\"}\n    assert event.contact_data.channel == ConnectContactFlowChannel.VOICE\n    assert event.contact_data.customer_endpoint.endpoint_type == ConnectContactFlowEndpointType.TELEPHONE_NUMBER\n    assert event.contact_data.initiation_method == ConnectContactFlowInitiationMethod.API\n</code></pre>"},{"location":"utilities/data_classes/#dynamodb-streams","title":"DynamoDB Streams","text":"<p>The DynamoDB data class utility provides the base class for <code>DynamoDBStreamEvent</code>, as well as enums for stream view type (<code>StreamViewType</code>) and event type. (<code>DynamoDBRecordEventName</code>). The class automatically deserializes DynamoDB types into their equivalent Python types.</p> app.pymultiple_records_types.py <pre><code>from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import (\n    DynamoDBStreamEvent,\n    DynamoDBRecordEventName\n)\n\ndef lambda_handler(event, context):\n    event: DynamoDBStreamEvent = DynamoDBStreamEvent(event)\n\n    # Multiple records can be delivered in a single event\n    for record in event.records:\n        if record.event_name == DynamoDBRecordEventName.MODIFY:\n            do_something_with(record.dynamodb.new_image)\n            do_something_with(record.dynamodb.old_image)\n</code></pre> <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, DynamoDBStreamEvent\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\n@event_source(data_class=DynamoDBStreamEvent)\ndef lambda_handler(event: DynamoDBStreamEvent, context: LambdaContext):\n    for record in event.records:\n        # {\"N\": \"123.45\"} =&gt; Decimal(\"123.45\")\n        key: str = record.dynamodb.keys[\"id\"]\n        print(key)\n</code></pre>"},{"location":"utilities/data_classes/#eventbridge","title":"EventBridge","text":"app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, EventBridgeEvent\n\n@event_source(data_class=EventBridgeEvent)\ndef lambda_handler(event: EventBridgeEvent, context):\n    do_something_with(event.detail)\n</code></pre>"},{"location":"utilities/data_classes/#kafka","title":"Kafka","text":"<p>This example is based on the AWS docs for Amazon MSK and self-managed Apache Kafka.</p> app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, KafkaEvent\n\n@event_source(data_class=KafkaEvent)\ndef lambda_handler(event: KafkaEvent, context):\n    for record in event.records:\n        do_something_with(record.decoded_key, record.json_value)\n</code></pre>"},{"location":"utilities/data_classes/#kinesis-streams","title":"Kinesis streams","text":"<p>Kinesis events by default contain base64 encoded data. You can use the helper function to access the data either as json or plain text, depending on the original payload.</p> app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, KinesisStreamEvent\n\n@event_source(data_class=KinesisStreamEvent)\ndef lambda_handler(event: KinesisStreamEvent, context):\n    kinesis_record = next(event.records).kinesis\n\n    # if data was delivered as text\n    data = kinesis_record.data_as_text()\n\n    # if data was delivered as json\n    data = kinesis_record.data_as_json()\n\n    do_something_with(data)\n</code></pre>"},{"location":"utilities/data_classes/#kinesis-firehose-delivery-stream","title":"Kinesis Firehose delivery stream","text":"<p>Kinesis Firehose Data Transformation can use a Lambda Function to modify the records inline, and re-emit them back to the Delivery Stream.</p> <p>Similar to Kinesis Data Streams, the events contain base64 encoded data. You can use the helper function to access the data either as json or plain text, depending on the original payload.</p> app.py <pre><code>import base64\nimport json\n\nfrom aws_lambda_powertools.utilities.data_classes import (\n    KinesisFirehoseEvent,\n    event_source,\n)\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\n@event_source(data_class=KinesisFirehoseEvent)\ndef lambda_handler(event: KinesisFirehoseEvent, context: LambdaContext):\n    result = []\n\n    for record in event.records:\n        # if data was delivered as json; caches loaded value\n        data = record.data_as_json\n\n        processed_record = {\n            \"recordId\": record.record_id,\n            \"data\": base64.b64encode(json.dumps(data).encode(\"utf-8\")),\n            \"result\": \"Ok\",\n        }\n\n        result.append(processed_record)\n\n    # return transformed records\n    return {\"records\": result}\n</code></pre>"},{"location":"utilities/data_classes/#lambda-function-url","title":"Lambda Function URL","text":"app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, LambdaFunctionUrlEvent\n\n@event_source(data_class=LambdaFunctionUrlEvent)\ndef lambda_handler(event: LambdaFunctionUrlEvent, context):\n    do_something_with(event.body)\n</code></pre>"},{"location":"utilities/data_classes/#rabbit-mq","title":"Rabbit MQ","text":"<p>It is used for Rabbit MQ payloads, also see the blog post for more details.</p> app.py <pre><code>from typing import Dict\n\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.data_classes import event_source\nfrom aws_lambda_powertools.utilities.data_classes.rabbit_mq_event import RabbitMQEvent\nlogger = Logger()\n\n@event_source(data_class=RabbitMQEvent)\ndef lambda_handler(event: RabbitMQEvent, context):\nfor queue_name, messages in event.rmq_messages_by_queue.items():\n        logger.debug(f\"Messages for queue: {queue_name}\")\n        for message in messages:\n            logger.debug(f\"MessageID: {message.basic_properties.message_id}\")\n            data: Dict = message.json_data\n            logger.debug(\"Process json in base64 encoded data str\", data)\n</code></pre>"},{"location":"utilities/data_classes/#s3","title":"S3","text":"app.py <pre><code>from urllib.parse import unquote_plus\nfrom aws_lambda_powertools.utilities.data_classes import event_source, S3Event\n\n@event_source(data_class=S3Event)\ndef lambda_handler(event: S3Event, context):\n    bucket_name = event.bucket_name\n\n    # Multiple records can be delivered in a single event\n    for record in event.records:\n        object_key = unquote_plus(record.s3.get_object.key)\n\n        do_something_with(f\"{bucket_name}/{object_key}\")\n</code></pre>"},{"location":"utilities/data_classes/#s3-object-lambda","title":"S3 Object Lambda","text":"<p>This example is based on the AWS Blog post Introducing Amazon S3 Object Lambda \u2013 Use Your Code to Process Data as It Is Being Retrieved from S3.</p> app.py <pre><code>import boto3\nimport requests\n\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.logging.correlation_paths import S3_OBJECT_LAMBDA\nfrom aws_lambda_powertools.utilities.data_classes.s3_object_event import S3ObjectLambdaEvent\nlogger = Logger()\nsession = boto3.Session()\ns3 = session.client(\"s3\")\n\n@logger.inject_lambda_context(correlation_id_path=S3_OBJECT_LAMBDA, log_event=True)\ndef lambda_handler(event, context):\nevent = S3ObjectLambdaEvent(event)\n# Get object from S3\n    response = requests.get(event.input_s3_url)\n    original_object = response.content.decode(\"utf-8\")\n\n    # Make changes to the object about to be returned\n    transformed_object = original_object.upper()\n\n    # Write object back to S3 Object Lambda\n    s3.write_get_object_response(\n        Body=transformed_object, RequestRoute=event.request_route, RequestToken=event.request_token\n    )\n\n    return {\"status_code\": 200}\n</code></pre>"},{"location":"utilities/data_classes/#s3-eventbridge-notification","title":"S3 EventBridge Notification","text":"app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, S3EventBridgeNotificationEvent\n\n@event_source(data_class=S3EventBridgeNotificationEvent)\ndef lambda_handler(event: S3EventBridgeNotificationEvent, context):\n    bucket_name = event.detail.bucket.name\n    file_key = event.detail.object.key\n</code></pre>"},{"location":"utilities/data_classes/#ses","title":"SES","text":"app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, SESEvent\n\n@event_source(data_class=SESEvent)\ndef lambda_handler(event: SESEvent, context):\n    # Multiple records can be delivered in a single event\n    for record in event.records:\n        mail = record.ses.mail\n        common_headers = mail.common_headers\n\n        do_something_with(common_headers.to, common_headers.subject)\n</code></pre>"},{"location":"utilities/data_classes/#sns","title":"SNS","text":"app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, SNSEvent\n\n@event_source(data_class=SNSEvent)\ndef lambda_handler(event: SNSEvent, context):\n    # Multiple records can be delivered in a single event\n    for record in event.records:\n        message = record.sns.message\n        subject = record.sns.subject\n\n        do_something_with(subject, message)\n</code></pre>"},{"location":"utilities/data_classes/#sqs","title":"SQS","text":"app.py <pre><code>from aws_lambda_powertools.utilities.data_classes import event_source, SQSEvent\n\n@event_source(data_class=SQSEvent)\ndef lambda_handler(event: SQSEvent, context):\n    # Multiple records can be delivered in a single event\n    for record in event.records:\n        do_something_with(record.body)\n</code></pre>"},{"location":"utilities/data_classes/#advanced","title":"Advanced","text":""},{"location":"utilities/data_classes/#debugging","title":"Debugging","text":"<p>Alternatively, you can print out the fields to obtain more information. All classes come with a <code>__str__</code> method that generates a dictionary string which can be quite useful for debugging.</p> <p>However, certain events may contain sensitive fields such as <code>secret_access_key</code> and <code>session_token</code>, which are labeled as <code>[SENSITIVE]</code> to prevent any accidental disclosure of confidential information.</p> <p>If we fail to deserialize a field value (e.g., JSON), they will appear as <code>[Cannot be deserialized]</code></p> debugging.pydebugging_event.jsondebugging_output.json <pre><code>from aws_lambda_powertools.utilities.data_classes import (\n    CodePipelineJobEvent,\n    event_source,\n)\n\n\n@event_source(data_class=CodePipelineJobEvent)\ndef lambda_handler(event, context):\nprint(event)\n</code></pre> <pre><code>{\n\"CodePipeline.job\": {\n\"id\": \"11111111-abcd-1111-abcd-111111abcdef\",\n\"accountId\": \"111111111111\",\n\"data\": {\n\"actionConfiguration\": {\n\"configuration\": {\n\"FunctionName\": \"MyLambdaFunctionForAWSCodePipeline\",\n\"UserParameters\": \"some-input-such-as-a-URL\"\n}\n},\n\"inputArtifacts\": [\n{\n\"name\": \"ArtifactName\",\n\"revision\": null,\n\"location\": {\n\"type\": \"S3\",\n\"s3Location\": {\n\"bucketName\": \"the name of the bucket configured as the pipeline artifact store in Amazon S3, for example codepipeline-us-east-2-1234567890\",\n\"objectKey\": \"the name of the application, for example CodePipelineDemoApplication.zip\"\n}\n}\n}\n],\n\"outputArtifacts\": [],\n\"artifactCredentials\": {\n\"accessKeyId\": \"AKIAIOSFODNN7EXAMPLE\",\n\"secretAccessKey\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n\"sessionToken\": \"MIICiTCCAfICCQD6m7oRw0uXOjANBgkqhkiG9w0BAQUFADCBiDELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAldBMRAwDgYDVQQHEwdTZWF0dGxlMQ8wDQYDVQQKEwZBbWF6b24xFDASBgNVBAsTC0lBTSBDb25zb2xlMRIwEAYDVQQDEwlUZXN0Q2lsYWMxHzAdBgkqhkiG9w0BCQEWEG5vb25lQGFtYXpvbi5jb20wHhcNMTEwNDI1MjA0NTIxWhcNMTIwNDI0MjA0NTIxWjCBiDELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAldBMRAwDgYDVQQHEwdTZWF0dGxlMQ8wDQYDVQQKEwZBbWF6b24xFDASBgNVBAsTC0lBTSBDb25zb2xlMRIwEAYDVQQDEwlUZXN0Q2lsYWMxHzAdBgkqhkiG9w0BCQEWEG5vb25lQGFtYXpvbi5jb20wgZ8wDQYJKoZIhvcNAQEBBQADgY0AMIGJAoGBAMaK0dn+a4GmWIWJ21uUSfwfEvySWtC2XADZ4nB+BLYgVIk60CpiwsZ3G93vUEIO3IyNoH/f0wYK8m9TrDHudUZg3qX4waLG5M43q7Wgc/MbQITxOUSQv7c7ugFFDzQGBzZswY6786m86gpEIbb3OhjZnzcvQAaRHhdlQWIMm2nrAgMBAAEwDQYJKoZIhvcNAQEFBQADgYEAtCu4nUhVVxYUntneD9+h8Mg9q6q+auNKyExzyLwaxlAoo7TJHidbtS4J5iNmZgXL0FkbFFBjvSfpJIlJ00zbhNYS5f6GuoEDmFJl0ZxBHjJnyp378OD8uTs7fLvjx79LjSTbNYiytVbZPQUQ5Yaxu2jXnimvw3rrszlaEXAMPLE=\"\n},\n\"continuationToken\": \"A continuation token if continuing job\"\n}\n}\n}\n</code></pre> <p><pre><code>{\n\"account_id\":\"111111111111\",\n\"data\":{\n\"action_configuration\":{\n\"configuration\":{\n\"decoded_user_parameters\":\"[Cannot be deserialized]\",\n\"function_name\":\"MyLambdaFunctionForAWSCodePipeline\",\n\"raw_event\":\"[SENSITIVE]\",\n\"user_parameters\":\"some-input-such-as-a-URL\"\n},\n\"raw_event\":\"[SENSITIVE]\"\n},\n\"artifact_credentials\":{\n\"access_key_id\":\"AKIAIOSFODNN7EXAMPLE\",\n\"expiration_time\":\"None\",\n\"raw_event\":\"[SENSITIVE]\",\n\"secret_access_key\":\"[SENSITIVE]\",\n\"session_token\":\"[SENSITIVE]\"\n},\n\"continuation_token\":\"A continuation token if continuing job\",\n\"encryption_key\":\"None\",\n\"input_artifacts\":[\n{\n\"location\":{\n\"get_type\":\"S3\",\n\"raw_event\":\"[SENSITIVE]\",\n\"s3_location\":{\n\"bucket_name\":\"the name of the bucket configured as the pipeline artifact store in Amazon S3, for example codepipeline-us-east-2-1234567890\",\n\"key\":\"the name of the application, for example CodePipelineDemoApplication.zip\",\n\"object_key\":\"the name of the application, for example CodePipelineDemoApplication.zip\",\n\"raw_event\":\"[SENSITIVE]\"\n}\n},\n\"name\":\"ArtifactName\",\n\"raw_event\":\"[SENSITIVE]\",\n\"revision\":\"None\"\n}\n],\n\"output_artifacts\":[\n\n],\n\"raw_event\":\"[SENSITIVE]\"\n},\n\"decoded_user_parameters\":\"[Cannot be deserialized]\",\n\"get_id\":\"11111111-abcd-1111-abcd-111111abcdef\",\n\"input_bucket_name\":\"the name of the bucket configured as the pipeline artifact store in Amazon S3, for example codepipeline-us-east-2-1234567890\",\n\"input_object_key\":\"the name of the application, for example CodePipelineDemoApplication.zip\",\n\"raw_event\":\"[SENSITIVE]\",\n\"user_parameters\":\"some-input-such-as-a-URL\"\n}\n</code></pre> ```</p>"},{"location":"utilities/feature_flags/","title":"Feature flags","text":"<p>The feature flags utility provides a simple rule engine to define when one or multiple features should be enabled depending on the input.</p> Info <p>We currently only support AppConfig using freeform configuration profile.</p>"},{"location":"utilities/feature_flags/#terminology","title":"Terminology","text":"<p>Feature flags are used to modify behaviour without changing the application's code. These flags can be static or dynamic.</p> <p>Static flags. Indicates something is simply <code>on</code> or <code>off</code>, for example <code>TRACER_ENABLED=True</code>.</p> <p>Dynamic flags. Indicates something can have varying states, for example enable a list of premium features for customer X not Y.</p> Tip <p>You can use Parameters utility for static flags while this utility can do both static and dynamic feature flags.</p> Warning <p>Be mindful that feature flags can increase the complexity of your application over time; use them sparingly.</p> <p>If you want to learn more about feature flags, their variations and trade-offs, check these articles:</p> <ul> <li>Feature Toggles (aka Feature Flags) - Pete Hodgson</li> <li>AWS Lambda Feature Toggles Made Simple - Ran Isenberg</li> <li>Feature Flags Getting Started - CloudBees</li> </ul> Note <p>AWS AppConfig requires two API calls to fetch configuration for the first time. You can improve latency by consolidating your feature settings in a single Configuration.</p>"},{"location":"utilities/feature_flags/#key-features","title":"Key features","text":"<ul> <li>Define simple feature flags to dynamically decide when to enable a feature</li> <li>Fetch one or all feature flags enabled for a given application context</li> <li>Support for static feature flags to simply turn on/off a feature without rules</li> </ul>"},{"location":"utilities/feature_flags/#getting-started","title":"Getting started","text":""},{"location":"utilities/feature_flags/#iam-permissions","title":"IAM Permissions","text":"<p>Your Lambda function IAM Role must have <code>appconfig:GetLatestConfiguration</code> and <code>appconfig:StartConfigurationSession</code> IAM permissions before using this feature.</p>"},{"location":"utilities/feature_flags/#required-resources","title":"Required resources","text":"<p>By default, this utility provides AWS AppConfig as a configuration store.</p> <p>The following sample infrastructure will be used throughout this documentation:</p> template.yamlCDK <pre><code>AWSTemplateFormatVersion: \"2010-09-09\"\nDescription: Lambda Powertools for Python Feature flags sample template\nResources:\nFeatureStoreApp:\nType: AWS::AppConfig::Application\nProperties:\nDescription: \"AppConfig Application for feature toggles\"\nName: product-catalogue\n\nFeatureStoreDevEnv:\nType: AWS::AppConfig::Environment\nProperties:\nApplicationId: !Ref FeatureStoreApp\nDescription: \"Development Environment for the App Config Store\"\nName: dev\n\nFeatureStoreConfigProfile:\nType: AWS::AppConfig::ConfigurationProfile\nProperties:\nApplicationId: !Ref FeatureStoreApp\nName: features\nLocationUri: \"hosted\"\n\nHostedConfigVersion:\nType: AWS::AppConfig::HostedConfigurationVersion\nProperties:\nApplicationId: !Ref FeatureStoreApp\nConfigurationProfileId: !Ref FeatureStoreConfigProfile\nDescription: 'A sample hosted configuration version'\nContent: |\n{\n\"premium_features\": {\n\"default\": false,\n\"rules\": {\n\"customer tier equals premium\": {\n\"when_match\": true,\n\"conditions\": [\n{\n\"action\": \"EQUALS\",\n\"key\": \"tier\",\n\"value\": \"premium\"\n}\n]\n}\n}\n},\n\"ten_percent_off_campaign\": {\n\"default\": false\n}\n}\nContentType: 'application/json'\n\nConfigDeployment:\nType: AWS::AppConfig::Deployment\nProperties:\nApplicationId: !Ref FeatureStoreApp\nConfigurationProfileId: !Ref FeatureStoreConfigProfile\nConfigurationVersion: !Ref HostedConfigVersion\nDeploymentStrategyId: \"AppConfig.AllAtOnce\"\nEnvironmentId: !Ref FeatureStoreDevEnv\n</code></pre> <pre><code>import json\n\nimport aws_cdk.aws_appconfig as appconfig\nfrom aws_cdk import core\n\n\nclass SampleFeatureFlagStore(core.Construct):\n    def __init__(self, scope: core.Construct, id_: str) -&gt; None:\n        super().__init__(scope, id_)\n\nfeatures_config = {\n\"premium_features\": {\n\"default\": False,\n\"rules\": {\n\"customer tier equals premium\": {\n\"when_match\": True,\n\"conditions\": [{\"action\": \"EQUALS\", \"key\": \"tier\", \"value\": \"premium\"}],\n}\n},\n},\n\"ten_percent_off_campaign\": {\"default\": True},\n}\nself.config_app = appconfig.CfnApplication(\nself,\n            id=\"app\",\n            name=\"product-catalogue\",\n        )\nself.config_env = appconfig.CfnEnvironment(\nself,\n            id=\"env\",\n            application_id=self.config_app.ref,\n            name=\"dev-env\",\n        )\nself.config_profile = appconfig.CfnConfigurationProfile(\nself,\n            id=\"profile\",\n            application_id=self.config_app.ref,\n            location_uri=\"hosted\",\n            name=\"features\",\n        )\nself.hosted_cfg_version = appconfig.CfnHostedConfigurationVersion(\nself,\n            \"version\",\n            application_id=self.config_app.ref,\n            configuration_profile_id=self.config_profile.ref,\n            content=json.dumps(features_config),\n            content_type=\"application/json\",\n        )\nself.app_config_deployment = appconfig.CfnDeployment(\nself,\n            id=\"deploy\",\n            application_id=self.config_app.ref,\n            configuration_profile_id=self.config_profile.ref,\n            configuration_version=self.hosted_cfg_version.ref,\n            deployment_strategy_id=\"AppConfig.AllAtOnce\",\n            environment_id=self.config_env.ref,\n        )\n</code></pre>"},{"location":"utilities/feature_flags/#evaluating-a-single-feature-flag","title":"Evaluating a single feature flag","text":"<p>To get started, you'd need to initialize <code>AppConfigStore</code> and <code>FeatureFlags</code>. Then call <code>FeatureFlags</code> <code>evaluate</code> method to fetch, validate, and evaluate your feature.</p> <p>The <code>evaluate</code> method supports two optional parameters:</p> <ul> <li>context: Value to be evaluated against each rule defined for the given feature</li> <li>default: Sentinel value to use in case we experience any issues with our store, or feature doesn't exist</li> </ul> app.pyevent.jsonfeatures.json <pre><code>from aws_lambda_powertools.utilities.feature_flags import FeatureFlags, AppConfigStore\n\napp_config = AppConfigStore(\nenvironment=\"dev\",\n    application=\"product-catalogue\",\n    name=\"features\"\n)\n\nfeature_flags = FeatureFlags(store=app_config)\ndef lambda_handler(event, context):\n    # Get customer's tier from incoming request\nctx = { \"tier\": event.get(\"tier\", \"standard\") }\n# Evaluate whether customer's tier has access to premium features\n    # based on `has_premium_features` rules\nhas_premium_features: bool = feature_flags.evaluate(name=\"premium_features\",\ncontext=ctx, default=False)\nif has_premium_features:\n# enable premium features\n        ...\n</code></pre> <pre><code>{\n\"username\": \"lessa\",\n\"tier\": \"premium\",\n\"basked_id\": \"random_id\"\n}\n</code></pre> <pre><code>{\n\"premium_features\": {\n\"default\": false,\n\"rules\": {\n\"customer tier equals premium\": {\n\"when_match\": true,\n\"conditions\": [\n{\n\"action\": \"EQUALS\",\n\"key\": \"tier\",\n\"value\": \"premium\"\n}\n]\n}\n}\n},\n\"ten_percent_off_campaign\": {\n\"default\": false\n}\n}\n</code></pre>"},{"location":"utilities/feature_flags/#static-flags","title":"Static flags","text":"<p>We have a static flag named <code>ten_percent_off_campaign</code>. Meaning, there are no conditional rules, it's either ON or OFF for all customers.</p> <p>In this case, we could omit the <code>context</code> parameter and simply evaluate whether we should apply the 10% discount.</p> app.pyfeatures.json <pre><code>from aws_lambda_powertools.utilities.feature_flags import FeatureFlags, AppConfigStore\n\napp_config = AppConfigStore(\n    environment=\"dev\",\n    application=\"product-catalogue\",\n    name=\"features\"\n)\n\nfeature_flags = FeatureFlags(store=app_config)\n\ndef lambda_handler(event, context):\napply_discount: bool = feature_flags.evaluate(name=\"ten_percent_off_campaign\",\ndefault=False)\nif apply_discount:\n        # apply 10% discount to product\n        ...\n</code></pre> <pre><code>{\n\"ten_percent_off_campaign\": {\n\"default\": false\n}\n}\n</code></pre>"},{"location":"utilities/feature_flags/#getting-all-enabled-features","title":"Getting all enabled features","text":"<p>As you might have noticed, each <code>evaluate</code> call means an API call to the Store and the more features you have the more costly this becomes.</p> <p>You can use <code>get_enabled_features</code> method for scenarios where you need a list of all enabled features according to the input context.</p> app.pyevent.jsonfeatures.json <pre><code>from aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.utilities.feature_flags import FeatureFlags, AppConfigStore\n\napp = APIGatewayRestResolver()\n\napp_config = AppConfigStore(\n    environment=\"dev\",\n    application=\"product-catalogue\",\n    name=\"features\"\n)\n\nfeature_flags = FeatureFlags(store=app_config)\n\n@app.get(\"/products\")\ndef list_products():\n    ctx = {\n**app.current_event.headers,\n**app.current_event.json_body\n}\n# all_features is evaluated to [\"geo_customer_campaign\", \"ten_percent_off_campaign\"]\n    all_features: list[str] = feature_flags.get_enabled_features(context=ctx)\nif \"geo_customer_campaign\" in all_features:\n        # apply discounts based on geo\n        ...\n\n    if \"ten_percent_off_campaign\" in all_features:\n        # apply additional 10% for all customers\n        ...\n\ndef lambda_handler(event, context):\n    return app.resolve(event, context)\n</code></pre> <pre><code>{\n\"body\": \"{\\\"username\\\": \\\"lessa\\\", \\\"tier\\\": \\\"premium\\\", \\\"basked_id\\\": \\\"random_id\\\"}\",\n\"resource\": \"/products\",\n\"path\": \"/products\",\n\"httpMethod\": \"GET\",\n\"isBase64Encoded\": false,\n\"headers\": {\n\"CloudFront-Viewer-Country\": \"NL\"\n}\n}\n</code></pre> <pre><code>{\n\"premium_features\": {\n\"default\": false,\n\"rules\": {\n\"customer tier equals premium\": {\n\"when_match\": true,\n\"conditions\": [\n{\n\"action\": \"EQUALS\",\n\"key\": \"tier\",\n\"value\": \"premium\"\n}\n]\n}\n}\n},\n\"ten_percent_off_campaign\": {\n\"default\": true\n},\n\"geo_customer_campaign\": {\n\"default\": false,\n\"rules\": {\n\"customer in temporary discount geo\": {\n\"when_match\": true,\n\"conditions\": [\n{\n\"action\": \"KEY_IN_VALUE\",\n\"key\": \"CloudFront-Viewer-Country\",\n\"value\": [\"NL\", \"IE\", \"UK\", \"PL\", \"PT\"]\n}\n]\n}\n}\n}\n}\n</code></pre>"},{"location":"utilities/feature_flags/#beyond-boolean-feature-flags","title":"Beyond boolean feature flags","text":"When is this useful? <p>You might have a list of features to unlock for premium customers, unlock a specific set of features for admin users, etc.</p> <p>Feature flags can return any JSON values when <code>boolean_type</code> parameter is set to <code>false</code>. These can be dictionaries, list, string, integers, etc.</p> app.pyevent.jsonfeatures.json <pre><code>from aws_lambda_powertools.utilities.feature_flags import FeatureFlags, AppConfigStore\n\napp_config = AppConfigStore(\nenvironment=\"dev\",\n    application=\"product-catalogue\",\n    name=\"features\"\n)\n\nfeature_flags = FeatureFlags(store=app_config)\ndef lambda_handler(event, context):\n    # Get customer's tier from incoming request\nctx = { \"tier\": event.get(\"tier\", \"standard\") }\n# Evaluate `has_premium_features` base don customer's tier\npremium_features: list[str] = feature_flags.evaluate(name=\"premium_features\",\ncontext=ctx, default=False)\nfor feature in premium_features:\n# enable premium features\n        ...\n</code></pre> <pre><code>{\n\"username\": \"lessa\",\n\"tier\": \"premium\",\n\"basked_id\": \"random_id\"\n}\n</code></pre> <pre><code>{\n\"premium_features\": {\n\"boolean_type\": false,\n\"default\": [],\n\"rules\": {\n\"customer tier equals premium\": {\n\"when_match\": [\"no_ads\", \"no_limits\", \"chat\"],\n\"conditions\": [\n{\n\"action\": \"EQUALS\",\n\"key\": \"tier\",\n\"value\": \"premium\"\n}\n]\n}\n}\n}\n}\n</code></pre>"},{"location":"utilities/feature_flags/#time-based-feature-flags","title":"Time based feature flags","text":"<p>Feature flags can also return enabled features based on time or datetime ranges. This allows you to have features that are only enabled on certain days of the week, certain time intervals or between certain calendar dates.</p> <p>Use cases:</p> <ul> <li>Enable maintenance mode during a weekend</li> <li>Disable support/chat feature after working hours</li> <li>Launch a new feature on a specific date and time</li> </ul> <p>You can also have features enabled only at certain times of the day for premium tier customers</p> app.pyevent.jsonfeatures.json <pre><code>from aws_lambda_powertools.utilities.feature_flags import AppConfigStore, FeatureFlags\n\napp_config = AppConfigStore(environment=\"dev\", application=\"product-catalogue\", name=\"features\")\n\nfeature_flags = FeatureFlags(store=app_config)\n\n\ndef lambda_handler(event, context):\n    # Get customer's tier from incoming request\n    ctx = {\"tier\": event.get(\"tier\", \"standard\")}\n\nweekend_premium_discount = feature_flags.evaluate(name=\"weekend_premium_discount\", default=False, context=ctx)\nif weekend_premium_discount:\n        # Enable special discount for premium members on weekends\n        pass\n</code></pre> <pre><code>{\n\"username\": \"rubefons\",\n\"tier\": \"premium\",\n\"basked_id\": \"random_id\"\n}\n</code></pre> <pre><code>{\n\"weekend_premium_discount\": {\n\"default\": false,\n\"rules\": {\n\"customer tier equals premium and its time for a discount\": {\n\"when_match\": true,\n\"conditions\": [\n{\n\"action\": \"EQUALS\",\n\"key\": \"tier\",\n\"value\": \"premium\"\n},\n{\n\"action\": \"SCHEDULE_BETWEEN_DAYS_OF_WEEK\",\n\"key\": \"CURRENT_DAY_OF_WEEK\",\n\"value\": {\n\"DAYS\": [\n\"SATURDAY\",\n\"SUNDAY\"\n],\n\"TIMEZONE\": \"America/New_York\"\n}\n}\n]\n}\n}\n}\n}\n</code></pre> <p>You can also have features enabled only at certain times of the day.</p> app.pyfeatures.json <pre><code>from aws_lambda_powertools.utilities.feature_flags import AppConfigStore, FeatureFlags\n\napp_config = AppConfigStore(environment=\"dev\", application=\"product-catalogue\", name=\"features\")\n\nfeature_flags = FeatureFlags(store=app_config)\n\n\ndef lambda_handler(event, context):\nis_happy_hour = feature_flags.evaluate(name=\"happy_hour\", default=False)\nif is_happy_hour:\n        # Apply special discount\n        pass\n</code></pre> <pre><code>{\n\"happy_hour\": {\n\"default\": false,\n\"rules\": {\n\"is happy hour\": {\n\"when_match\": true,\n\"conditions\": [\n{\n\"action\": \"SCHEDULE_BETWEEN_TIME_RANGE\",\n\"key\": \"CURRENT_TIME\",\n\"value\": {\n\"START\": \"17:00\",\n\"END\": \"19:00\",\n\"TIMEZONE\": \"Europe/Copenhagen\"\n}\n}\n]\n}\n}\n}\n}\n</code></pre> <p>You can also have features enabled only at specific days, for example: enable christmas sale discount during specific dates.</p> app.pyfeatures.json <pre><code>from aws_lambda_powertools.utilities.feature_flags import AppConfigStore, FeatureFlags\n\napp_config = AppConfigStore(environment=\"dev\", application=\"product-catalogue\", name=\"features\")\n\nfeature_flags = FeatureFlags(store=app_config)\n\n\ndef lambda_handler(event, context):\n    # Get customer's tier from incoming request\nxmas_discount = feature_flags.evaluate(name=\"christmas_discount\", default=False)\nif xmas_discount:\n        # Enable special discount on christmas:\n        pass\n</code></pre> <pre><code>{\n\"christmas_discount\": {\n\"default\": false,\n\"rules\": {\n\"enable discount during christmas\": {\n\"when_match\": true,\n\"conditions\": [\n{\n\"action\": \"SCHEDULE_BETWEEN_DATETIME_RANGE\",\n\"key\": \"CURRENT_DATETIME\",\n\"value\": {\n\"START\": \"2022-12-25T12:00:00\",\n\"END\": \"2022-12-31T23:59:59\",\n\"TIMEZONE\": \"America/New_York\"\n}\n}\n]\n}\n}\n}\n}\n</code></pre> How should I use timezones? <p>You can use any IANA time zone (as originally specified in PEP 615) as part of your rules definition. Powertools takes care of converting and calculate the correct timestamps for you.</p> <p>When using <code>SCHEDULE_BETWEEN_DATETIME_RANGE</code>, use timestamps without timezone information, and specify the timezone manually. This way, you'll avoid hitting problems with day light savings.</p>"},{"location":"utilities/feature_flags/#advanced","title":"Advanced","text":""},{"location":"utilities/feature_flags/#adjusting-in-memory-cache","title":"Adjusting in-memory cache","text":"<p>By default, we cache configuration retrieved from the Store for 5 seconds for performance and reliability reasons.</p> <p>You can override <code>max_age</code> parameter when instantiating the store.</p> app.py <pre><code>from aws_lambda_powertools.utilities.feature_flags import FeatureFlags, AppConfigStore\n\napp_config = AppConfigStore(\n    environment=\"dev\",\n    application=\"product-catalogue\",\n    name=\"features\",\nmax_age=300\n)\n</code></pre>"},{"location":"utilities/feature_flags/#getting-fetched-configuration","title":"Getting fetched configuration","text":"When is this useful? <p>You might have application configuration in addition to feature flags in your store.</p> <p>This means you don't need to make another call only to fetch app configuration.</p> <p>You can access the configuration fetched from the store via <code>get_raw_configuration</code> property within the store instance.</p> app.py <pre><code>from aws_lambda_powertools.utilities.feature_flags import FeatureFlags, AppConfigStore\n\napp_config = AppConfigStore(\n    environment=\"dev\",\n    application=\"product-catalogue\",\n    name=\"configuration\",\n    envelope = \"feature_flags\"\n)\n\nfeature_flags = FeatureFlags(store=app_config)\n\nconfig = app_config.get_raw_configuration\n</code></pre>"},{"location":"utilities/feature_flags/#schema","title":"Schema","text":"<p>This utility expects a certain schema to be stored as JSON within AWS AppConfig.</p>"},{"location":"utilities/feature_flags/#features","title":"Features","text":"<p>A feature can simply have its name and a <code>default</code> value. This is either on or off, also known as a static flag.</p> minimal_schema.json<pre><code>{\n\"global_feature\": {\n\"default\": true\n},\n\"non_boolean_global_feature\": {\n\"default\": {\"group\": \"read-only\"},\n\"boolean_type\": false\n},\n}\n</code></pre> <p>If you need more control and want to provide context such as user group, permissions, location, etc., you need to add rules to your feature flag configuration.</p>"},{"location":"utilities/feature_flags/#rules","title":"Rules","text":"<p>When adding <code>rules</code> to a feature, they must contain:</p> <ol> <li>A rule name as a key</li> <li><code>when_match</code> boolean or JSON value that should be used when conditions match</li> <li>A list of <code>conditions</code> for evaluation</li> </ol> feature_with_rules.json<pre><code>{\n\"premium_feature\": {\n\"default\": false,\n\"rules\": {\n\"customer tier equals premium\": {\n\"when_match\": true,\n\"conditions\": [\n{\n\"action\": \"EQUALS\",\n\"key\": \"tier\",\n\"value\": \"premium\"\n}\n]\n}\n}\n},\n\"non_boolean_premium_feature\": {\n\"default\": [],\n\"rules\": {\n\"customer tier equals premium\": {\n\"when_match\": [\"remove_limits\", \"remove_ads\"],\n\"conditions\": [\n{\n\"action\": \"EQUALS\",\n\"key\": \"tier\",\n\"value\": \"premium\"\n}\n]\n}\n}\n}\n}\n</code></pre> <p>You can have multiple rules with different names. The rule engine will return the first result <code>when_match</code> of the matching rule configuration, or <code>default</code> value when none of the rules apply.</p>"},{"location":"utilities/feature_flags/#conditions","title":"Conditions","text":"<p>The <code>conditions</code> block is a list of conditions that contain <code>action</code>, <code>key</code>, and <code>value</code> keys:</p> conditions.json<pre><code>{\n...\n\"conditions\": [\n{\n\"action\": \"EQUALS\",\n\"key\": \"tier\",\n\"value\": \"premium\"\n}\n]\n}\n</code></pre> <p>The <code>action</code> configuration can have the following values, where the expressions <code>a</code> is the <code>key</code> and <code>b</code> is the <code>value</code> above:</p> Action Equivalent expression EQUALS <code>lambda a, b: a == b</code> NOT_EQUALS <code>lambda a, b: a != b</code> KEY_GREATER_THAN_VALUE <code>lambda a, b: a &gt; b</code> KEY_GREATER_THAN_OR_EQUAL_VALUE <code>lambda a, b: a &gt;= b</code> KEY_LESS_THAN_VALUE <code>lambda a, b: a &lt; b</code> KEY_LESS_THAN_OR_EQUAL_VALUE <code>lambda a, b: a &lt;= b</code> STARTSWITH <code>lambda a, b: a.startswith(b)</code> ENDSWITH <code>lambda a, b: a.endswith(b)</code> KEY_IN_VALUE <code>lambda a, b: a in b</code> KEY_NOT_IN_VALUE <code>lambda a, b: a not in b</code> VALUE_IN_KEY <code>lambda a, b: b in a</code> VALUE_NOT_IN_KEY <code>lambda a, b: b not in a</code> SCHEDULE_BETWEEN_TIME_RANGE <code>lambda a, b: time(a).start &lt;= b &lt;= time(a).end</code> SCHEDULE_BETWEEN_DATETIME_RANGE <code>lambda a, b: datetime(a).start &lt;= b &lt;= datetime(b).end</code> SCHEDULE_BETWEEN_DAYS_OF_WEEK <code>lambda a, b: day_of_week(a) in b</code> Info <p>The <code>**key**</code> and <code>**value**</code> will be compared to the input from the <code>**context**</code> parameter.</p> Time based keys <p>For time based keys, we provide a list of predefined keys. These will automatically get converted to the corresponding timestamp on each invocation of your Lambda function.</p> Key Meaning CURRENT_TIME The current time, 24 hour format (HH:mm) CURRENT_DATETIME The current datetime (ISO8601) CURRENT_DAY_OF_WEEK The current day of the week (Monday-Sunday) <p>If not specified, the timezone used for calculations will be UTC.</p> <p>For multiple conditions, we will evaluate the list of conditions as a logical <code>AND</code>, so all conditions needs to match to return <code>when_match</code> value.</p>"},{"location":"utilities/feature_flags/#rule-engine-flowchart","title":"Rule engine flowchart","text":"<p>Now that you've seen all properties of a feature flag schema, this flowchart describes how the rule engine decides what value to return.</p> <p></p>"},{"location":"utilities/feature_flags/#envelope","title":"Envelope","text":"<p>There are scenarios where you might want to include feature flags as part of an existing application configuration.</p> <p>For this to work, you need to use a JMESPath expression via the <code>envelope</code> parameter to extract that key as the feature flags configuration.</p> app.pyconfiguration.json <pre><code>from aws_lambda_powertools.utilities.feature_flags import FeatureFlags, AppConfigStore\n\napp_config = AppConfigStore(\n    environment=\"dev\",\n    application=\"product-catalogue\",\n    name=\"configuration\",\nenvelope = \"feature_flags\"\n)\n</code></pre> <pre><code>{\n\"logging\": {\n\"level\": \"INFO\",\n\"sampling_rate\": 0.1\n},\n\"feature_flags\": {\n\"premium_feature\": {\n\"default\": false,\n\"rules\": {\n\"customer tier equals premium\": {\n\"when_match\": true,\n\"conditions\": [\n{\n\"action\": \"EQUALS\",\n\"key\": \"tier\",\n\"value\": \"premium\"\n}\n]\n}\n}\n},\n\"feature2\": {\n\"default\": false\n}\n}\n}\n</code></pre>"},{"location":"utilities/feature_flags/#built-in-store-provider","title":"Built-in store provider","text":"Info <p>For GA, you'll be able to bring your own store.</p>"},{"location":"utilities/feature_flags/#appconfig","title":"AppConfig","text":"<p>AppConfig store provider fetches any JSON document from AWS AppConfig.</p> <p>These are the available options for further customization.</p> Parameter Default Description environment <code>\"\"</code> AWS AppConfig Environment, e.g. <code>test</code> application <code>\"\"</code> AWS AppConfig Application name <code>\"\"</code> AWS AppConfig Configuration name envelope <code>None</code> JMESPath expression to use to extract feature flags configuration from AWS AppConfig configuration max_age <code>5</code> Number of seconds to cache feature flags configuration fetched from AWS AppConfig sdk_config <code>None</code> Botocore Config object jmespath_options <code>None</code> For advanced use cases when you want to bring your own JMESPath functions logger <code>logging.Logger</code> Logger to use for debug.  You can optionally supply an instance of Powertools Logger. AppConfigStore sample<pre><code>from botocore.config import Config\n\nimport jmespath\n\nfrom aws_lambda_powertools.utilities.feature_flags import AppConfigStore\n\nboto_config = Config(read_timeout=10, retries={\"total_max_attempts\": 2})\n\n# Custom JMESPath functions\nclass CustomFunctions(jmespath.functions.Functions):\n\n    @jmespath.functions.signature({'types': ['string']})\n    def _func_special_decoder(self, s):\n        return my_custom_decoder_logic(s)\n\n\ncustom_jmespath_options = {\"custom_functions\": CustomFunctions()}\n\n\napp_config = AppConfigStore(\nenvironment=\"dev\",\napplication=\"product-catalogue\",\nname=\"configuration\",\nmax_age=120,\nenvelope = \"features\",\nsdk_config=boto_config,\njmespath_options=custom_jmespath_options\n)\n</code></pre>"},{"location":"utilities/feature_flags/#testing-your-code","title":"Testing your code","text":"<p>You can unit test your feature flags locally and independently without setting up AWS AppConfig.</p> <p><code>AppConfigStore</code> only fetches a JSON document with a specific schema. This allows you to mock the response and use it to verify the rule evaluation.</p> Warning <p>This excerpt relies on <code>pytest</code> and <code>pytest-mock</code> dependencies.</p> Unit testing feature flags<pre><code>from aws_lambda_powertools.utilities.feature_flags import FeatureFlags, AppConfigStore, RuleAction\n\n\ndef init_feature_flags(mocker, mock_schema, envelope=\"\") -&gt; FeatureFlags:\n\"\"\"Mock AppConfig Store get_configuration method to use mock schema instead\"\"\"\n\nmethod_to_mock = \"aws_lambda_powertools.utilities.feature_flags.AppConfigStore.get_configuration\"\nmocked_get_conf = mocker.patch(method_to_mock)\nmocked_get_conf.return_value = mock_schema\napp_conf_store = AppConfigStore(\n        environment=\"test_env\",\n        application=\"test_app\",\n        name=\"test_conf_name\",\n        envelope=envelope,\n    )\n\n    return FeatureFlags(store=app_conf_store)\n\n\ndef test_flags_condition_match(mocker):\n    # GIVEN\n    expected_value = True\n    mocked_app_config_schema = {\n        \"my_feature\": {\n            \"default\": False,\n            \"rules\": {\n                \"tenant id equals 12345\": {\n                    \"when_match\": expected_value,\n                    \"conditions\": [\n                        {\n                            \"action\": RuleAction.EQUALS.value,\n                            \"key\": \"tenant_id\",\n                            \"value\": \"12345\",\n                        }\n                    ],\n                }\n            },\n            }\n    }\n\n    # WHEN\n    ctx = {\"tenant_id\": \"12345\", \"username\": \"a\"}\n    feature_flags = init_feature_flags(mocker=mocker, mock_schema=mocked_app_config_schema)\n    flag = feature_flags.evaluate(name=\"my_feature\", context=ctx, default=False)\n\n    # THEN\n    assert flag == expected_value\n</code></pre>"},{"location":"utilities/feature_flags/#feature-flags-vs-parameters-vs-env-vars","title":"Feature flags vs Parameters vs env vars","text":"Method When to use Requires new deployment on changes Supported services Environment variables Simple configuration that will rarely if ever change, because changing it requires a Lambda function deployment. Yes Lambda Parameters utility Access to secrets, or fetch parameters in different formats from AWS System Manager Parameter Store or Amazon DynamoDB. No Parameter Store, DynamoDB, Secrets Manager, AppConfig Feature flags utility Rule engine to define when one or multiple features should be enabled depending on the input. No AppConfig"},{"location":"utilities/feature_flags/#deprecation-list-when-ga","title":"Deprecation list when GA","text":"Breaking change Recommendation <code>IN</code> RuleAction Use <code>KEY_IN_VALUE</code> instead <code>NOT_IN</code> RuleAction Use <code>KEY_NOT_IN_VALUE</code> instead <code>get_enabled_features</code> Return type changes from <code>List[str]</code> to <code>Dict[str, Any]</code>. New return will contain a list of features enabled and their values. List of enabled features will be in <code>enabled_features</code> key to keep ease of assertion we have in Beta. <code>boolean_type</code> Schema This might not be necessary anymore before we go GA. We will return either the <code>default</code> value when there are no rules as well as <code>when_match</code> value. This will simplify on-boarding if we can keep the same set of validations already offered."},{"location":"utilities/idempotency/","title":"Idempotency","text":"<p>The idempotency utility provides a simple solution to convert your Lambda functions into idempotent operations which are safe to retry.</p>"},{"location":"utilities/idempotency/#terminology","title":"Terminology","text":"<p>The property of idempotency means that an operation does not cause additional side effects if it is called more than once with the same input parameters.</p> <p>Idempotent operations will return the same result when they are called multiple times with the same parameters. This makes idempotent operations safe to retry.</p> <p>Idempotency key is a hash representation of either the entire event or a specific configured subset of the event, and invocation results are JSON serialized and stored in your persistence storage layer.</p> <p>Idempotency record is the data representation of an idempotent request saved in your preferred  storage layer. We use it to coordinate whether a request is idempotent, whether it's still valid or expired based on timestamps, etc.</p> <p> <pre><code>classDiagram\n    direction LR\n    class IdempotencyRecord {\n        idempotency_key str\n        status Status\n        expiry_timestamp int\n        in_progress_expiry_timestamp int\n        response_data Json~str~\n        payload_hash str\n    }\n    class Status {\n        &lt;&lt;Enumeration&gt;&gt;\n        INPROGRESS\n        COMPLETE\n        EXPIRED internal_only\n    }\n    IdempotencyRecord -- Status</code></pre> <p>Idempotency record representation </p>"},{"location":"utilities/idempotency/#key-features","title":"Key features","text":"<ul> <li>Prevent Lambda handler from executing more than once on the same event payload during a time window</li> <li>Ensure Lambda handler returns the same result when called with the same payload</li> <li>Select a subset of the event as the idempotency key using JMESPath expressions</li> <li>Set a time window in which records with the same payload should be considered duplicates</li> <li>Expires in-progress executions if the Lambda function times out halfway through</li> </ul>"},{"location":"utilities/idempotency/#getting-started","title":"Getting started","text":""},{"location":"utilities/idempotency/#iam-permissions","title":"IAM Permissions","text":"<p>Your Lambda function IAM Role must have <code>dynamodb:GetItem</code>, <code>dynamodb:PutItem</code>, <code>dynamodb:UpdateItem</code> and <code>dynamodb:DeleteItem</code> IAM permissions before using this feature.</p> Note <p>If you're using our example AWS Serverless Application Model (SAM), it already adds the required permissions.</p>"},{"location":"utilities/idempotency/#required-resources","title":"Required resources","text":"<p>Before getting started, you need to create a persistent storage layer where the idempotency utility can store its state - your lambda functions will need read and write access to it.</p> <p>As of now, Amazon DynamoDB is the only supported persistent storage layer, so you'll need to create a table first.</p> <p>Default table configuration</p> <p>If you're not changing the default configuration for the DynamoDB persistence layer, this is the expected default configuration:</p> Configuration Value Notes Partition key <code>id</code> TTL attribute name <code>expiration</code> This can only be configured after your table is created if you're using AWS Console Tip: You can share a single state table for all functions <p>You can reuse the same DynamoDB table to store idempotency state. We add <code>module_name</code> and qualified name for classes and functions in addition to the idempotency key as a hash key.</p> AWS Serverless Application Model (SAM) example<pre><code>Resources:\nIdempotencyTable:\nType: AWS::DynamoDB::Table\nProperties:\nAttributeDefinitions:\n-   AttributeName: id\nAttributeType: S\nKeySchema:\n-   AttributeName: id\nKeyType: HASH\nTimeToLiveSpecification:\nAttributeName: expiration\nEnabled: true\nBillingMode: PAY_PER_REQUEST\n\nHelloWorldFunction:\nType: AWS::Serverless::Function\nProperties:\nRuntime: python3.9\n...\nPolicies:\n- DynamoDBCrudPolicy:\nTableName: !Ref IdempotencyTable\n</code></pre> Warning: Large responses with DynamoDB persistence layer <p>When using this utility with DynamoDB, your function's responses must be smaller than 400KB.</p> <p>Larger items cannot be written to DynamoDB and will cause exceptions.</p> Info: DynamoDB <p>Each function invocation will generally make 2 requests to DynamoDB. If the result returned by your Lambda is less than 1kb, you can expect 2 WCUs per invocation. For retried invocations, you will see 1WCU and 1RCU. Review the DynamoDB pricing documentation to estimate the cost.</p>"},{"location":"utilities/idempotency/#idempotent-decorator","title":"Idempotent decorator","text":"<p>You can quickly start by initializing the <code>DynamoDBPersistenceLayer</code> class and using it with the <code>idempotent</code> decorator on your lambda handler.</p> Note <p>In this example, the entire Lambda handler is treated as a single idempotent operation. If your Lambda handler can cause multiple side effects, or you're only interested in making a specific logic idempotent, use <code>idempotent_function</code> instead.</p> <p>See Choosing a payload subset for idempotency for more elaborate use cases.</p> app.pyExample event <pre><code>from aws_lambda_powertools.utilities.idempotency import (\nDynamoDBPersistenceLayer, idempotent\n)\npersistence_layer = DynamoDBPersistenceLayer(table_name=\"IdempotencyTable\")\n@idempotent(persistence_store=persistence_layer)\ndef handler(event, context):\n    payment = create_subscription_payment(\n        user=event['user'],\n        product=event['product_id']\n    )\n    ...\nreturn {\n\"payment_id\": payment.id,\n        \"message\": \"success\",\n        \"statusCode\": 200,\n    }\n</code></pre> <pre><code>{\n\"username\": \"xyz\",\n\"product_id\": \"123456789\"\n}\n</code></pre> <p>After processing this request successfully, a second request containing the exact same payload above will now return the same response, ensuring our customer isn't charged twice.</p> <p>New to idempotency concept? Please review our Terminology section if you haven't yet.</p>"},{"location":"utilities/idempotency/#idempotent_function-decorator","title":"Idempotent_function decorator","text":"<p>Similar to idempotent decorator, you can use <code>idempotent_function</code> decorator for any synchronous Python function.</p> <p>When using <code>idempotent_function</code>, you must tell us which keyword parameter in your function signature has the data we should use via <code>data_keyword_argument</code>.</p> <p>We support JSON serializable data, Python Dataclasses, Parser/Pydantic Models, and our Event Source Data Classes.</p> Limitation <p>Make sure to call your decorated function using keyword arguments.</p> dataclass_sample.pyparser_pydantic_sample.py <pre><code>from dataclasses import dataclass\n\nfrom aws_lambda_powertools.utilities.idempotency import (\nDynamoDBPersistenceLayer, IdempotencyConfig, idempotent_function)\ndynamodb = DynamoDBPersistenceLayer(table_name=\"idem\")\nconfig =  IdempotencyConfig(\n    event_key_jmespath=\"order_id\",  # see Choosing a payload subset section\n    use_local_cache=True,\n)\n\n@dataclass\nclass OrderItem:\n    sku: str\n    description: str\n\n@dataclass\nclass Order:\n    item: OrderItem\n    order_id: int\n\n\n@idempotent_function(data_keyword_argument=\"order\", config=config, persistence_store=dynamodb)\ndef process_order(order: Order):\n    return f\"processed order {order.order_id}\"\n\ndef lambda_handler(event, context):\n    config.register_lambda_context(context) # see Lambda timeouts section\n    order_item = OrderItem(sku=\"fake\", description=\"sample\")\n    order = Order(item=order_item, order_id=\"fake-id\")\n\n    # `order` parameter must be called as a keyword argument to work\nprocess_order(order=order)\n</code></pre> <pre><code>from aws_lambda_powertools.utilities.idempotency import (\nDynamoDBPersistenceLayer, IdempotencyConfig, idempotent_function)\nfrom aws_lambda_powertools.utilities.parser import BaseModel\n\ndynamodb = DynamoDBPersistenceLayer(table_name=\"idem\")\nconfig =  IdempotencyConfig(\n    event_key_jmespath=\"order_id\",  # see Choosing a payload subset section\n    use_local_cache=True,\n)\n\n\nclass OrderItem(BaseModel):\n    sku: str\n    description: str\n\n\nclass Order(BaseModel):\n    item: OrderItem\n    order_id: int\n\n\n@idempotent_function(data_keyword_argument=\"order\", config=config, persistence_store=dynamodb)\ndef process_order(order: Order):\n    return f\"processed order {order.order_id}\"\n\ndef lambda_handler(event, context):\n    config.register_lambda_context(context) # see Lambda timeouts section\n    order_item = OrderItem(sku=\"fake\", description=\"sample\")\n    order = Order(item=order_item, order_id=\"fake-id\")\n\n    # `order` parameter must be called as a keyword argument to work\nprocess_order(order=order)\n</code></pre>"},{"location":"utilities/idempotency/#batch-integration","title":"Batch integration","text":"<p>You can can easily integrate with Batch utility via context manager. This ensures that you process each record in an idempotent manner, and guard against a Lambda timeout idempotent situation.</p> Choosing an unique batch record attribute <p>In this example, we choose <code>messageId</code> as our idempotency key since we know it'll be unique.</p> <p>Depending on your use case, it might be more accurate to choose another field your producer intentionally set to define uniqueness.</p> batch_sample.pybatch_event.json <pre><code>from aws_lambda_powertools.utilities.batch import BatchProcessor, EventType\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.idempotency import (\nDynamoDBPersistenceLayer, IdempotencyConfig, idempotent_function)\nprocessor = BatchProcessor(event_type=EventType.SQS)\ndynamodb = DynamoDBPersistenceLayer(table_name=\"idem\")\nconfig =  IdempotencyConfig(\nevent_key_jmespath=\"messageId\",  # see Choosing a payload subset section\nuse_local_cache=True,\n)\n\n\n@idempotent_function(data_keyword_argument=\"record\", config=config, persistence_store=dynamodb)\ndef record_handler(record: SQSRecord):\n    return {\"message\": record.body}\n\n\ndef lambda_handler(event, context):\nconfig.register_lambda_context(context) # see Lambda timeouts section\n# with Lambda context registered for Idempotency\n    # we can now kick in the Bach processing logic\nbatch = event[\"Records\"]\nwith processor(records=batch, handler=record_handler):\n# in case you want to access each record processed by your record_handler\n        # otherwise ignore the result variable assignment\nprocessed_messages = processor.process()\nreturn processor.response()\n</code></pre> <pre><code>{\n\"Records\": [\n{\n\"messageId\": \"059f36b4-87a3-44ab-83d2-661975830a7d\",\n\"receiptHandle\": \"AQEBwJnKyrHigUMZj6rYigCgxlaS3SLy0a...\",\n\"body\": \"Test message.\",\n\"attributes\": {\n\"ApproximateReceiveCount\": \"1\",\n\"SentTimestamp\": \"1545082649183\",\n\"SenderId\": \"AIDAIENQZJOLO23YVJ4VO\",\n\"ApproximateFirstReceiveTimestamp\": \"1545082649185\"\n},\n\"messageAttributes\": {\n\"testAttr\": {\n\"stringValue\": \"100\",\n\"binaryValue\": \"base64Str\",\n\"dataType\": \"Number\"\n}\n},\n\"md5OfBody\": \"e4e68fb7bd0e697a0ae8f1bb342846b3\",\n\"eventSource\": \"aws:sqs\",\n\"eventSourceARN\": \"arn:aws:sqs:us-east-2:123456789012:my-queue\",\n\"awsRegion\": \"us-east-2\"\n}\n]\n}\n</code></pre>"},{"location":"utilities/idempotency/#choosing-a-payload-subset-for-idempotency","title":"Choosing a payload subset for idempotency","text":"Tip: Dealing with always changing payloads <p>When dealing with a more elaborate payload, where parts of the payload always change, you should use <code>event_key_jmespath</code> parameter.</p> <p>Use <code>IdempotencyConfig</code> to instruct the idempotent decorator to only use a portion of your payload to verify whether a request is idempotent, and therefore it should not be retried.</p> <p>Payment scenario</p> <p>In this example, we have a Lambda handler that creates a payment for a user subscribing to a product. We want to ensure that we don't accidentally charge our customer by subscribing them more than once.</p> <p>Imagine the function executes successfully, but the client never receives the response due to a connection issue. It is safe to retry in this instance, as the idempotent decorator will return a previously saved response.</p> <p>What we want here is to instruct Idempotency to use <code>user</code> and <code>product_id</code> fields from our incoming payload as our idempotency key. If we were to treat the entire request as our idempotency key, a simple HTTP header change would cause our customer to be charged twice.</p> Deserializing JSON strings in payloads for increased accuracy. <p>The payload extracted by the <code>event_key_jmespath</code> is treated as a string by default. This means there could be differences in whitespace even when the JSON payload itself is identical.</p> <p>To alter this behaviour, we can use the JMESPath built-in function <code>powertools_json()</code> to treat the payload as a JSON object (dict) rather than a string.</p> payment.pyExample event <pre><code>import json\nfrom aws_lambda_powertools.utilities.idempotency import (\nIdempotencyConfig, DynamoDBPersistenceLayer, idempotent\n)\npersistence_layer = DynamoDBPersistenceLayer(table_name=\"IdempotencyTable\")\n\n# Deserialize JSON string under the \"body\" key\n# then extract \"user\" and \"product_id\" data\nconfig = IdempotencyConfig(event_key_jmespath=\"powertools_json(body).[user, product_id]\")\n@idempotent(config=config, persistence_store=persistence_layer)\ndef handler(event, context):\n    body = json.loads(event['body'])\npayment = create_subscription_payment(\nuser=body['user'],\n        product=body['product_id']\n    )\n    ...\nreturn {\n\"payment_id\": payment.id,\n        \"message\": \"success\",\n        \"statusCode\": 200\n    }\n</code></pre> <pre><code>{\n\"version\":\"2.0\",\n\"routeKey\":\"ANY /createpayment\",\n\"rawPath\":\"/createpayment\",\n\"rawQueryString\":\"\",\n\"headers\": {\n\"Header1\": \"value1\",\n\"Header2\": \"value2\"\n},\n\"requestContext\":{\n\"accountId\":\"123456789012\",\n\"apiId\":\"api-id\",\n\"domainName\":\"id.execute-api.us-east-1.amazonaws.com\",\n\"domainPrefix\":\"id\",\n\"http\":{\n\"method\":\"POST\",\n\"path\":\"/createpayment\",\n\"protocol\":\"HTTP/1.1\",\n\"sourceIp\":\"ip\",\n\"userAgent\":\"agent\"\n},\n\"requestId\":\"id\",\n\"routeKey\":\"ANY /createpayment\",\n\"stage\":\"$default\",\n\"time\":\"10/Feb/2021:13:40:43 +0000\",\n\"timeEpoch\":1612964443723\n},\n\"body\":\"{\\\"user\\\":\\\"xyz\\\",\\\"product_id\\\":\\\"123456789\\\"}\",\n\"isBase64Encoded\":false\n}\n</code></pre>"},{"location":"utilities/idempotency/#lambda-timeouts","title":"Lambda timeouts","text":"Note <p>This is automatically done when you decorate your Lambda handler with @idempotent decorator.</p> <p>To prevent against extended failed retries when a Lambda function times out, Powertools calculates and includes the remaining invocation available time as part of the idempotency record.</p> Example <p>If a second invocation happens after this timestamp, and the record is marked as <code>INPROGRESS</code>, we will execute the invocation again as if it was in the <code>EXPIRED</code> state (e.g, <code>expire_seconds</code> field elapsed).</p> <p>This means that if an invocation expired during execution, it will be quickly executed again on the next retry.</p> Important <p>If you are only using the @idempotent_function decorator to guard isolated parts of your code, you must use <code>register_lambda_context</code> available in the idempotency config object to benefit from this protection.</p> <p>Here is an example on how you register the Lambda context in your handler:</p> Registering the Lambda context<pre><code>from aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.idempotency import (\n    IdempotencyConfig, idempotent_function\n)\n\npersistence_layer = DynamoDBPersistenceLayer(table_name=\"...\")\n\nconfig = IdempotencyConfig()\n@idempotent_function(data_keyword_argument=\"record\", persistence_store=persistence_layer, config=config)\ndef record_handler(record: SQSRecord):\n    return {\"message\": record[\"body\"]}\n\n\ndef lambda_handler(event, context):\nconfig.register_lambda_context(context)\nreturn record_handler(event)\n</code></pre>"},{"location":"utilities/idempotency/#handling-exceptions","title":"Handling exceptions","text":"<p>If you are using the <code>idempotent</code> decorator on your Lambda handler, any unhandled exceptions that are raised during the code execution will cause the record in the persistence layer to be deleted. This means that new invocations will execute your code again despite having the same payload. If you don't want the record to be deleted, you need to catch exceptions within the idempotent function and return a successful response.</p> <p> <pre><code>sequenceDiagram\n    participant Client\n    participant Lambda\n    participant Persistence Layer\n    Client-&gt;&gt;Lambda: Invoke (event)\n    Lambda-&gt;&gt;Persistence Layer: Get or set (id=event.search(payload))\n    activate Persistence Layer\n    Note right of Persistence Layer: Locked during this time. Prevents multiple&lt;br/&gt;Lambda invocations with the same&lt;br/&gt;payload running concurrently.\n    Lambda--xLambda: Call handler (event).&lt;br/&gt;Raises exception\n    Lambda-&gt;&gt;Persistence Layer: Delete record (id=event.search(payload))\n    deactivate Persistence Layer\n    Lambda--&gt;&gt;Client: Return error response</code></pre> Idempotent sequence exception </p> <p>If you are using <code>idempotent_function</code>, any unhandled exceptions that are raised inside the decorated function will cause the record in the persistence layer to be deleted, and allow the function to be executed again if retried.</p> <p>If an Exception is raised outside the scope of the decorated function and after your function has been called, the persistent record will not be affected. In this case, idempotency will be maintained for your decorated function. Example:</p> Exception not affecting idempotency record sample<pre><code>def lambda_handler(event, context):\n# If an exception is raised here, no idempotent record will ever get created as the\n# idempotent function does not get called\ndo_some_stuff()\nresult = call_external_service(data={\"user\": \"user1\", \"id\": 5})\n\n# This exception will not cause the idempotent record to be deleted, since it\n# happens after the decorated function has been successfully called\nraise Exception\n@idempotent_function(data_keyword_argument=\"data\", config=config, persistence_store=dynamodb)\ndef call_external_service(data: dict, **kwargs):\n    result = requests.post('http://example.com', json={\"user\": data['user'], \"transaction_id\": data['id']}\n    return result.json()\n</code></pre> Warning <p>We will raise <code>IdempotencyPersistenceLayerError</code> if any of the calls to the persistence layer fail unexpectedly.</p> <p>As this happens outside the scope of your decorated function, you are not able to catch it if you're using the <code>idempotent</code> decorator on your Lambda handler.</p>"},{"location":"utilities/idempotency/#idempotency-request-flow","title":"Idempotency request flow","text":"<p>The following sequence diagrams explain how the Idempotency feature behaves under different scenarios.</p>"},{"location":"utilities/idempotency/#successful-request","title":"Successful request","text":"<p> <pre><code>sequenceDiagram\n    participant Client\n    participant Lambda\n    participant Persistence Layer\n    alt initial request\n        Client-&gt;&gt;Lambda: Invoke (event)\n        Lambda-&gt;&gt;Persistence Layer: Get or set idempotency_key=hash(payload)\n        activate Persistence Layer\n        Note over Lambda,Persistence Layer: Set record status to INPROGRESS. &lt;br&gt; Prevents concurrent invocations &lt;br&gt; with the same payload\n        Lambda--&gt;&gt;Lambda: Call your function\n        Lambda-&gt;&gt;Persistence Layer: Update record with result\n        deactivate Persistence Layer\n        Persistence Layer--&gt;&gt;Persistence Layer: Update record\n        Note over Lambda,Persistence Layer: Set record status to COMPLETE. &lt;br&gt; New invocations with the same payload &lt;br&gt; now return the same result\n        Lambda--&gt;&gt;Client: Response sent to client\n    else retried request\n        Client-&gt;&gt;Lambda: Invoke (event)\n        Lambda-&gt;&gt;Persistence Layer: Get or set idempotency_key=hash(payload)\n        activate Persistence Layer\n        Persistence Layer--&gt;&gt;Lambda: Already exists in persistence layer.\n        deactivate Persistence Layer\n        Note over Lambda,Persistence Layer: Record status is COMPLETE and not expired\n        Lambda--&gt;&gt;Client: Same response sent to client\n    end</code></pre> Idempotent successful request </p>"},{"location":"utilities/idempotency/#successful-request-with-cache-enabled","title":"Successful request with cache enabled","text":"<p>In-memory cache is disabled by default.</p> <p> <pre><code>sequenceDiagram\n    participant Client\n    participant Lambda\n    participant Persistence Layer\n    alt initial request\n      Client-&gt;&gt;Lambda: Invoke (event)\n      Lambda-&gt;&gt;Persistence Layer: Get or set idempotency_key=hash(payload)\n      activate Persistence Layer\n      Note over Lambda,Persistence Layer: Set record status to INPROGRESS. &lt;br&gt; Prevents concurrent invocations &lt;br&gt; with the same payload\n      Lambda--&gt;&gt;Lambda: Call your function\n      Lambda-&gt;&gt;Persistence Layer: Update record with result\n      deactivate Persistence Layer\n      Persistence Layer--&gt;&gt;Persistence Layer: Update record\n      Note over Lambda,Persistence Layer: Set record status to COMPLETE. &lt;br&gt; New invocations with the same payload &lt;br&gt; now return the same result\n      Lambda--&gt;&gt;Lambda: Save record and result in memory\n      Lambda--&gt;&gt;Client: Response sent to client\n    else retried request\n      Client-&gt;&gt;Lambda: Invoke (event)\n      Lambda--&gt;&gt;Lambda: Get idempotency_key=hash(payload)\n      Note over Lambda,Persistence Layer: Record status is COMPLETE and not expired\n      Lambda--&gt;&gt;Client: Same response sent to client\n    end</code></pre> Idempotent successful request cached </p>"},{"location":"utilities/idempotency/#expired-idempotency-records","title":"Expired idempotency records","text":"<p> <pre><code>sequenceDiagram\n    participant Client\n    participant Lambda\n    participant Persistence Layer\n    alt initial request\n        Client-&gt;&gt;Lambda: Invoke (event)\n        Lambda-&gt;&gt;Persistence Layer: Get or set idempotency_key=hash(payload)\n        activate Persistence Layer\n        Note over Lambda,Persistence Layer: Set record status to INPROGRESS. &lt;br&gt; Prevents concurrent invocations &lt;br&gt; with the same payload\n        Lambda--&gt;&gt;Lambda: Call your function\n        Lambda-&gt;&gt;Persistence Layer: Update record with result\n        deactivate Persistence Layer\n        Persistence Layer--&gt;&gt;Persistence Layer: Update record\n        Note over Lambda,Persistence Layer: Set record status to COMPLETE. &lt;br&gt; New invocations with the same payload &lt;br&gt; now return the same result\n        Lambda--&gt;&gt;Client: Response sent to client\n    else retried request\n        Client-&gt;&gt;Lambda: Invoke (event)\n        Lambda-&gt;&gt;Persistence Layer: Get or set idempotency_key=hash(payload)\n        activate Persistence Layer\n        Persistence Layer--&gt;&gt;Lambda: Already exists in persistence layer.\n        deactivate Persistence Layer\n        Note over Lambda,Persistence Layer: Record status is COMPLETE but expired hours ago\n        loop Repeat initial request process\n            Note over Lambda,Persistence Layer: 1. Set record to INPROGRESS, &lt;br&gt; 2. Call your function, &lt;br&gt; 3. Set record to COMPLETE\n        end\n        Lambda--&gt;&gt;Client: Same response sent to client\n    end</code></pre> Previous Idempotent request expired </p>"},{"location":"utilities/idempotency/#concurrent-identical-in-flight-requests","title":"Concurrent identical in-flight requests","text":"<p> <pre><code>sequenceDiagram\n    participant Client\n    participant Lambda\n    participant Persistence Layer\n    Client-&gt;&gt;Lambda: Invoke (event)\n    Lambda-&gt;&gt;Persistence Layer: Get or set idempotency_key=hash(payload)\n    activate Persistence Layer\n    Note over Lambda,Persistence Layer: Set record status to INPROGRESS. &lt;br&gt; Prevents concurrent invocations &lt;br&gt; with the same payload\n      par Second request\n          Client-&gt;&gt;Lambda: Invoke (event)\n          Lambda-&gt;&gt;Persistence Layer: Get or set idempotency_key=hash(payload)\n          Lambda--xLambda: IdempotencyAlreadyInProgressError\n          Lambda-&gt;&gt;Client: Error sent to client if unhandled\n      end\n    Lambda--&gt;&gt;Lambda: Call your function\n    Lambda-&gt;&gt;Persistence Layer: Update record with result\n    deactivate Persistence Layer\n    Persistence Layer--&gt;&gt;Persistence Layer: Update record\n    Note over Lambda,Persistence Layer: Set record status to COMPLETE. &lt;br&gt; New invocations with the same payload &lt;br&gt; now return the same result\n    Lambda--&gt;&gt;Client: Response sent to client</code></pre> Concurrent identical in-flight requests </p>"},{"location":"utilities/idempotency/#lambda-request-timeout","title":"Lambda request timeout","text":"<p> <pre><code>sequenceDiagram\n    participant Client\n    participant Lambda\n    participant Persistence Layer\n    alt initial request\n        Client-&gt;&gt;Lambda: Invoke (event)\n        Lambda-&gt;&gt;Persistence Layer: Get or set idempotency_key=hash(payload)\n        activate Persistence Layer\n        Note over Lambda,Persistence Layer: Set record status to INPROGRESS. &lt;br&gt; Prevents concurrent invocations &lt;br&gt; with the same payload\n        Lambda--&gt;&gt;Lambda: Call your function\n        Note right of Lambda: Time out\n        Lambda--xLambda: Time out error\n        Lambda--&gt;&gt;Client: Return error response\n        deactivate Persistence Layer\n    else retry after Lambda timeout elapses\n        Client-&gt;&gt;Lambda: Invoke (event)\n        Lambda-&gt;&gt;Persistence Layer: Get or set idempotency_key=hash(payload)\n        activate Persistence Layer\n        Note over Lambda,Persistence Layer: Set record status to INPROGRESS. &lt;br&gt; Reset in_progress_expiry attribute\n        Lambda--&gt;&gt;Lambda: Call your function\n        Lambda-&gt;&gt;Persistence Layer: Update record with result\n        deactivate Persistence Layer\n        Persistence Layer--&gt;&gt;Persistence Layer: Update record\n        Lambda--&gt;&gt;Client: Response sent to client\n    end</code></pre> Idempotent request during and after Lambda timeouts </p>"},{"location":"utilities/idempotency/#advanced","title":"Advanced","text":""},{"location":"utilities/idempotency/#persistence-layers","title":"Persistence layers","text":""},{"location":"utilities/idempotency/#dynamodbpersistencelayer","title":"DynamoDBPersistenceLayer","text":"<p>This persistence layer is built-in, and you can either use an existing DynamoDB table or create a new one dedicated for idempotency state (recommended).</p> Customizing DynamoDBPersistenceLayer to suit your table structure<pre><code>from aws_lambda_powertools.utilities.idempotency import DynamoDBPersistenceLayer\n\npersistence_layer = DynamoDBPersistenceLayer(\n    table_name=\"IdempotencyTable\",\nkey_attr=\"idempotency_key\",\nexpiry_attr=\"expires_at\",\nin_progress_expiry_attr=\"in_progress_expires_at\",\nstatus_attr=\"current_status\",\ndata_attr=\"result_data\",\nvalidation_key_attr=\"validation_key\",\n)\n</code></pre> <p>When using DynamoDB as a persistence layer, you can alter the attribute names by passing these parameters when initializing the persistence layer:</p> Parameter Required Default Description table_name Table name to store state key_attr <code>id</code> Partition key of the table. Hashed representation of the payload (unless sort_key_attr is specified) expiry_attr <code>expiration</code> Unix timestamp of when record expires in_progress_expiry_attr <code>in_progress_expiration</code> Unix timestamp of when record expires while in progress (in case of the invocation times out) status_attr <code>status</code> Stores status of the lambda execution during and after invocation data_attr <code>data</code> Stores results of successfully executed Lambda handlers validation_key_attr <code>validation</code> Hashed representation of the parts of the event used for validation sort_key_attr Sort key of the table (if table is configured with a sort key). static_pk_value <code>idempotency#{LAMBDA_FUNCTION_NAME}</code> Static value to use as the partition key. Only used when sort_key_attr is set."},{"location":"utilities/idempotency/#customizing-the-default-behavior","title":"Customizing the default behavior","text":"<p>Idempotent decorator can be further configured with <code>IdempotencyConfig</code> as seen in the previous example. These are the available options for further configuration</p> Parameter Default Description event_key_jmespath <code>\"\"</code> JMESPath expression to extract the idempotency key from the event record using built-in functions payload_validation_jmespath <code>\"\"</code> JMESPath expression to validate whether certain parameters have changed in the event while the event payload raise_on_no_idempotency_key <code>False</code> Raise exception if no idempotency key was found in the request expires_after_seconds 3600 The number of seconds to wait before a record is expired use_local_cache <code>False</code> Whether to locally cache idempotency results local_cache_max_items 256 Max number of items to store in local cache hash_function <code>md5</code> Function to use for calculating hashes, as provided by hashlib in the standard library."},{"location":"utilities/idempotency/#handling-concurrent-executions-with-the-same-payload","title":"Handling concurrent executions with the same payload","text":"<p>This utility will raise an <code>IdempotencyAlreadyInProgressError</code> exception if you receive multiple invocations with the same payload while the first invocation hasn't completed yet.</p> Info <p>If you receive <code>IdempotencyAlreadyInProgressError</code>, you can safely retry the operation.</p> <p>This is a locking mechanism for correctness. Since we don't know the result from the first invocation yet, we can't safely allow another concurrent execution.</p>"},{"location":"utilities/idempotency/#using-in-memory-cache","title":"Using in-memory cache","text":"<p>By default, in-memory local caching is disabled, since we don't know how much memory you consume per invocation compared to the maximum configured in your Lambda function.</p> Note: This in-memory cache is local to each Lambda execution environment <p>This means it will be effective in cases where your function's concurrency is low in comparison to the number of \"retry\" invocations with the same payload, because cache might be empty.</p> <p>You can enable in-memory caching with the <code>use_local_cache</code> parameter:</p> Caching idempotent transactions in-memory to prevent multiple calls to storage<pre><code>from aws_lambda_powertools.utilities.idempotency import (\n    IdempotencyConfig, DynamoDBPersistenceLayer, idempotent\n)\n\npersistence_layer = DynamoDBPersistenceLayer(table_name=\"IdempotencyTable\")\nconfig =  IdempotencyConfig(\n    event_key_jmespath=\"body\",\nuse_local_cache=True,\n)\n\n@idempotent(config=config, persistence_store=persistence_layer)\ndef handler(event, context):\n    ...\n</code></pre> <p>When enabled, the default is to cache a maximum of 256 records in each Lambda execution environment - You can change it with the <code>local_cache_max_items</code> parameter.</p>"},{"location":"utilities/idempotency/#expiring-idempotency-records","title":"Expiring idempotency records","text":"<p>By default, we expire idempotency records after an hour (3600 seconds).</p> <p>In most cases, it is not desirable to store the idempotency records forever. Rather, you want to guarantee that the same payload won't be executed within a period of time.</p> <p>You can change this window with the <code>expires_after_seconds</code> parameter:</p> Adjusting idempotency record expiration<pre><code>from aws_lambda_powertools.utilities.idempotency import (\n    IdempotencyConfig, DynamoDBPersistenceLayer, idempotent\n)\n\npersistence_layer = DynamoDBPersistenceLayer(table_name=\"IdempotencyTable\")\nconfig =  IdempotencyConfig(\n    event_key_jmespath=\"body\",\nexpires_after_seconds=5*60,  # 5 minutes\n)\n\n@idempotent(config=config, persistence_store=persistence_layer)\ndef handler(event, context):\n    ...\n</code></pre> <p>This will mark any records older than 5 minutes as expired, and your function will be executed as normal if it is invoked with a matching payload.</p> Idempotency record expiration vs DynamoDB time-to-live (TTL) <p>DynamoDB TTL is a feature to remove items after a certain period of time, it may occur within 48 hours of expiration.</p> <p>We don't rely on DynamoDB or any persistence storage layer to determine whether a record is expired to avoid eventual inconsistency states.</p> <p>Instead, Idempotency records saved in the storage layer contain timestamps that can be verified upon retrieval and double checked within Idempotency feature.</p> <p>Why?</p> <p>A record might still be valid (<code>COMPLETE</code>) when we retrieved, but in some rare cases it might expire a second later. A record could also be cached in memory. You might also want to have idempotent transactions that should expire in seconds.</p>"},{"location":"utilities/idempotency/#payload-validation","title":"Payload validation","text":"Question: What if your function is invoked with the same payload except some outer parameters have changed? <p>Example: A payment transaction for a given productID was requested twice for the same customer, however the amount to be paid has changed in the second transaction.</p> <p>By default, we will return the same result as it returned before, however in this instance it may be misleading; we provide a fail fast payload validation to address this edge case.</p> <p>With <code>payload_validation_jmespath</code>, you can provide an additional JMESPath expression to specify which part of the event body should be validated against previous idempotent invocations</p> app.pyExample Event 1Example Event 2 <pre><code>from aws_lambda_powertools.utilities.idempotency import (\n    IdempotencyConfig, DynamoDBPersistenceLayer, idempotent\n)\n\nconfig = IdempotencyConfig(\n    event_key_jmespath=\"[userDetail, productId]\",\npayload_validation_jmespath=\"amount\"\n)\npersistence_layer = DynamoDBPersistenceLayer(table_name=\"IdempotencyTable\")\n\n@idempotent(config=config, persistence_store=persistence_layer)\ndef handler(event, context):\n    # Creating a subscription payment is a side\n    # effect of calling this function!\n    payment = create_subscription_payment(\n      user=event['userDetail']['username'],\n      product=event['product_id'],\namount=event['amount']\n)\n    ...\n    return {\n        \"message\": \"success\",\n        \"statusCode\": 200,\n        \"payment_id\": payment.id,\n\"amount\": payment.amount\n}\n</code></pre> <pre><code>{\n\"userDetail\": {\n\"username\": \"User1\",\n\"user_email\": \"user@example.com\"\n},\n\"productId\": 1500,\n\"charge_type\": \"subscription\",\n\"amount\": 500\n}\n</code></pre> <pre><code>{\n\"userDetail\": {\n\"username\": \"User1\",\n\"user_email\": \"user@example.com\"\n},\n\"productId\": 1500,\n\"charge_type\": \"subscription\",\n\"amount\": 1\n}\n</code></pre> <p>In this example, the <code>userDetail</code> and <code>productId</code> keys are used as the payload to generate the idempotency key, as per <code>event_key_jmespath</code> parameter.</p> Note <p>If we try to send the same request but with a different amount, we will raise <code>IdempotencyValidationError</code>.</p> <p>Without payload validation, we would have returned the same result as we did for the initial request. Since we're also returning an amount in the response, this could be quite confusing for the client.</p> <p>By using <code>payload_validation_jmespath=\"amount\"</code>, we prevent this potentially confusing behavior and instead raise an Exception.</p>"},{"location":"utilities/idempotency/#making-idempotency-key-required","title":"Making idempotency key required","text":"<p>If you want to enforce that an idempotency key is required, you can set <code>raise_on_no_idempotency_key</code> to <code>True</code>.</p> <p>This means that we will raise <code>IdempotencyKeyError</code> if the evaluation of <code>event_key_jmespath</code> is <code>None</code>.</p> app.pySuccess EventFailure Event <pre><code>from aws_lambda_powertools.utilities.idempotency import (\n    IdempotencyConfig, DynamoDBPersistenceLayer, idempotent\n)\n\npersistence_layer = DynamoDBPersistenceLayer(table_name=\"IdempotencyTable\")\n\n# Requires \"user\".\"uid\" and \"order_id\" to be present\nconfig = IdempotencyConfig(\nevent_key_jmespath=\"[user.uid, order_id]\",\nraise_on_no_idempotency_key=True,\n)\n\n@idempotent(config=config, persistence_store=persistence_layer)\ndef handler(event, context):\n    pass\n</code></pre> <pre><code>{\n\"user\": {\n\"uid\": \"BB0D045C-8878-40C8-889E-38B3CB0A61B1\",\n\"name\": \"Foo\"\n},\n\"order_id\": 10000\n}\n</code></pre> <p>Notice that <code>order_id</code> is now accidentally within <code>user</code> key</p> <pre><code>{\n\"user\": {\n\"uid\": \"DE0D000E-1234-10D1-991E-EAC1DD1D52C8\",\n\"name\": \"Joe Bloggs\",\n\"order_id\": 10000\n},\n}\n</code></pre>"},{"location":"utilities/idempotency/#customizing-boto-configuration","title":"Customizing boto configuration","text":"<p>The <code>boto_config</code> and <code>boto3_session</code> parameters enable you to pass in a custom botocore config object or a custom boto3 session when constructing the persistence store.</p> Custom sessionCustom config <pre><code>import boto3\nfrom aws_lambda_powertools.utilities.idempotency import (\n    IdempotencyConfig, DynamoDBPersistenceLayer, idempotent\n)\n\nboto3_session = boto3.session.Session()\npersistence_layer = DynamoDBPersistenceLayer(\n    table_name=\"IdempotencyTable\",\nboto3_session=boto3_session\n)\n\nconfig = IdempotencyConfig(event_key_jmespath=\"body\")\n\n@idempotent(config=config, persistence_store=persistence_layer)\ndef handler(event, context):\n   ...\n</code></pre> <pre><code>from botocore.config import Config\nfrom aws_lambda_powertools.utilities.idempotency import (\n    IdempotencyConfig, DynamoDBPersistenceLayer, idempotent\n)\n\nconfig = IdempotencyConfig(event_key_jmespath=\"body\")\nboto_config = Config()\npersistence_layer = DynamoDBPersistenceLayer(\n    table_name=\"IdempotencyTable\",\nboto_config=boto_config\n)\n\n@idempotent(config=config, persistence_store=persistence_layer)\ndef handler(event, context):\n   ...\n</code></pre>"},{"location":"utilities/idempotency/#using-a-dynamodb-table-with-a-composite-primary-key","title":"Using a DynamoDB table with a composite primary key","text":"<p>When using a composite primary key table (hash+range key), use <code>sort_key_attr</code> parameter when initializing your persistence layer.</p> <p>With this setting, we will save the idempotency key in the sort key instead of the primary key. By default, the primary key will now be set to <code>idempotency#{LAMBDA_FUNCTION_NAME}</code>.</p> <p>You can optionally set a static value for the partition key using the <code>static_pk_value</code> parameter.</p> Reusing a DynamoDB table that uses a composite primary key<pre><code>from aws_lambda_powertools.utilities.idempotency import DynamoDBPersistenceLayer, idempotent\n\npersistence_layer = DynamoDBPersistenceLayer(\n    table_name=\"IdempotencyTable\",\nsort_key_attr='sort_key')\n@idempotent(persistence_store=persistence_layer)\ndef handler(event, context):\n    return {\"message\": \"success\": \"id\": event['body']['id]}\n</code></pre> <p>The example function above would cause data to be stored in DynamoDB like this:</p> id sort_key expiration status data idempotency#MyLambdaFunction 1e956ef7da78d0cb890be999aecc0c9e 1636549553 COMPLETED {\"id\": 12391, \"message\": \"success\"} idempotency#MyLambdaFunction 2b2cdb5f86361e97b4383087c1ffdf27 1636549571 COMPLETED {\"id\": 527212, \"message\": \"success\"} idempotency#MyLambdaFunction f091d2527ad1c78f05d54cc3f363be80 1636549585 IN_PROGRESS"},{"location":"utilities/idempotency/#bring-your-own-persistent-store","title":"Bring your own persistent store","text":"<p>This utility provides an abstract base class (ABC), so that you can implement your choice of persistent storage layer.</p> <p>You can inherit from the <code>BasePersistenceLayer</code> class and implement the abstract methods <code>_get_record</code>, <code>_put_record</code>, <code>_update_record</code> and <code>_delete_record</code>.</p> Excerpt DynamoDB Persistence Layer implementation for reference<pre><code>import datetime\nimport logging\nfrom typing import Any, Dict, Optional\n\nimport boto3\nfrom botocore.config import Config\n\nfrom aws_lambda_powertools.utilities.idempotency import BasePersistenceLayer\nfrom aws_lambda_powertools.utilities.idempotency.exceptions import (\nIdempotencyItemAlreadyExistsError,\nIdempotencyItemNotFoundError,\n)\nfrom aws_lambda_powertools.utilities.idempotency.persistence.base import DataRecord\nlogger = logging.getLogger(__name__)\n\n\nclass DynamoDBPersistenceLayer(BasePersistenceLayer):\n    def __init__(\n        self,\n        table_name: str,\n        key_attr: str = \"id\",\n        expiry_attr: str = \"expiration\",\n        status_attr: str = \"status\",\n        data_attr: str = \"data\",\n        validation_key_attr: str = \"validation\",\n        boto_config: Optional[Config] = None,\n        boto3_session: Optional[boto3.session.Session] = None,\n    ):\n        boto_config = boto_config or Config()\n        session = boto3_session or boto3.session.Session()\n        self._ddb_resource = session.resource(\"dynamodb\", config=boto_config)\n        self.table_name = table_name\n        self.table = self._ddb_resource.Table(self.table_name)\n        self.key_attr = key_attr\n        self.expiry_attr = expiry_attr\n        self.status_attr = status_attr\n        self.data_attr = data_attr\n        self.validation_key_attr = validation_key_attr\n        super(DynamoDBPersistenceLayer, self).__init__()\n\n    def _item_to_data_record(self, item: Dict[str, Any]) -&gt; DataRecord:\n\"\"\"\n        Translate raw item records from DynamoDB to DataRecord\n\n        Parameters\n        ----------\n        item: Dict[str, Union[str, int]]\n            Item format from dynamodb response\n\n        Returns\n        -------\n        DataRecord\n            representation of item\n\n        \"\"\"\nreturn DataRecord(\nidempotency_key=item[self.key_attr],\n            status=item[self.status_attr],\n            expiry_timestamp=item[self.expiry_attr],\n            response_data=item.get(self.data_attr),\n            payload_hash=item.get(self.validation_key_attr),\n        )\n\ndef _get_record(self, idempotency_key) -&gt; DataRecord:\nresponse = self.table.get_item(Key={self.key_attr: idempotency_key}, ConsistentRead=True)\n\n        try:\n            item = response[\"Item\"]\n        except KeyError:\n            raise IdempotencyItemNotFoundError\n        return self._item_to_data_record(item)\n\ndef _put_record(self, data_record: DataRecord) -&gt; None:\nitem = {\n            self.key_attr: data_record.idempotency_key,\n            self.expiry_attr: data_record.expiry_timestamp,\n            self.status_attr: data_record.status,\n        }\n\n        if self.payload_validation_enabled:\n            item[self.validation_key_attr] = data_record.payload_hash\n\n        now = datetime.datetime.now()\n        try:\n            logger.debug(f\"Putting record for idempotency key: {data_record.idempotency_key}\")\n            self.table.put_item(\n                Item=item,\n                ConditionExpression=f\"attribute_not_exists({self.key_attr}) OR {self.expiry_attr} &lt; :now\",\n                ExpressionAttributeValues={\":now\": int(now.timestamp())},\n            )\n        except self._ddb_resource.meta.client.exceptions.ConditionalCheckFailedException:\n            logger.debug(f\"Failed to put record for already existing idempotency key: {data_record.idempotency_key}\")\n            raise IdempotencyItemAlreadyExistsError\n\ndef _update_record(self, data_record: DataRecord):\nlogger.debug(f\"Updating record for idempotency key: {data_record.idempotency_key}\")\n        update_expression = \"SET #response_data = :response_data, #expiry = :expiry, #status = :status\"\n        expression_attr_values = {\n            \":expiry\": data_record.expiry_timestamp,\n            \":response_data\": data_record.response_data,\n            \":status\": data_record.status,\n        }\n        expression_attr_names = {\n            \"#response_data\": self.data_attr,\n            \"#expiry\": self.expiry_attr,\n            \"#status\": self.status_attr,\n        }\n\n        if self.payload_validation_enabled:\n            update_expression += \", #validation_key = :validation_key\"\n            expression_attr_values[\":validation_key\"] = data_record.payload_hash\n            expression_attr_names[\"#validation_key\"] = self.validation_key_attr\n\n        kwargs = {\n            \"Key\": {self.key_attr: data_record.idempotency_key},\n            \"UpdateExpression\": update_expression,\n            \"ExpressionAttributeValues\": expression_attr_values,\n            \"ExpressionAttributeNames\": expression_attr_names,\n        }\n\n        self.table.update_item(**kwargs)\n\ndef _delete_record(self, data_record: DataRecord) -&gt; None:\nlogger.debug(f\"Deleting record for idempotency key: {data_record.idempotency_key}\")\n        self.table.delete_item(Key={self.key_attr: data_record.idempotency_key},)\n</code></pre> Danger <p>Pay attention to the documentation for each - you may need to perform additional checks inside these methods to ensure the idempotency guarantees remain intact.</p> <p>For example, the <code>_put_record</code> method needs to raise an exception if a non-expired record already exists in the data store with a matching key.</p>"},{"location":"utilities/idempotency/#compatibility-with-other-utilities","title":"Compatibility with other utilities","text":""},{"location":"utilities/idempotency/#batch","title":"Batch","text":"<p>See Batch integration above.</p>"},{"location":"utilities/idempotency/#validation-utility","title":"Validation utility","text":"<p>The idempotency utility can be used with the <code>validator</code> decorator. Ensure that idempotency is the innermost decorator.</p> Warning <p>If you use an envelope with the validator, the event received by the idempotency utility will be the unwrapped event - not the \"raw\" event Lambda was invoked with.</p> <p>Make sure to account for this behaviour, if you set the <code>event_key_jmespath</code>.</p> Using Idempotency with JSONSchema Validation utility<pre><code>from aws_lambda_powertools.utilities.validation import validator, envelopes\nfrom aws_lambda_powertools.utilities.idempotency import (\n    IdempotencyConfig, DynamoDBPersistenceLayer, idempotent\n)\n\nconfig = IdempotencyConfig(event_key_jmespath=\"[message, username]\")\npersistence_layer = DynamoDBPersistenceLayer(table_name=\"IdempotencyTable\")\n\n@validator(envelope=envelopes.API_GATEWAY_HTTP)\n@idempotent(config=config, persistence_store=persistence_layer)\ndef lambda_handler(event, context):\n    cause_some_side_effects(event['username')\n    return {\"message\": event['message'], \"statusCode\": 200}\n</code></pre> Tip: JMESPath Powertools functions are also available <p>Built-in functions known in the validation utility like <code>powertools_json</code>, <code>powertools_base64</code>, <code>powertools_base64_gzip</code> are also available to use in this utility.</p>"},{"location":"utilities/idempotency/#testing-your-code","title":"Testing your code","text":"<p>The idempotency utility provides several routes to test your code.</p>"},{"location":"utilities/idempotency/#disabling-the-idempotency-utility","title":"Disabling the idempotency utility","text":"<p>When testing your code, you may wish to disable the idempotency logic altogether and focus on testing your business logic. To do this, you can set the environment variable <code>POWERTOOLS_IDEMPOTENCY_DISABLED</code> with a truthy value. If you prefer setting this for specific tests, and are using Pytest, you can use monkeypatch fixture:</p> tests.pyapp.py <pre><code>from dataclasses import dataclass\n\nimport pytest\n\nimport app\n\n\n@pytest.fixture\ndef lambda_context():\n    @dataclass\n    class LambdaContext:\n        function_name: str = \"test\"\n        memory_limit_in_mb: int = 128\n        invoked_function_arn: str = \"arn:aws:lambda:eu-west-1:809313241:function:test\"\n        aws_request_id: str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\"\n\n        def get_remaining_time_in_millis(self) -&gt; int:\n          return 5\n\n    return LambdaContext()\n\n\ndef test_idempotent_lambda_handler(monkeypatch, lambda_context):\n# Set POWERTOOLS_IDEMPOTENCY_DISABLED before calling decorated functions\nmonkeypatch.setenv(\"POWERTOOLS_IDEMPOTENCY_DISABLED\", 1)\nresult = handler({}, lambda_context)\n    ...\n</code></pre> <pre><code>from aws_lambda_powertools.utilities.idempotency import (\nDynamoDBPersistenceLayer, idempotent\n)\n\npersistence_layer = DynamoDBPersistenceLayer(table_name=\"idempotency\")\n\n@idempotent(persistence_store=persistence_layer)\ndef handler(event, context):\n    print('expensive operation')\n    return {\n        \"payment_id\": 12345,\n        \"message\": \"success\",\n        \"statusCode\": 200,\n    }\n</code></pre>"},{"location":"utilities/idempotency/#testing-with-dynamodb-local","title":"Testing with DynamoDB Local","text":"<p>To test with DynamoDB Local, you can replace the <code>DynamoDB client</code> used by the persistence layer with one you create inside your tests. This allows you to set the endpoint_url.</p> tests.pyapp.py <pre><code>from dataclasses import dataclass\n\nimport boto3\nimport pytest\n\nimport app\n\n\n@pytest.fixture\ndef lambda_context():\n    @dataclass\n    class LambdaContext:\n        function_name: str = \"test\"\n        memory_limit_in_mb: int = 128\n        invoked_function_arn: str = \"arn:aws:lambda:eu-west-1:809313241:function:test\"\n        aws_request_id: str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\"\n\n        def get_remaining_time_in_millis(self) -&gt; int:\n          return 5\n\n    return LambdaContext()\n\ndef test_idempotent_lambda(lambda_context):\n# Configure the boto3 to use the endpoint for the DynamoDB Local instance\ndynamodb_local_client = boto3.client(\"dynamodb\", endpoint_url='http://localhost:8000')\napp.persistence_layer.client = dynamodb_local_client\n# If desired, you can use a different DynamoDB Local table name than what your code already uses\n    # app.persistence_layer.table_name = \"another table name\"\n\n    result = app.handler({'testkey': 'testvalue'}, lambda_context)\n    assert result['payment_id'] == 12345\n</code></pre> <pre><code>from aws_lambda_powertools.utilities.idempotency import (\nDynamoDBPersistenceLayer, idempotent\n)\n\npersistence_layer = DynamoDBPersistenceLayer(table_name=\"idempotency\")\n\n@idempotent(persistence_store=persistence_layer)\ndef handler(event, context):\n    print('expensive operation')\n    return {\n        \"payment_id\": 12345,\n        \"message\": \"success\",\n        \"statusCode\": 200,\n    }\n</code></pre>"},{"location":"utilities/idempotency/#how-do-i-mock-all-dynamodb-io-operations","title":"How do I mock all DynamoDB I/O operations","text":"<p>The idempotency utility lazily creates the dynamodb Table which it uses to access DynamoDB. This means it is possible to pass a mocked Table resource, or stub various methods.</p> tests.pyapp.py <pre><code>from dataclasses import dataclass\nfrom unittest.mock import MagicMock\n\nimport boto3\nimport pytest\n\nimport app\n\n\n@pytest.fixture\ndef lambda_context():\n    @dataclass\n    class LambdaContext:\n        function_name: str = \"test\"\n        memory_limit_in_mb: int = 128\n        invoked_function_arn: str = \"arn:aws:lambda:eu-west-1:809313241:function:test\"\n        aws_request_id: str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\"\n\n        def get_remaining_time_in_millis(self) -&gt; int:\n          return 5\n\n    return LambdaContext()\n\n\ndef test_idempotent_lambda(lambda_context):\nmock_client = MagicMock()\napp.persistence_layer.client = mock_client\nresult = app.handler({'testkey': 'testvalue'}, lambda_context)\nmock_client.put_item.assert_called()\n...\n</code></pre> <pre><code>from aws_lambda_powertools.utilities.idempotency import (\nDynamoDBPersistenceLayer, idempotent\n)\n\npersistence_layer = DynamoDBPersistenceLayer(table_name=\"idempotency\")\n\n@idempotent(persistence_store=persistence_layer)\ndef handler(event, context):\n    print('expensive operation')\n    return {\n        \"payment_id\": 12345,\n        \"message\": \"success\",\n        \"statusCode\": 200,\n    }\n</code></pre>"},{"location":"utilities/idempotency/#extra-resources","title":"Extra resources","text":"<p>If you're interested in a deep dive on how Amazon uses idempotency when building our APIs, check out this article.</p>"},{"location":"utilities/jmespath_functions/","title":"JMESPath Functions","text":"Tip <p>JMESPath is a query language for JSON used by AWS CLI, AWS Python SDK, and AWS Lambda Powertools for Python.</p> <p>Built-in JMESPath Functions to easily deserialize common encoded JSON payloads in Lambda functions.</p>"},{"location":"utilities/jmespath_functions/#key-features","title":"Key features","text":"<ul> <li>Deserialize JSON from JSON strings, base64, and compressed data</li> <li>Use JMESPath to extract and combine data recursively</li> <li>Provides commonly used JMESPath expression with popular event sources</li> </ul>"},{"location":"utilities/jmespath_functions/#getting-started","title":"Getting started","text":"Tip <p>All examples shared in this documentation are available within the project repository.</p> <p>You might have events that contains encoded JSON payloads as string, base64, or even in compressed format. It is a common use case to decode and extract them partially or fully as part of your Lambda function invocation.</p> <p>Powertools also have utilities like validation, idempotency, or feature flags where you might need to extract a portion of your data before using them.</p> Terminology <p>Envelope is the terminology we use for the JMESPath expression to extract your JSON object from your data input. We might use those two terms interchangeably.</p>"},{"location":"utilities/jmespath_functions/#extracting-data","title":"Extracting data","text":"<p>You can use the <code>extract_data_from_envelope</code> function with any JMESPath expression.</p> Tip <p>Another common use case is to fetch deeply nested data, filter, flatten, and more.</p> extract_data_from_envelope.pyextract_data_from_envelope.json <pre><code>from aws_lambda_powertools.utilities.jmespath_utils import extract_data_from_envelope\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef handler(event: dict, context: LambdaContext) -&gt; dict:\npayload = extract_data_from_envelope(data=event, envelope=\"powertools_json(body)\")\ncustomer_id = payload.get(\"customerId\")  # now deserialized\n\n    # also works for fetching and flattening deeply nested data\nsome_data = extract_data_from_envelope(data=event, envelope=\"deeply_nested[*].some_data[]\")\nreturn {\"customer_id\": customer_id, \"message\": \"success\", \"context\": some_data, \"statusCode\": 200}\n</code></pre> <pre><code>{\n\"body\": \"{\\\"customerId\\\":\\\"dd4649e6-2484-4993-acb8-0f9123103394\\\"}\",\n\"deeply_nested\": [\n{\n\"some_data\": [\n1,\n2,\n3\n]\n}\n]\n}\n</code></pre>"},{"location":"utilities/jmespath_functions/#built-in-envelopes","title":"Built-in envelopes","text":"<p>We provide built-in envelopes for popular AWS Lambda event sources to easily decode and/or deserialize JSON objects.</p> extract_data_from_builtin_envelope.pyextract_data_from_builtin_envelope.json <pre><code>from __future__ import annotations\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.jmespath_utils import (\nenvelopes,\n    extract_data_from_envelope,\n)\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nlogger = Logger()\n\n\ndef handler(event: dict, context: LambdaContext) -&gt; dict:\n    records: list = extract_data_from_envelope(data=event, envelope=envelopes.SQS)\n    for record in records:  # records is a list\n        logger.info(record.get(\"customerId\"))  # now deserialized\n\n    return {\"message\": \"success\", \"statusCode\": 200}\n</code></pre> <pre><code>{\n\"Records\": [\n{\n\"messageId\": \"19dd0b57-b21e-4ac1-bd88-01bbb068cb78\",\n\"receiptHandle\": \"MessageReceiptHandle\",\n\"body\": \"{\\\"customerId\\\":\\\"dd4649e6-2484-4993-acb8-0f9123103394\\\",\\\"booking\\\":{\\\"id\\\":\\\"5b2c4803-330b-42b7-811a-c68689425de1\\\",\\\"reference\\\":\\\"ySz7oA\\\",\\\"outboundFlightId\\\":\\\"20c0d2f2-56a3-4068-bf20-ff7703db552d\\\"},\\\"payment\\\":{\\\"receipt\\\":\\\"https:\\/\\/pay.stripe.com\\/receipts\\/acct_1Dvn7pF4aIiftV70\\/ch_3JTC14F4aIiftV700iFq2CHB\\/rcpt_K7QsrFln9FgFnzUuBIiNdkkRYGxUL0X\\\",\\\"amount\\\":100}}\",\n\"attributes\": {\n\"ApproximateReceiveCount\": \"1\",\n\"SentTimestamp\": \"1523232000000\",\n\"SenderId\": \"123456789012\",\n\"ApproximateFirstReceiveTimestamp\": \"1523232000001\"\n},\n\"messageAttributes\": {},\n\"md5OfBody\": \"7b270e59b47ff90a553787216d55d91d\",\n\"eventSource\": \"aws:sqs\",\n\"eventSourceARN\": \"arn:aws:sqs:us-east-1:123456789012:MyQueue\",\n\"awsRegion\": \"us-east-1\"\n}\n]\n}\n</code></pre> <p>These are all built-in envelopes you can use along with their expression as a reference:</p> Envelope JMESPath expression <code>API_GATEWAY_HTTP</code> <code>powertools_json(body)</code> <code>API_GATEWAY_REST</code> <code>powertools_json(body)</code> <code>CLOUDWATCH_EVENTS_SCHEDULED</code> <code>detail</code> <code>CLOUDWATCH_LOGS</code> <code>awslogs.powertools_base64_gzip(data)                                                     | powertools_json(@).logEvents[*]</code> <code>EVENTBRIDGE</code> <code>detail</code> <code>KINESIS_DATA_STREAM</code> <code>Records[*].kinesis.powertools_json(powertools_base64(data))</code> <code>S3_EVENTBRIDGE_SQS</code> <code>Records[*].powertools_json(body).detail</code> <code>S3_KINESIS_FIREHOSE</code> <code>records[*].powertools_json(powertools_base64(data)).Records[0]</code> <code>S3_SNS_KINESIS_FIREHOSE</code> <code>records[*].powertools_json(powertools_base64(data)).powertools_json(Message).Records[0]</code> <code>S3_SNS_SQS</code> <code>Records[*].powertools_json(body).powertools_json(Message).Records[0]</code> <code>S3_SQS</code> <code>Records[*].powertools_json(body).Records[0]</code> <code>SNS</code> <code>Records[0].Sns.Message                                                                   | powertools_json(@)</code> <code>SQS</code> <code>Records[*].powertools_json(body)</code> Using SNS? <p>If you don't require SNS metadata, enable raw message delivery{target=\"blank\"}. It will reduce multiple payload layers and size, when using SNS in combination with other services (_e.g., SQS, S3, etc).</p>"},{"location":"utilities/jmespath_functions/#advanced","title":"Advanced","text":""},{"location":"utilities/jmespath_functions/#built-in-jmespath-functions","title":"Built-in JMESPath functions","text":"<p>You can use our built-in JMESPath functions within your envelope expression. They handle deserialization for common data formats found in AWS Lambda event sources such as JSON strings, base64, and uncompress gzip data.</p> Info <p>We use these everywhere in Powertools to easily decode and unwrap events from Amazon API Gateway, Amazon Kinesis, AWS CloudWatch Logs, etc.</p>"},{"location":"utilities/jmespath_functions/#powertools_json-function","title":"powertools_json function","text":"<p>Use <code>powertools_json</code> function to decode any JSON string anywhere a JMESPath expression is allowed.</p> <p>Validation scenario</p> <p>This sample will deserialize the JSON string within the <code>data</code> key before validation.</p> powertools_json_jmespath_function.pypowertools_json_jmespath_schema.pypowertools_json_jmespath_payload.json <pre><code>import json\nfrom dataclasses import asdict, dataclass, field, is_dataclass\nfrom uuid import uuid4\n\nimport powertools_json_jmespath_schema as schemas\nfrom jmespath.exceptions import JMESPathTypeError\n\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nfrom aws_lambda_powertools.utilities.validation import SchemaValidationError, validate\n\n\n@dataclass\nclass Order:\n    user_id: int\n    product_id: int\n    quantity: int\n    price: float\n    currency: str\n    order_id: str = field(default_factory=lambda: f\"{uuid4()}\")\n\n\nclass DataclassCustomEncoder(json.JSONEncoder):\n\"\"\"A custom JSON encoder to serialize dataclass obj\"\"\"\n\n    def default(self, obj):\n        # Only called for values that aren't JSON serializable\n        # where `obj` will be an instance of Order in this example\n        return asdict(obj) if is_dataclass(obj) else super().default(obj)\n\n\ndef lambda_handler(event, context: LambdaContext) -&gt; dict:\n    try:\n        # Validate order against our schema\nvalidate(event=event, schema=schemas.INPUT, envelope=\"powertools_json(payload)\")\n# Deserialize JSON string order as dict\n        # alternatively, extract_data_from_envelope works here too\n        order_payload: dict = json.loads(event.get(\"payload\"))\n\n        return {\n            \"order\": json.dumps(Order(**order_payload), cls=DataclassCustomEncoder),\n            \"message\": \"order created\",\n            \"success\": True,\n        }\nexcept JMESPathTypeError:\n# The powertools_json() envelope function must match a valid path\n        return return_error_message(\"Invalid request.\")\nexcept SchemaValidationError as exception:\n# SchemaValidationError indicates where a data mismatch is\n        return return_error_message(str(exception))\nexcept json.JSONDecodeError:\nreturn return_error_message(\"Payload must be valid JSON (base64 encoded).\")\n\n\ndef return_error_message(message: str) -&gt; dict:\n    return {\"order\": None, \"message\": message, \"success\": False}\n</code></pre> <pre><code>INPUT = {\n    \"$schema\": \"http://json-schema.org/draft-07/schema\",\n    \"$id\": \"http://example.com/example.json\",\n    \"type\": \"object\",\n    \"title\": \"Sample order schema\",\n    \"description\": \"The root schema comprises the entire JSON document.\",\n\"examples\": [{\"user_id\": 123, \"product_id\": 1, \"quantity\": 2, \"price\": 10.40, \"currency\": \"USD\"}],\n\"required\": [\"user_id\", \"product_id\", \"quantity\", \"price\", \"currency\"],\n\"properties\": {\n\"user_id\": {\n\"$id\": \"#/properties/user_id\",\n\"type\": \"integer\",\n\"title\": \"The unique identifier of the user\",\n            \"examples\": [123],\n            \"maxLength\": 10,\n        },\n\"product_id\": {\n\"$id\": \"#/properties/product_id\",\n\"type\": \"integer\",\n\"title\": \"The unique identifier of the product\",\n            \"examples\": [1],\n            \"maxLength\": 10,\n        },\n\"quantity\": {\n\"$id\": \"#/properties/quantity\",\n\"type\": \"integer\",\n\"title\": \"The quantity of the product\",\n            \"examples\": [2],\n            \"maxLength\": 10,\n        },\n\"price\": {\n\"$id\": \"#/properties/price\",\n\"type\": \"number\",\n\"title\": \"The individual price of the product\",\n            \"examples\": [10.40],\n            \"maxLength\": 10,\n        },\n\"currency\": {\n\"$id\": \"#/properties/currency\",\n\"type\": \"string\",\n\"title\": \"The currency\",\n            \"examples\": [\"The currency of the order\"],\n            \"maxLength\": 100,\n        },\n    },\n}\n</code></pre> <pre><code>{\n\"payload\":\"{\\\"user_id\\\": 123, \\\"product_id\\\": 1, \\\"quantity\\\": 2, \\\"price\\\": 10.40, \\\"currency\\\": \\\"USD\\\"}\"\n}\n</code></pre> <p>Idempotency scenario</p> <p>This sample will deserialize the JSON string within the <code>body</code> key before Idempotency processes it.</p> powertools_json_idempotency_jmespath.pypowertools_json_idempotency_jmespath.json <pre><code>import json\nfrom uuid import uuid4\n\nimport requests\n\nfrom aws_lambda_powertools.utilities.idempotency import (\n    DynamoDBPersistenceLayer,\n    IdempotencyConfig,\n    idempotent,\n)\n\npersistence_layer = DynamoDBPersistenceLayer(table_name=\"IdempotencyTable\")\n\n# Treat everything under the \"body\" key\n# in the event json object as our payload\nconfig = IdempotencyConfig(event_key_jmespath=\"powertools_json(body)\")\nclass PaymentError(Exception):\n    ...\n\n\n@idempotent(config=config, persistence_store=persistence_layer)\ndef handler(event, context) -&gt; dict:\n    body = json.loads(event[\"body\"])\n    try:\n        payment: dict = create_subscription_payment(user=body[\"user\"], product_id=body[\"product_id\"])\n        return {\"payment_id\": payment.get(\"id\"), \"message\": \"success\", \"statusCode\": 200}\n    except requests.HTTPError as e:\n        raise PaymentError(\"Unable to create payment subscription\") from e\n\n\ndef create_subscription_payment(user: str, product_id: str) -&gt; dict:\n    payload = {\"user\": user, \"product_id\": product_id}\n    ret: requests.Response = requests.post(url=\"https://httpbin.org/anything\", data=payload)\n    ret.raise_for_status()\n\n    return {\"id\": f\"{uuid4()}\", \"message\": \"paid\"}\n</code></pre> <pre><code>{\n\"version\":\"2.0\",\n\"routeKey\":\"ANY /createpayment\",\n\"rawPath\":\"/createpayment\",\n\"rawQueryString\":\"\",\n\"headers\": {\n\"Header1\": \"value1\",\n\"Header2\": \"value2\"\n},\n\"requestContext\":{\n\"accountId\":\"123456789012\",\n\"apiId\":\"api-id\",\n\"domainName\":\"id.execute-api.us-east-1.amazonaws.com\",\n\"domainPrefix\":\"id\",\n\"http\":{\n\"method\":\"POST\",\n\"path\":\"/createpayment\",\n\"protocol\":\"HTTP/1.1\",\n\"sourceIp\":\"ip\",\n\"userAgent\":\"agent\"\n},\n\"requestId\":\"id\",\n\"routeKey\":\"ANY /createpayment\",\n\"stage\":\"$default\",\n\"time\":\"10/Feb/2021:13:40:43 +0000\",\n\"timeEpoch\":1612964443723\n},\n\"body\":\"{\\\"user\\\":\\\"xyz\\\",\\\"product_id\\\":\\\"123456789\\\"}\",\n\"isBase64Encoded\":false\n}\n</code></pre>"},{"location":"utilities/jmespath_functions/#powertools_base64-function","title":"powertools_base64 function","text":"<p>Use <code>powertools_base64</code> function to decode any base64 data.</p> <p>This sample will decode the base64 value within the <code>data</code> key, and deserialize the JSON string before validation.</p> powertools_base64_jmespath_function.pypowertools_base64_jmespath_schema.pypowertools_base64_jmespath_payload.json <pre><code>import base64\nimport binascii\nimport json\nfrom dataclasses import asdict, dataclass, field, is_dataclass\nfrom uuid import uuid4\n\nimport powertools_base64_jmespath_schema as schemas\nfrom jmespath.exceptions import JMESPathTypeError\n\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nfrom aws_lambda_powertools.utilities.validation import SchemaValidationError, validate\n\n\n@dataclass\nclass Order:\n    user_id: int\n    product_id: int\n    quantity: int\n    price: float\n    currency: str\n    order_id: str = field(default_factory=lambda: f\"{uuid4()}\")\n\n\nclass DataclassCustomEncoder(json.JSONEncoder):\n\"\"\"A custom JSON encoder to serialize dataclass obj\"\"\"\n\n    def default(self, obj):\n        # Only called for values that aren't JSON serializable\n        # where `obj` will be an instance of Todo in this example\n        return asdict(obj) if is_dataclass(obj) else super().default(obj)\n\n\ndef lambda_handler(event, context: LambdaContext) -&gt; dict:\n\n    # Try to validate the schema\n    try:\nvalidate(event=event, schema=schemas.INPUT, envelope=\"powertools_json(powertools_base64(payload))\")\n# alternatively, extract_data_from_envelope works here too\n        payload_decoded = base64.b64decode(event[\"payload\"]).decode()\n\n        order_payload: dict = json.loads(payload_decoded)\n\n        return {\n            \"order\": json.dumps(Order(**order_payload), cls=DataclassCustomEncoder),\n            \"message\": \"order created\",\n            \"success\": True,\n        }\nexcept JMESPathTypeError:\nreturn return_error_message(\n            \"The powertools_json(powertools_base64()) envelope function must match a valid path.\"\n        )\nexcept binascii.Error:\nreturn return_error_message(\"Payload must be a valid base64 encoded string\")\nexcept json.JSONDecodeError:\nreturn return_error_message(\"Payload must be valid JSON (base64 encoded).\")\nexcept SchemaValidationError as exception:\n# SchemaValidationError indicates where a data mismatch is\n        return return_error_message(str(exception))\n\n\ndef return_error_message(message: str) -&gt; dict:\n    return {\"order\": None, \"message\": message, \"success\": False}\n</code></pre> <pre><code>INPUT = {\n    \"$schema\": \"http://json-schema.org/draft-07/schema\",\n    \"$id\": \"http://example.com/example.json\",\n    \"type\": \"object\",\n    \"title\": \"Sample order schema\",\n    \"description\": \"The root schema comprises the entire JSON document.\",\n\"examples\": [{\"user_id\": 123, \"product_id\": 1, \"quantity\": 2, \"price\": 10.40, \"currency\": \"USD\"}],\n\"required\": [\"user_id\", \"product_id\", \"quantity\", \"price\", \"currency\"],\n\"properties\": {\n\"user_id\": {\n\"$id\": \"#/properties/user_id\",\n\"type\": \"integer\",\n\"title\": \"The unique identifier of the user\",\n            \"examples\": [123],\n            \"maxLength\": 10,\n        },\n\"product_id\": {\n\"$id\": \"#/properties/product_id\",\n\"type\": \"integer\",\n\"title\": \"The unique identifier of the product\",\n            \"examples\": [1],\n            \"maxLength\": 10,\n        },\n\"quantity\": {\n\"$id\": \"#/properties/quantity\",\n\"type\": \"integer\",\n\"title\": \"The quantity of the product\",\n            \"examples\": [2],\n            \"maxLength\": 10,\n        },\n\"price\": {\n\"$id\": \"#/properties/price\",\n\"type\": \"number\",\n\"title\": \"The individual price of the product\",\n            \"examples\": [10.40],\n            \"maxLength\": 10,\n        },\n\"currency\": {\n\"$id\": \"#/properties/currency\",\n\"type\": \"string\",\n\"title\": \"The currency\",\n            \"examples\": [\"The currency of the order\"],\n            \"maxLength\": 100,\n        },\n    },\n}\n</code></pre> <pre><code>{\n\"payload\":\"eyJ1c2VyX2lkIjogMTIzLCAicHJvZHVjdF9pZCI6IDEsICJxdWFudGl0eSI6IDIsICJwcmljZSI6IDEwLjQwLCAiY3VycmVuY3kiOiAiVVNEIn0=\"\n}\n</code></pre>"},{"location":"utilities/jmespath_functions/#powertools_base64_gzip-function","title":"powertools_base64_gzip function","text":"<p>Use <code>powertools_base64_gzip</code> function to decompress and decode base64 data.</p> <p>This sample will decompress and decode base64 data from Cloudwatch Logs, then use JMESPath pipeline expression to pass the result for decoding its JSON string.</p> powertools_base64_gzip_jmespath_function.pypowertools_base64_gzip_jmespath_schema.pypowertools_base64_gzip_jmespath_payload.json <pre><code>import base64\nimport binascii\nimport gzip\nimport json\n\nimport powertools_base64_gzip_jmespath_schema as schemas\nfrom jmespath.exceptions import JMESPathTypeError\n\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nfrom aws_lambda_powertools.utilities.validation import SchemaValidationError, validate\ndef lambda_handler(event, context: LambdaContext) -&gt; dict:\n    try:\nvalidate(event=event, schema=schemas.INPUT, envelope=\"powertools_base64_gzip(payload) | powertools_json(@)\")\n# Alternatively, extract_data_from_envelope works here too\n        encoded_payload = base64.b64decode(event[\"payload\"])\n        uncompressed_payload = gzip.decompress(encoded_payload).decode()\n        log: dict = json.loads(uncompressed_payload)\n\n        return {\n            \"message\": \"Logs processed\",\n            \"log_group\": log.get(\"logGroup\"),\n            \"owner\": log.get(\"owner\"),\n            \"success\": True,\n        }\n\nexcept JMESPathTypeError:\nreturn return_error_message(\"The powertools_base64_gzip() envelope function must match a valid path.\")\nexcept binascii.Error:\nreturn return_error_message(\"Payload must be a valid base64 encoded string\")\nexcept json.JSONDecodeError:\nreturn return_error_message(\"Payload must be valid JSON (base64 encoded).\")\nexcept SchemaValidationError as exception:\n# SchemaValidationError indicates where a data mismatch is\n        return return_error_message(str(exception))\n\n\ndef return_error_message(message: str) -&gt; dict:\n    return {\"message\": message, \"success\": False}\n</code></pre> <pre><code>INPUT = {\n    \"$schema\": \"http://json-schema.org/draft-07/schema\",\n    \"$id\": \"http://example.com/example.json\",\n    \"type\": \"object\",\n    \"title\": \"Sample schema\",\n    \"description\": \"The root schema comprises the entire JSON document.\",\n\"examples\": [\n{\n\"owner\": \"123456789012\",\n\"logGroup\": \"/aws/lambda/powertools-example\",\n\"logStream\": \"2022/08/07/[$LATEST]d3a8dcaffc7f4de2b8db132e3e106660\",\n\"logEvents\": {},\n}\n],\n\"required\": [\"owner\", \"logGroup\", \"logStream\", \"logEvents\"],\n\"properties\": {\n\"owner\": {\n\"$id\": \"#/properties/owner\",\n\"type\": \"string\",\n\"title\": \"The owner\",\n            \"examples\": [\"123456789012\"],\n            \"maxLength\": 12,\n        },\n\"logGroup\": {\n\"$id\": \"#/properties/logGroup\",\n\"type\": \"string\",\n\"title\": \"The logGroup\",\n            \"examples\": [\"/aws/lambda/powertools-example\"],\n            \"maxLength\": 100,\n        },\n\"logStream\": {\n\"$id\": \"#/properties/logStream\",\n\"type\": \"string\",\n\"title\": \"The logGroup\",\n            \"examples\": [\"2022/08/07/[$LATEST]d3a8dcaffc7f4de2b8db132e3e106660\"],\n            \"maxLength\": 100,\n        },\n\"logEvents\": {\n\"$id\": \"#/properties/logEvents\",\n\"type\": \"array\",\n\"title\": \"The logEvents\",\n            \"examples\": [\n                \"{'id': 'eventId1', 'message': {'username': 'lessa', 'message': 'hello world'}, 'timestamp': 1440442987000}\"  # noqa E501\n            ],\n        },\n    },\n}\n</code></pre> <pre><code>{\n\"payload\": \"H4sIACZAXl8C/52PzUrEMBhFX2UILpX8tPbHXWHqIOiq3Q1F0ubrWEiakqTWofTdTYYB0YWL2d5zvnuTFellBIOedoiyKH5M0iwnlKH7HZL6dDB6ngLDfLFYctUKjie9gHFaS/sAX1xNEq525QxwFXRGGMEkx4Th491rUZdV3YiIZ6Ljfd+lfSyAtZloacQgAkqSJCGhxM6t7cwwuUGPz4N0YKyvO6I9WDeMPMSo8Z4Ca/kJ6vMEYW5f1MX7W1lVxaG8vqX8hNFdjlc0iCBBSF4ERT/3Pl7RbMGMXF2KZMh/C+gDpNS7RRsp0OaRGzx0/t8e0jgmcczyLCWEePhni/23JWalzjdu0a3ZvgEaNLXeugEAAA==\"\n}\n</code></pre>"},{"location":"utilities/jmespath_functions/#bring-your-own-jmespath-function","title":"Bring your own JMESPath function","text":"Warning <p>This should only be used for advanced use cases where you have special formats not covered by the built-in functions.</p> <p>For special binary formats that you want to decode before applying JSON Schema validation, you can bring your own JMESPath function and any additional option via <code>jmespath_options</code> param. To keep Powertools built-in functions, you can subclass from <code>PowertoolsFunctions</code>.</p> <p>Here is an example of how to decompress messages using snappy:</p> powertools_custom_jmespath_function.pypowertools_custom_jmespath_function.json <pre><code>import base64\nimport binascii\n\nimport snappy\nfrom jmespath.exceptions import JMESPathTypeError\nfrom jmespath.functions import signature\n\nfrom aws_lambda_powertools.utilities.jmespath_utils import (\nPowertoolsFunctions,\nextract_data_from_envelope,\n)\n\n\nclass CustomFunctions(PowertoolsFunctions):\n# only decode if value is a string\n    # see supported data types: https://jmespath.org/specification.html#built-in-functions\n@signature({\"types\": [\"string\"]})\ndef _func_decode_snappy_compression(self, payload: str):\ndecoded: bytes = base64.b64decode(payload)\n        return snappy.uncompress(decoded)\n\n\ncustom_jmespath_options = {\"custom_functions\": CustomFunctions()}\ndef lambda_handler(event, context) -&gt; dict:\n\n    try:\n        logs = []\n        logs.append(\n            extract_data_from_envelope(\n                data=event,\n                # NOTE: Use the prefix `_func_` before the name of the function\nenvelope=\"Records[*].decode_snappy_compression(log)\",\njmespath_options=custom_jmespath_options,\n            )\n        )\n        return {\"logs\": logs, \"message\": \"Extracted messages\", \"success\": True}\nexcept JMESPathTypeError:\nreturn return_error_message(\"The envelope function must match a valid path.\")\nexcept snappy.UncompressError:\nreturn return_error_message(\"Log must be a valid snappy compressed binary\")\nexcept binascii.Error:\nreturn return_error_message(\"Log must be a valid base64 encoded string\")\n\n\ndef return_error_message(message: str) -&gt; dict:\n    return {\"logs\": None, \"message\": message, \"success\": False}\n</code></pre> <pre><code>{\n\"Records\": [\n{\n\"user\": \"integration-kafka\",\n\"datetime\": \"2022-01-01T00:00:00.000Z\",\n\"log\": \"/QGIMjAyMi8wNi8xNiAxNjoyNTowMCBbY3JpdF0gMzA1MTg5MCMNCPBEOiAqMSBjb25uZWN0KCkg\\ndG8gMTI3LjAuMC4xOjUwMDAgZmFpbGVkICgxMzogUGVybWlzc2lvbiBkZW5pZWQpIHdoaWxlEUEI\\naW5nAUJAdXBzdHJlYW0sIGNsaWVudDoZVKgsIHNlcnZlcjogXywgcmVxdWVzdDogIk9QVElPTlMg\\nLyBIVFRQLzEuMSIsFUckOiAiaHR0cDovLzabABQvIiwgaG8FQDAxMjcuMC4wLjE6ODEi\\n\"\n},\n{\n\"user\": \"integration-kafka\",\n\"datetime\": \"2022-01-01T00:00:01.000Z\",\n\"log\": \"tQHwnDEyNy4wLjAuMSAtIC0gWzE2L0p1bi8yMDIyOjE2OjMwOjE5ICswMTAwXSAiT1BUSU9OUyAv\\nIEhUVFAvMS4xIiAyMDQgMCAiLSIgIk1vemlsbGEvNS4wIChYMTE7IExpbnV4IHg4Nl82NCkgQXBw\\nbGVXZWJLaXQvNTM3LjM2IChLSFRNTCwgbGlrZSBHZWNrbykgQ2hyb21lLzEwMi4BmUwwIFNhZmFy\\naS81MzcuMzYiICItIg==\\n\"\n}\n]\n}\n</code></pre>"},{"location":"utilities/middleware_factory/","title":"Middleware factory","text":"<p>Middleware factory provides a decorator factory to create your own middleware to run logic before, and after each Lambda invocation synchronously.</p>"},{"location":"utilities/middleware_factory/#key-features","title":"Key features","text":"<ul> <li>Run logic before, after, and handle exceptions</li> <li>Built-in tracing opt-in capability</li> </ul>"},{"location":"utilities/middleware_factory/#getting-started","title":"Getting started","text":"Tip <p>All examples shared in this documentation are available within the project repository.</p> <p>You might need a custom middleware to abstract non-functional code. These are often custom authorization or any reusable logic you might need to run before/after a Lambda function invocation.</p>"},{"location":"utilities/middleware_factory/#middleware-with-no-params","title":"Middleware with no params","text":"<p>You can create your own middleware using <code>lambda_handler_decorator</code>. The decorator factory expects 3 arguments in your function signature:</p> <ul> <li>handler - Lambda function handler</li> <li>event - Lambda function invocation event</li> <li>context - Lambda function context object</li> </ul>"},{"location":"utilities/middleware_factory/#middleware-with-before-logic","title":"Middleware with before logic","text":"getting_started_middleware_before_logic_function.pygetting_started_middleware_before_logic_payload.json <pre><code>from dataclasses import dataclass, field\nfrom typing import Callable\nfrom uuid import uuid4\n\nfrom aws_lambda_powertools.middleware_factory import lambda_handler_decorator\nfrom aws_lambda_powertools.utilities.jmespath_utils import (\n    envelopes,\n    extract_data_from_envelope,\n)\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\n@dataclass\nclass Payment:\n    user_id: str\n    order_id: str\n    amount: float\n    status_id: str\n    payment_id: str = field(default_factory=lambda: f\"{uuid4()}\")\n\n\nclass PaymentError(Exception):\n    ...\n\n\n@lambda_handler_decorator\ndef middleware_before(handler, event, context) -&gt; Callable:\n# extract payload from a EventBridge event\n    detail: dict = extract_data_from_envelope(data=event, envelope=envelopes.EVENTBRIDGE)\n\n    # check if status_id exists in payload, otherwise add default state before processing payment\nif \"status_id\" not in detail:\nevent[\"detail\"][\"status_id\"] = \"pending\"\nresponse = handler(event, context)\nreturn response\n\n\n@middleware_before\ndef lambda_handler(event, context: LambdaContext) -&gt; dict:\ntry:\n        payment_payload: dict = extract_data_from_envelope(data=event, envelope=envelopes.EVENTBRIDGE)\n        return {\n            \"order\": Payment(**payment_payload).__dict__,\n            \"message\": \"payment created\",\n            \"success\": True,\n        }\n    except Exception as e:\n        raise PaymentError(\"Unable to create payment\") from e\n</code></pre> <pre><code>{\n\"version\": \"0\",\n\"id\": \"9c95e8e4-96a4-ef3f-b739-b6aa5b193afb\",\n\"detail-type\": \"PaymentCreated\",\n\"source\": \"app.payment\",\n\"account\": \"0123456789012\",\n\"time\": \"2022-08-08T20:41:53Z\",\n\"region\": \"eu-east-1\",\n\"detail\": {\n\"amount\": \"150.00\",\n\"order_id\": \"8f1f1710-1b30-48a5-a6bd-153fd23b866b\",\n\"user_id\": \"f80e3c51-5b8c-49d5-af7d-c7804966235f\"\n}\n}\n</code></pre>"},{"location":"utilities/middleware_factory/#middleware-with-after-logic","title":"Middleware with after logic","text":"getting_started_middleware_after_logic_function.pygetting_started_middleware_after_logic_payload.json <pre><code>import time\nfrom typing import Callable\n\nimport requests\nfrom requests import Response\n\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.middleware_factory import lambda_handler_decorator\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\napp = APIGatewayRestResolver()\n\n\n@lambda_handler_decorator\ndef middleware_after(handler, event, context) -&gt; Callable:\nstart_time = time.time()\n    response = handler(event, context)\n    execution_time = time.time() - start_time\n\n# adding custom headers in response object after lambda executing\nresponse[\"headers\"][\"execution_time\"] = execution_time\nresponse[\"headers\"][\"aws_request_id\"] = context.aws_request_id\nreturn response\n\n\n@app.post(\"/todos\")\ndef create_todo() -&gt; dict:\n    todo_data: dict = app.current_event.json_body  # deserialize json str to dict\n    todo: Response = requests.post(\"https://jsonplaceholder.typicode.com/todos\", data=todo_data)\n    todo.raise_for_status()\n\n    return {\"todo\": todo.json()}\n\n\n@middleware_after\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> <pre><code>{\n\"resource\": \"/todos\",\n\"path\": \"/todos\",\n\"httpMethod\": \"POST\",\n\"body\": \"{\\\"title\\\": \\\"foo\\\", \\\"userId\\\": 1, \\\"completed\\\": false}\"\n}\n</code></pre>"},{"location":"utilities/middleware_factory/#middleware-with-params","title":"Middleware with params","text":"<p>You can also have your own keyword arguments after the mandatory arguments.</p> getting_started_middleware_with_params_function.pygetting_started_middleware_with_params_payload.json <pre><code>import base64\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, List\nfrom uuid import uuid4\n\nfrom aws_lambda_powertools.middleware_factory import lambda_handler_decorator\nfrom aws_lambda_powertools.utilities.jmespath_utils import (\n    envelopes,\n    extract_data_from_envelope,\n)\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\n@dataclass\nclass Booking:\n    days: int\n    date_from: str\n    date_to: str\n    hotel_id: int\n    country: str\n    city: str\n    guest: dict\n    booking_id: str = field(default_factory=lambda: f\"{uuid4()}\")\n\n\nclass BookingError(Exception):\n    ...\n\n\n@lambda_handler_decorator\ndef obfuscate_sensitive_data(handler, event, context, fields: List) -&gt; Callable:\n# extracting payload from a EventBridge event\ndetail: dict = extract_data_from_envelope(data=event, envelope=envelopes.EVENTBRIDGE)\n    guest_data: Any = detail.get(\"guest\")\n\n# Obfuscate fields (email, vat, passport) before calling Lambda handler\nfor guest_field in fields:\n        if guest_data.get(guest_field):\n            event[\"detail\"][\"guest\"][guest_field] = obfuscate_data(str(guest_data.get(guest_field)))\n\n    response = handler(event, context)\n\n    return response\n\n\ndef obfuscate_data(value: str) -&gt; bytes:\n    # base64 is not effective for obfuscation, this is an example\n    return base64.b64encode(value.encode(\"ascii\"))\n\n\n@obfuscate_sensitive_data(fields=[\"email\", \"passport\", \"vat\"])\ndef lambda_handler(event, context: LambdaContext) -&gt; dict:\ntry:\n        booking_payload: dict = extract_data_from_envelope(data=event, envelope=envelopes.EVENTBRIDGE)\n        return {\n            \"book\": Booking(**booking_payload).__dict__,\n            \"message\": \"booking created\",\n            \"success\": True,\n        }\n    except Exception as e:\n        raise BookingError(\"Unable to create booking\") from e\n</code></pre> <pre><code>{\n\"version\": \"0\",\n\"id\": \"9c95e8e4-96a4-ef3f-b739-b6aa5b193afb\",\n\"detail-type\": \"BookingCreated\",\n\"source\": \"app.booking\",\n\"account\": \"0123456789012\",\n\"time\": \"2022-08-08T20:41:53Z\",\n\"region\": \"eu-east-1\",\n\"detail\": {\n\"days\": 5,\n\"date_from\": \"2020-08-08\",\n\"date_to\": \"2020-08-13\",\n\"hotel_id\": \"1\",\n\"country\": \"Portugal\",\n\"city\": \"Lisbon\",\n\"guest\": {\n\"name\": \"Lambda\",\n\"email\": \"lambda@powertool.tools\",\n\"passport\": \"AA123456\",\n\"vat\": \"123456789\"\n}\n}\n}\n</code></pre>"},{"location":"utilities/middleware_factory/#advanced","title":"Advanced","text":"<p>For advanced use cases, you can instantiate Tracer inside your middleware, and add annotations as well as metadata for additional operational insights.</p> advanced_middleware_tracer_function.pyadvanced_middleware_tracer_payload.json <pre><code>import time\nfrom typing import Callable\n\nimport requests\nfrom requests import Response\n\nfrom aws_lambda_powertools import Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.middleware_factory import lambda_handler_decorator\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ntracer = Tracer()\napp = APIGatewayRestResolver()\n\n\n@lambda_handler_decorator(trace_execution=True)\ndef middleware_with_advanced_tracing(handler, event, context) -&gt; Callable:\ntracer.put_metadata(key=\"resource\", value=event.get(\"resource\"))\nstart_time = time.time()\n    response = handler(event, context)\n    execution_time = time.time() - start_time\n\ntracer.put_annotation(key=\"TotalExecutionTime\", value=str(execution_time))\n# adding custom headers in response object after lambda executing\n    response[\"headers\"][\"execution_time\"] = execution_time\n    response[\"headers\"][\"aws_request_id\"] = context.aws_request_id\n\n    return response\n\n\n@app.get(\"/products\")\ndef create_product() -&gt; dict:\n    product: Response = requests.get(\"https://dummyjson.com/products/1\")\n    product.raise_for_status()\n\n    return {\"product\": product.json()}\n\n\n@middleware_with_advanced_tracing\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> <pre><code>{\n\"resource\": \"/products\",\n\"path\": \"/products\",\n\"httpMethod\": \"GET\"\n}\n</code></pre> <p></p>"},{"location":"utilities/middleware_factory/#tracing-middleware-execution","title":"Tracing middleware execution","text":"<p>If you are making use of Tracer, you can trace the execution of your middleware to ease operations.</p> <p>This makes use of an existing Tracer instance that you may have initialized anywhere in your code.</p> Warning <p>You must enable Active Tracing in your Lambda function when using this feature, otherwise Lambda cannot send traces to XRay.</p> getting_started_middleware_tracer_function.pygetting_started_middleware_tracer_payload.json <pre><code>import time\nfrom typing import Callable\n\nimport requests\nfrom requests import Response\n\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.middleware_factory import lambda_handler_decorator\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\napp = APIGatewayRestResolver()\n\n\n@lambda_handler_decorator(trace_execution=True)\ndef middleware_with_tracing(handler, event, context) -&gt; Callable:\nstart_time = time.time()\n    response = handler(event, context)\n    execution_time = time.time() - start_time\n\n    # adding custom headers in response object after lambda executing\n    response[\"headers\"][\"execution_time\"] = execution_time\n    response[\"headers\"][\"aws_request_id\"] = context.aws_request_id\n\n    return response\n\n\n@app.get(\"/products\")\ndef create_product() -&gt; dict:\n    product: Response = requests.get(\"https://dummyjson.com/products/1\")\n    product.raise_for_status()\n\n    return {\"product\": product.json()}\n\n\n@middleware_with_tracing\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> <pre><code>{\n\"resource\": \"/products\",\n\"path\": \"/products\",\n\"httpMethod\": \"GET\"\n}\n</code></pre> <p>When executed, your middleware name will appear in AWS X-Ray Trace details as <code>## middleware_name</code>, in this example the middleware name is <code>## middleware_with_tracing</code>.</p> <p></p>"},{"location":"utilities/middleware_factory/#combining-powertools-utilities","title":"Combining Powertools utilities","text":"<p>You can create your own middleware and combine many features of Lambda Powertools such as trace, logs, feature flags, validation, jmespath_functions and others to abstract non-functional code.</p> <p>In the example below, we create a Middleware with the following features:</p> <ul> <li>Logs and traces</li> <li>Validate if the payload contains a specific header</li> <li>Extract specific keys from event</li> <li>Automatically add security headers on every execution</li> <li>Validate if a specific feature flag is enabled</li> <li>Save execution history to a DynamoDB table</li> </ul> combining_powertools_utilities_function.pycombining_powertools_utilities_schema.pycombining_powertools_utilities_event.jsonSAM TEMPLATE <pre><code>import json\nfrom typing import Callable\n\nimport boto3\nimport combining_powertools_utilities_schema as schemas\nimport requests\n\nfrom aws_lambda_powertools import Logger, Tracer\nfrom aws_lambda_powertools.event_handler import APIGatewayRestResolver\nfrom aws_lambda_powertools.event_handler.exceptions import InternalServerError\nfrom aws_lambda_powertools.middleware_factory import lambda_handler_decorator\nfrom aws_lambda_powertools.shared.types import JSONType\nfrom aws_lambda_powertools.utilities.feature_flags import AppConfigStore, FeatureFlags\nfrom aws_lambda_powertools.utilities.jmespath_utils import extract_data_from_envelope\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nfrom aws_lambda_powertools.utilities.validation import SchemaValidationError, validate\n\napp = APIGatewayRestResolver()\ntracer = Tracer()\nlogger = Logger()\n\ntable_historic = boto3.resource(\"dynamodb\").Table(\"HistoricTable\")\n\napp_config = AppConfigStore(environment=\"dev\", application=\"comments\", name=\"features\")\nfeature_flags = FeatureFlags(store=app_config)\n\n\n@lambda_handler_decorator(trace_execution=True)\ndef middleware_custom(handler: Callable, event: dict, context: LambdaContext):\n# validating the INPUT with the given schema\n    # X-Customer-Id header must be informed in all requests\n    try:\n        validate(event=event, schema=schemas.INPUT)\n    except SchemaValidationError as e:\n        return {\n            \"statusCode\": 400,\n            \"body\": json.dumps(str(e)),\n        }\n\n    # extracting headers and requestContext from event\n    headers = extract_data_from_envelope(data=event, envelope=\"headers\")\n    request_context = extract_data_from_envelope(data=event, envelope=\"requestContext\")\n\n    logger.debug(f\"X-Customer-Id =&gt; {headers.get('X-Customer-Id')}\")\n    tracer.put_annotation(key=\"CustomerId\", value=headers.get(\"X-Customer-Id\"))\n\n    response = handler(event, context)\n\n    # automatically adding security headers to all responses\n    # see: https://securityheaders.com/\nlogger.info(\"Injecting security headers\")\nresponse[\"headers\"][\"Referrer-Policy\"] = \"no-referrer\"\n    response[\"headers\"][\"Strict-Transport-Security\"] = \"max-age=15552000; includeSubDomains; preload\"\n    response[\"headers\"][\"X-DNS-Prefetch-Control\"] = \"off\"\n    response[\"headers\"][\"X-Content-Type-Options\"] = \"nosniff\"\n    response[\"headers\"][\"X-Permitted-Cross-Domain-Policies\"] = \"none\"\n    response[\"headers\"][\"X-Download-Options\"] = \"noopen\"\n\n    logger.info(\"Saving api call in history table\")\nsave_api_execution_history(str(event.get(\"path\")), headers, request_context)\n# return lambda execution\n    return response\n\n\n@tracer.capture_method\ndef save_api_execution_history(path: str, headers: dict, request_context: dict) -&gt; None:\n\n    try:\n        # using the feature flags utility to check if the new feature \"save api call to history\" is enabled by default\n        # see: https://awslabs.github.io/aws-lambda-powertools-python/latest/utilities/feature_flags/#static-flags\nsave_history: JSONType = feature_flags.evaluate(name=\"save_history\", default=False)\nif save_history:\n            # saving history in dynamodb table\n            tracer.put_metadata(key=\"execution detail\", value=request_context)\n            table_historic.put_item(\n                Item={\n                    \"customer_id\": headers.get(\"X-Customer-Id\"),\n                    \"request_id\": request_context.get(\"requestId\"),\n                    \"path\": path,\n                    \"request_time\": request_context.get(\"requestTime\"),\n                    \"source_ip\": request_context.get(\"identity\", {}).get(\"sourceIp\"),\n                    \"http_method\": request_context.get(\"httpMethod\"),\n                }\n            )\n\n        return None\n    except Exception:\n        # you can add more logic here to handle exceptions or even save this to a DLQ\n        # but not to make this example too long, we just return None since the Lambda has been successfully executed\n        return None\n\n\n@app.get(\"/comments\")\n@tracer.capture_method\ndef get_comments():\n    try:\n        comments: requests.Response = requests.get(\"https://jsonplaceholder.typicode.com/comments\")\n        comments.raise_for_status()\n\n        return {\"comments\": comments.json()[:10]}\n    except Exception as exc:\n        raise InternalServerError(str(exc))\n\n\n@app.get(\"/comments/&lt;comment_id&gt;\")\n@tracer.capture_method\ndef get_comments_by_id(comment_id: str):\n    try:\n        comments: requests.Response = requests.get(f\"https://jsonplaceholder.typicode.com/comments/{comment_id}\")\n        comments.raise_for_status()\n\n        return {\"comments\": comments.json()}\n    except Exception as exc:\n        raise InternalServerError(str(exc))\n\n\n@middleware_custom\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(event, context)\n</code></pre> <pre><code>INPUT = {\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"$id\": \"https://example.com/object1661012141.json\",\n    \"title\": \"Root\",\n    \"type\": \"object\",\n    \"required\": [\"headers\"],\n    \"properties\": {\n        \"headers\": {\n            \"$id\": \"#root/headers\",\n            \"title\": \"Headers\",\n            \"type\": \"object\",\n\"required\": [\"X-Customer-Id\"],\n\"properties\": {\n\"X-Customer-Id\": {\n\"$id\": \"#root/headers/X-Customer-Id\",\n                    \"title\": \"X-customer-id\",\n                    \"type\": \"string\",\n                    \"default\": \"\",\n                    \"examples\": [\"1\"],\n                    \"pattern\": \"^.*$\",\n                }\n            },\n        }\n    },\n}\n</code></pre> <pre><code>{\n    \"body\":\"None\",\n    \"headers\":{\n       \"Accept\":\"*/*\",\n       \"Accept-Encoding\":\"gzip, deflate, br\",\n       \"Connection\":\"keep-alive\",\n       \"Host\":\"127.0.0.1:3001\",\n       \"Postman-Token\":\"a9d49365-ebe1-4bb0-8627-d5e37cdce86d\",\n       \"User-Agent\":\"PostmanRuntime/7.29.0\",\n\"X-Customer-Id\":\"1\",\n\"X-Forwarded-Port\":\"3001\",\n       \"X-Forwarded-Proto\":\"http\"\n    },\n    \"httpMethod\":\"GET\",\n    \"isBase64Encoded\":false,\n    \"multiValueHeaders\":{\n       \"Accept\":[\n          \"*/*\"\n       ],\n       \"Accept-Encoding\":[\n          \"gzip, deflate, br\"\n       ],\n       \"Connection\":[\n          \"keep-alive\"\n       ],\n       \"Host\":[\n          \"127.0.0.1:3001\"\n       ],\n       \"Postman-Token\":[\n          \"a9d49365-ebe1-4bb0-8627-d5e37cdce86d\"\n       ],\n       \"User-Agent\":[\n          \"PostmanRuntime/7.29.0\"\n       ],\n       \"X-Customer-Id\":[\n          \"1\"\n       ],\n       \"X-Forwarded-Port\":[\n          \"3001\"\n       ],\n       \"X-Forwarded-Proto\":[\n          \"http\"\n       ]\n    },\n    \"multiValueQueryStringParameters\":\"None\",\n    \"path\":\"/comments\",\n    \"pathParameters\":\"None\",\n    \"queryStringParameters\":\"None\",\n    \"requestContext\":{\n       \"accountId\":\"123456789012\",\n       \"apiId\":\"1234567890\",\n       \"domainName\":\"127.0.0.1:3001\",\n       \"extendedRequestId\":\"None\",\n       \"httpMethod\":\"GET\",\n       \"identity\":{\n          \"accountId\":\"None\",\n          \"apiKey\":\"None\",\n          \"caller\":\"None\",\n          \"cognitoAuthenticationProvider\":\"None\",\n          \"cognitoAuthenticationType\":\"None\",\n          \"cognitoIdentityPoolId\":\"None\",\n          \"sourceIp\":\"127.0.0.1\",\n          \"user\":\"None\",\n          \"userAgent\":\"Custom User Agent String\",\n          \"userArn\":\"None\"\n       },\n       \"path\":\"/comments\",\n       \"protocol\":\"HTTP/1.1\",\n       \"requestId\":\"56d1a102-6d9d-4f13-b4f7-26751c10a131\",\n       \"requestTime\":\"20/Aug/2022:18:18:58 +0000\",\n       \"requestTimeEpoch\":1661019538,\n       \"resourceId\":\"123456\",\n       \"resourcePath\":\"/comments\",\n       \"stage\":\"Prod\"\n    },\n    \"resource\":\"/comments\",\n    \"stageVariables\":\"None\",\n    \"version\":\"1.0\"\n }\n</code></pre> <pre><code>AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: Middleware-powertools-utilities example\n\nGlobals:\n  Function:\n    Timeout: 5\n    Runtime: python3.9\n    Tracing: Active\n    Architectures:\n      - x86_64\n    Environment:\n      Variables:\n        LOG_LEVEL: DEBUG\n        POWERTOOLS_LOGGER_SAMPLE_RATE: 0.1\n        POWERTOOLS_LOGGER_LOG_EVENT: true\n        POWERTOOLS_SERVICE_NAME: middleware\n\nResources:\n  MiddlewareFunction:\n    Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction\n    Properties:\n      CodeUri: middleware/\n      Handler: app.lambda_handler\n      Description: Middleware function\n      Policies:\n      - AWSLambdaBasicExecutionRole # Managed Policy\n      - Version: '2012-10-17' # Policy Document\n        Statement:\n          - Effect: Allow\n            Action:\n              - dynamodb:PutItem\n            Resource: !GetAtt HistoryTable.Arn\n          - Effect: Allow\n            Action: # https://docs.aws.amazon.com/appconfig/latest/userguide/getting-started-with-appconfig-permissions.html\n              - ssm:GetDocument\n              - ssm:ListDocuments\n              - appconfig:GetLatestConfiguration\n              - appconfig:StartConfigurationSession\n              - appconfig:ListApplications\n              - appconfig:GetApplication\n              - appconfig:ListEnvironments\n              - appconfig:GetEnvironment\n              - appconfig:ListConfigurationProfiles\n              - appconfig:GetConfigurationProfile\n              - appconfig:ListDeploymentStrategies\n              - appconfig:GetDeploymentStrategy\n              - appconfig:GetConfiguration\n              - appconfig:ListDeployments\n              - appconfig:GetDeployment\n            Resource: \"*\"\n      Events:\n        GetComments:\n          Type: Api\n          Properties:\n            Path: /comments\n            Method: GET\n        GetCommentsById:\n          Type: Api\n          Properties:\n            Path: /comments/{comment_id}\n            Method: GET\n\n  # DynamoDB table to store historical data\n  HistoryTable:\nType: AWS::DynamoDB::Table\nProperties:\n      TableName: \"HistoryTable\"\n      AttributeDefinitions:\n        - AttributeName: customer_id\n          AttributeType: S\n        - AttributeName: request_id\n          AttributeType: S\n      KeySchema:\n        - AttributeName: customer_id\n          KeyType: HASH\n        - AttributeName: request_id\n          KeyType: \"RANGE\"\n      BillingMode: PAY_PER_REQUEST\n\n  # Feature flags using AppConfig\n  FeatureCommentApp:\nType: AWS::AppConfig::Application\nProperties:\n      Description: \"Comments Application for feature toggles\"\n      Name: comments\n\n  FeatureCommentDevEnv:\nType: AWS::AppConfig::Environment\nProperties:\n      ApplicationId: !Ref FeatureCommentApp\n      Description: \"Development Environment for the App Config Comments\"\n      Name: dev\n\n  FeatureCommentConfigProfile:\nType: AWS::AppConfig::ConfigurationProfile\nProperties:\n      ApplicationId: !Ref FeatureCommentApp\n      Name: features\n      LocationUri: \"hosted\"\n\n  HostedConfigVersion:\nType: AWS::AppConfig::HostedConfigurationVersion\nProperties:\n      ApplicationId: !Ref FeatureCommentApp\n      ConfigurationProfileId: !Ref FeatureCommentConfigProfile\n      Description: 'A sample hosted configuration version'\nContent: |\n{\n\"save_history\": {\n\"default\": true\n}\n}\nContentType: 'application/json'\n\n  # this is just an example\n  # change this values according your deployment strategy\n  BasicDeploymentStrategy:\nType: AWS::AppConfig::DeploymentStrategy\nProperties:\n      Name: \"Deployment\"\n      Description: \"Deployment strategy for comments app.\"\n      DeploymentDurationInMinutes: 1\n      FinalBakeTimeInMinutes: 1\n      GrowthFactor: 100\n      GrowthType: LINEAR\n      ReplicateTo: NONE\n\n  ConfigDeployment:\nType: AWS::AppConfig::Deployment\nProperties:\n      ApplicationId: !Ref FeatureCommentApp\n      ConfigurationProfileId: !Ref FeatureCommentConfigProfile\n      ConfigurationVersion: !Ref HostedConfigVersion\n      DeploymentStrategyId: !Ref BasicDeploymentStrategy\n      EnvironmentId: !Ref FeatureCommentDevEnv\n</code></pre>"},{"location":"utilities/middleware_factory/#tips","title":"Tips","text":"<ul> <li>Use <code>trace_execution</code> to quickly understand the performance impact of your middlewares, and reduce or merge tasks when necessary</li> <li>When nesting multiple middlewares, always return the handler with event and context, or response</li> <li>Keep in mind Python decorators execution order. Lambda handler is actually called once (top-down)</li> <li>Async middlewares are not supported</li> </ul>"},{"location":"utilities/parameters/","title":"Parameters","text":"<p>The parameters utility provides high-level functions to retrieve one or multiple parameter values from AWS Systems Manager Parameter Store, AWS Secrets Manager, AWS AppConfig, Amazon DynamoDB, or bring your own.</p>"},{"location":"utilities/parameters/#key-features","title":"Key features","text":"<ul> <li>Retrieve one or multiple parameters from the underlying provider</li> <li>Cache parameter values for a given amount of time (defaults to 5 seconds)</li> <li>Transform parameter values from JSON or base 64 encoded strings</li> <li>Bring Your Own Parameter Store Provider</li> </ul>"},{"location":"utilities/parameters/#getting-started","title":"Getting started","text":"Tip <p>All examples shared in this documentation are available within the project repository.</p> <p>By default, we fetch parameters from System Manager Parameter Store, secrets from Secrets Manager, and application configuration from AppConfig.</p>"},{"location":"utilities/parameters/#iam-permissions","title":"IAM Permissions","text":"<p>This utility requires additional permissions to work as expected.</p> Note <p>Different parameter providers require different permissions.</p> Provider Function/Method IAM Permission SSM <code>get_parameter</code>, <code>SSMProvider.get</code> <code>ssm:GetParameter</code> SSM <code>get_parameters</code>, <code>SSMProvider.get_multiple</code> <code>ssm:GetParametersByPath</code> SSM <code>get_parameters_by_name</code>, <code>SSMProvider.get_parameters_by_name</code> <code>ssm:GetParameter</code> and <code>ssm:GetParameters</code> SSM If using <code>decrypt=True</code> You must add an additional permission <code>kms:Decrypt</code> Secrets <code>get_secret</code>, <code>SecretsProvider.get</code> <code>secretsmanager:GetSecretValue</code> DynamoDB <code>DynamoDBProvider.get</code> <code>dynamodb:GetItem</code> DynamoDB <code>DynamoDBProvider.get_multiple</code> <code>dynamodb:Query</code> AppConfig <code>get_app_config</code>, <code>AppConfigProvider.get_app_config</code> <code>appconfig:GetLatestConfiguration</code> and <code>appconfig:StartConfigurationSession</code>"},{"location":"utilities/parameters/#fetching-parameters","title":"Fetching parameters","text":"<p>You can retrieve a single parameter using the <code>get_parameter</code> high-level function.</p> getting_started_single_ssm_parameter.py <pre><code>import requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    try:\n        # Retrieve a single parameter\nendpoint_comments: str = parameters.get_parameter(\"/lambda-powertools/endpoint_comments\")  # type: ignore[assignment] # noqa: E501\n# the value of this parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <p>For multiple parameters, you can use either:</p> <ul> <li><code>get_parameters</code> to recursively fetch all parameters by path.</li> <li><code>get_parameters_by_name</code> to fetch distinct parameters by their full name. It also accepts custom caching, transform, decrypt per parameter.</li> </ul> getting_started_recursive_ssm_parameter.pygetting_started_parameter_by_name.py <pre><code>import requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve multiple parameters from a path prefix\nall_parameters: dict = parameters.get_parameters(\"/lambda-powertools/\", max_age=20)\nendpoint_comments = None\n\nfor parameter, value in all_parameters.items():\nif parameter == \"endpoint_comments\":\n                endpoint_comments = value\n\n        if endpoint_comments is None:\n            return {\"comments\": None}\n\n        # the value of parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10]}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>from typing import Any\n\nfrom aws_lambda_powertools.utilities.parameters.ssm import get_parameters_by_name\nparameters = {\n    \"/develop/service/commons/telemetry/config\": {\"max_age\": 300, \"transform\": \"json\"},\n    \"/no_cache_param\": {\"max_age\": 0},\n    # inherit default values\n    \"/develop/service/payment/api/capture/url\": {},\n}\n\n\ndef handler(event, context):\n# This returns a dict with the parameter name as key\nresponse: dict[str, Any] = get_parameters_by_name(parameters=parameters, max_age=60)\n    for parameter, value in response.items():\n        print(f\"{parameter}: {value}\")\n</code></pre> <code>get_parameters_by_name</code> supports graceful error handling <p>By default, we will raise <code>GetParameterError</code> when any parameter fails to be fetched. You can override it by setting <code>raise_on_error=False</code>.</p> <p>When disabled, we take the following actions:</p> <ul> <li>Add failed parameter name in the <code>_errors</code> key, e.g., <code>{_errors: [\"/param1\", \"/param2\"]}</code></li> <li>Keep only successful parameter names and their values in the response</li> <li>Raise <code>GetParameterError</code> if any of your parameters is named <code>_errors</code></li> </ul> get_parameter_by_name_error_handling.py <pre><code>from typing import Any\n\nfrom aws_lambda_powertools.utilities.parameters.ssm import get_parameters_by_name\nparameters = {\n\"/develop/service/commons/telemetry/config\": {\"max_age\": 300, \"transform\": \"json\"},\n    # it would fail by default\n    \"/this/param/does/not/exist\": {},\n}\n\n\ndef handler(event, context):\nvalues: dict[str, Any] = get_parameters_by_name(parameters=parameters, raise_on_error=False)\nerrors: list[str] = values.get(\"_errors\", [])\n# Handle gracefully, since '/this/param/does/not/exist' will only be available in `_errors`\n    if errors:\n        ...\n\n    for parameter, value in values.items():\n        print(f\"{parameter}: {value}\")\n</code></pre>"},{"location":"utilities/parameters/#fetching-secrets","title":"Fetching secrets","text":"<p>You can fetch secrets stored in Secrets Manager using <code>get_secret</code>.</p> getting_started_secret.py <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n\n    try:\n        # Usually an endpoint is not sensitive data, so we store it in SSM Parameters\n        endpoint_comments: Any = parameters.get_parameter(\"/lambda-powertools/endpoint_comments\")\n        # An API-KEY is a sensitive data and should be stored in SecretsManager\napi_key: Any = parameters.get_secret(\"/lambda-powertools/api-key\")\nheaders: dict = {\"X-API-Key\": api_key}\n\n        comments: requests.Response = requests.get(endpoint_comments, headers=headers)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre>"},{"location":"utilities/parameters/#fetching-app-configurations","title":"Fetching app configurations","text":"<p>You can fetch application configurations in AWS AppConfig using <code>get_app_config</code>.</p> <p>The following will retrieve the latest version and store it in the cache.</p> getting_started_appconfig.py <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve a single parameter\nendpoint_comments: Any = parameters.get_app_config(name=\"config\", environment=\"dev\", application=\"comments\")\n# the value of this parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre>"},{"location":"utilities/parameters/#advanced","title":"Advanced","text":""},{"location":"utilities/parameters/#adjusting-cache-ttl","title":"Adjusting cache TTL","text":"Tip <p><code>max_age</code> parameter is also available in underlying provider functions like <code>get()</code>, <code>get_multiple()</code>, etc.</p> <p>By default, we cache parameters retrieved in-memory for 5 seconds. If you want to change this default value and set the same TTL for all parameters, you can set the <code>POWERTOOLS_PARAMETERS_MAX_AGE</code> environment variable. You can still set <code>max_age</code> for individual parameters.</p> <p>You can adjust how long we should keep values in cache by using the param <code>max_age</code>, when using  <code>get_parameter()</code>, <code>get_parameters()</code> and <code>get_secret()</code> methods across all providers.</p> single_ssm_parameter_with_cache.pyrecursive_ssm_parameter_with_cache.pysecret_with_cache.pyappconfig_with_cache.py <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve a single parameter with 20s cache\nendpoint_comments: Any = parameters.get_parameter(\"/lambda-powertools/endpoint_comments\", max_age=20)\n# the value of this parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve multiple parameters from a path prefix\nall_parameters: Any = parameters.get_parameters(\"/lambda-powertools/\", max_age=20)\nendpoint_comments = \"https://jsonplaceholder.typicode.com/noexists/\"\n\n        for parameter, value in all_parameters.items():\n\n            if parameter == \"endpoint_comments\":\n                endpoint_comments = value\n\n        # the value of parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10]}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n\n    try:\n        # Usually an endpoint is not sensitive data, so we store it in SSM Parameters\n        endpoint_comments: Any = parameters.get_parameter(\"/lambda-powertools/endpoint_comments\")\n        # An API-KEY is a sensitive data and should be stored in SecretsManager\napi_key: Any = parameters.get_secret(\"/lambda-powertools/api-key\", max_age=20)\nheaders: dict = {\"X-API-Key\": api_key}\n\n        comments: requests.Response = requests.get(endpoint_comments, headers=headers)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve a single parameter\nendpoint_comments: Any = parameters.get_app_config(\nname=\"config\", environment=\"dev\", application=\"comments\", max_age=20\n)\n# the value of this parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre>"},{"location":"utilities/parameters/#always-fetching-the-latest","title":"Always fetching the latest","text":"<p>If you'd like to always ensure you fetch the latest parameter from the store regardless if already available in cache, use <code>force_fetch</code> param.</p> single_ssm_parameter_force_fetch.pyrecursive_ssm_parameter_force_fetch.pysecret_force_fetch.pyappconfig_force_fetch.py <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve a single parameter with 20s cache\nendpoint_comments: Any = parameters.get_parameter(\"/lambda-powertools/endpoint_comments\", force_fetch=True)\n# the value of this parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve multiple parameters from a path prefix\nall_parameters: Any = parameters.get_parameters(\"/lambda-powertools/\", force_fetch=True)\nendpoint_comments = \"https://jsonplaceholder.typicode.com/noexists/\"\n\n        for parameter, value in all_parameters.items():\n\n            if parameter == \"endpoint_comments\":\n                endpoint_comments = value\n\n        # the value of parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10]}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n\n    try:\n        # Usually an endpoint is not sensitive data, so we store it in SSM Parameters\n        endpoint_comments: Any = parameters.get_parameter(\"/lambda-powertools/endpoint_comments\")\n        # An API-KEY is a sensitive data and should be stored in SecretsManager\napi_key: Any = parameters.get_secret(\"/lambda-powertools/api-key\", force_fetch=True)\nheaders: dict = {\"X-API-Key\": api_key}\n\n        comments: requests.Response = requests.get(endpoint_comments, headers=headers)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve a single parameter\nendpoint_comments: Any = parameters.get_app_config(\nname=\"config\", environment=\"dev\", application=\"comments\", force_fetch=True\n)\n# the value of this parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre>"},{"location":"utilities/parameters/#built-in-provider-class","title":"Built-in provider class","text":"<p>For greater flexibility such as configuring the underlying SDK client used by built-in providers, you can use their respective Provider Classes directly.</p> Tip <p>This is useful when you need to customize parameters for the SDK client, such as region, credentials, retries and others. For more information, read botocore.config and boto3.session.</p>"},{"location":"utilities/parameters/#ssmprovider","title":"SSMProvider","text":"builtin_provider_ssm_single_parameter.pybuiltin_provider_ssm_recursive_parameter.py <pre><code>from typing import Any\n\nimport requests\nfrom botocore.config import Config\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n# changing region_name, connect_timeout and retrie configurations\n# see: https://botocore.amazonaws.com/v1/documentation/api/latest/reference/config.html\nconfig = Config(region_name=\"sa-east-1\", connect_timeout=1, retries={\"total_max_attempts\": 2, \"max_attempts\": 5})\nssm_provider = parameters.SSMProvider(config=config)\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve a single parameter\n        endpoint_comments: Any = ssm_provider.get(\"/lambda-powertools/endpoint_comments\")\n\n        # the value of this parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>from typing import Any\n\nimport boto3\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n# assuming role from another account to get parameter there\n# see: https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\nsts_client = boto3.client(\"sts\")\nassumed_role_object = sts_client.assume_role(\n    RoleArn=\"arn:aws:iam::account-of-role-to-assume:role/name-of-role\", RoleSessionName=\"RoleAssume1\"\n)\ncredentials = assumed_role_object[\"Credentials\"]\n\n# using temporary credentials in your SSMProvider provider\n# see: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html#module-boto3.session\nboto3_session = boto3.session.Session(\nregion_name=\"us-east-1\",\naws_access_key_id=credentials[\"AccessKeyId\"],\naws_secret_access_key=credentials[\"SecretAccessKey\"],\naws_session_token=credentials[\"SessionToken\"],\n)\nssm_provider = parameters.SSMProvider(boto3_session=boto3_session)\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve multiple parameters from a path prefix\n        all_parameters: Any = ssm_provider.get_multiple(\"/lambda-powertools/\")\n        endpoint_comments = \"https://jsonplaceholder.typicode.com/noexists/\"\n\n        for parameter, value in all_parameters.items():\n\n            if parameter == \"endpoint_comments\":\n                endpoint_comments = value\n\n        # the value of parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10]}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <p>The AWS Systems Manager Parameter Store provider supports two additional arguments for the <code>get()</code> and <code>get_multiple()</code> methods:</p> Parameter Default Description decrypt <code>False</code> Will automatically decrypt the parameter. recursive <code>True</code> For <code>get_multiple()</code> only, will fetch all parameter values recursively based on a path prefix. <p>You can create <code>SecureString</code> parameters, which are parameters that have a plaintext parameter name and an encrypted parameter value. If you don't use the <code>decrypt</code> argument, you will get an encrypted value. Read here about best practices using KMS to secure your parameters.</p> Tip <p>If you want to always decrypt parameters, you can set the <code>POWERTOOLS_PARAMETERS_SSM_DECRYPT=true</code> environment variable. This will override the default value of <code>false</code> but you can still set the <code>decrypt</code> option for individual parameters.</p> builtin_provider_ssm_with_decrypt.pybuiltin_provider_ssm_with_no_recursive.py <pre><code>from typing import Any\nfrom uuid import uuid4\n\nimport boto3\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nec2 = boto3.resource(\"ec2\")\nssm_provider = parameters.SSMProvider()\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve the key pair from secure string parameter\nec2_pem: Any = ssm_provider.get(\"/lambda-powertools/ec2_pem\", decrypt=True)\nname_key_pair = f\"kp_{uuid4()}\"\n\n        ec2.import_key_pair(KeyName=name_key_pair, PublicKeyMaterial=ec2_pem)\n\n        ec2.create_instances(\n            ImageId=\"ami-026b57f3c383c2eec\", InstanceType=\"t2.micro\", MinCount=1, MaxCount=1, KeyName=name_key_pair\n        )\n\n        return {\"message\": \"EC2 created\", \"success\": True}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"message\": f\"Error creating EC2 =&gt; {str(error)}\", \"success\": False}\n</code></pre> <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nssm_provider = parameters.SSMProvider()\nclass ConfigNotFound(Exception):\n    ...\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve multiple parameters from a path prefix\n        # /config = root\n        # /config/endpoint = url\n        # /config/endpoint/query = querystring\nall_parameters: Any = ssm_provider.get_multiple(\"/config\", recursive=False)\nendpoint_comments = \"https://jsonplaceholder.typicode.com/comments/\"\n\n        for parameter, value in all_parameters.items():\n\n            # query parameter is used to query endpoint\n            if \"query\" in parameter:\n                endpoint_comments = f\"{endpoint_comments}{value}\"\n                break\n        else:\n            # scheme config was not found because get_multiple is not recursive\n            raise ConfigNotFound(\"URL query parameter was not found\")\n\n        # the value of parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre>"},{"location":"utilities/parameters/#secretsprovider","title":"SecretsProvider","text":"builtin_provider_secret.py <pre><code>from typing import Any\n\nimport requests\nfrom botocore.config import Config\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nconfig = Config(region_name=\"sa-east-1\", connect_timeout=1, retries={\"total_max_attempts\": 2, \"max_attempts\": 5})\nssm_provider = parameters.SecretsProvider(config=config)\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n\n    try:\n        # Usually an endpoint is not sensitive data, so we store it in SSM Parameters\n        endpoint_comments: Any = parameters.get_parameter(\"/lambda-powertools/endpoint_comments\")\n        # An API-KEY is a sensitive data and should be stored in SecretsManager\n        api_key: Any = ssm_provider.get(\"/lambda-powertools/api-key\")\n\n        headers: dict = {\"X-API-Key\": api_key}\n\n        comments: requests.Response = requests.get(endpoint_comments, headers=headers)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre>"},{"location":"utilities/parameters/#dynamodbprovider","title":"DynamoDBProvider","text":"<p>The DynamoDB Provider does not have any high-level functions, as it needs to know the name of the DynamoDB table containing the parameters.</p> <p>DynamoDB table structure for single parameters</p> <p>For single parameters, you must use <code>id</code> as the partition key for that table.</p> Example <p>DynamoDB table with <code>id</code> partition key and <code>value</code> as attribute</p> id value my-parameter my-value <p>With this table, <code>dynamodb_provider.get(\"my-parameter\")</code> will return <code>my-value</code>.</p> builtin_provider_dynamodb_single_parameter.pysam_dynamodb_table_single.yaml <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ndynamodb_provider = parameters.DynamoDBProvider(table_name=\"ParameterTable\")\ndef lambda_handler(event: dict, context: LambdaContext):\n\n    try:\n        # Usually an endpoint is not sensitive data, so we store it in DynamoDB Table\nendpoint_comments: Any = dynamodb_provider.get(\"comments_endpoint\")\ncomments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    # general exception\n    except Exception as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: 'DynamoDB Table example'\nResources:\nParameterTable:\nType: AWS::DynamoDB::Table\nProperties:\nTableName: ParameterTable\nAttributeDefinitions:\n-   AttributeName: id\nAttributeType: S\nKeySchema:\n-   AttributeName: id\nKeyType: HASH\nTimeToLiveSpecification:\nAttributeName: expiration\nEnabled: true\nBillingMode: PAY_PER_REQUEST\n</code></pre> <p>You can initialize the DynamoDB provider pointing to DynamoDB Local using <code>endpoint_url</code> parameter:</p> builtin_provider_dynamodb_custom_endpoint.py <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ndynamodb_provider = parameters.DynamoDBProvider(table_name=\"ParameterTable\", endpoint_url=\"http://localhost:8000\")\ndef lambda_handler(event: dict, context: LambdaContext):\n\n    try:\n        # Usually an endpoint is not sensitive data, so we store it in DynamoDB Table\nendpoint_comments: Any = dynamodb_provider.get(\"comments_endpoint\")\ncomments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    # general exception\n    except Exception as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <p>DynamoDB table structure for multiple values parameters</p> <p>You can retrieve multiple parameters sharing the same <code>id</code> by having a sort key named <code>sk</code>.</p> Example <p>DynamoDB table with <code>id</code> primary key, <code>sk</code> as sort key and <code>value</code> as attribute</p> id sk value config endpoint_comments https://jsonplaceholder.typicode.com/comments/ config limit 10 <p>With this table, <code>dynamodb_provider.get_multiple(\"config\")</code> will return a dictionary response in the shape of <code>sk:value</code>.</p> builtin_provider_dynamodb_recursive_parameter.pysam_dynamodb_table_recursive.yaml <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ndynamodb_provider = parameters.DynamoDBProvider(table_name=\"ParameterTable\")\ndef lambda_handler(event: dict, context: LambdaContext):\n\n    try:\n        # Retrieve multiple parameters using HASH KEY\nall_parameters: Any = dynamodb_provider.get_multiple(\"config\")\nendpoint_comments = \"https://jsonplaceholder.typicode.com/noexists/\"\n        limit = 2\n\n        for parameter, value in all_parameters.items():\n\n            if parameter == \"endpoint_comments\":\n                endpoint_comments = value\n\n            if parameter == \"limit\":\n                limit = int(value)\n\n        # the value of parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[limit]}\n    # general exception\n    except Exception as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: 'DynamoDB Table example'\nResources:\nParameterTable:\nType: AWS::DynamoDB::Table\nProperties:\nTableName: ParameterTable\nAttributeDefinitions:\n-   AttributeName: id\nAttributeType: S\n-   AttributeName: sk\nAttributeType: S\nKeySchema:\n-   AttributeName: id\nKeyType: HASH\n-   AttributeName: sk\nKeyType: RANGE\nTimeToLiveSpecification:\nAttributeName: expiration\nEnabled: true\nBillingMode: PAY_PER_REQUEST\n</code></pre> <p>Customizing DynamoDBProvider</p> <p>DynamoDB provider can be customized at initialization to match your table structure:</p> Parameter Mandatory Default Description table_name Yes (N/A) Name of the DynamoDB table containing the parameter values. key_attr No <code>id</code> Hash key for the DynamoDB table. sort_attr No <code>sk</code> Range key for the DynamoDB table. You don't need to set this if you don't use the <code>get_multiple()</code> method. value_attr No <code>value</code> Name of the attribute containing the parameter value. builtin_provider_dynamodb_custom_fields.pysam_dynamodb_custom_fields.yaml <pre><code>from typing import Any\n\nimport requests\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\ndynamodb_provider = parameters.DynamoDBProvider(\ntable_name=\"ParameterTable\", key_attr=\"IdKeyAttr\", sort_attr=\"SkKeyAttr\", value_attr=\"ValueAttr\"\n)\ndef lambda_handler(event: dict, context: LambdaContext):\n\n    try:\n        # Usually an endpoint is not sensitive data, so we store it in DynamoDB Table\nendpoint_comments: Any = dynamodb_provider.get(\"comments_endpoint\")\ncomments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    # general exception\n    except Exception as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>AWSTemplateFormatVersion: '2010-09-09'\nTransform: AWS::Serverless-2016-10-31\nDescription: 'DynamoDB Table example'\nResources:\nParameterTable:\nType: AWS::DynamoDB::Table\nProperties:\nTableName: ParameterTable\nAttributeDefinitions:\n-   AttributeName: IdKeyAttr\nAttributeType: S\n-   AttributeName: SkKeyAttr\nAttributeType: S\nKeySchema:\n-   AttributeName: IdKeyAttr\nKeyType: HASH\n-   AttributeName: SkKeyAttr\nKeyType: RANGE\nTimeToLiveSpecification:\nAttributeName: expiration\nEnabled: true\nBillingMode: PAY_PER_REQUEST\n</code></pre>"},{"location":"utilities/parameters/#appconfigprovider","title":"AppConfigProvider","text":"builtin_provider_appconfig.py <pre><code>from typing import Any\n\nimport requests\nfrom botocore.config import Config\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nconfig = Config(region_name=\"sa-east-1\")\nappconf_provider = parameters.AppConfigProvider(environment=\"dev\", application=\"comments\", config=config)\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve a single parameter\nendpoint_comments: Any = appconf_provider.get(\"config\")\n# the value of this parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre>"},{"location":"utilities/parameters/#create-your-own-provider","title":"Create your own provider","text":"<p>You can create your own custom parameter store provider by inheriting the <code>BaseProvider</code> class, and implementing both <code>_get()</code> and <code>_get_multiple()</code> methods to retrieve a single, or multiple parameters from your custom store.</p> <p>All transformation and caching logic is handled by the <code>get()</code> and <code>get_multiple()</code> methods from the base provider class.</p> <p>Here are two examples of implementing a custom parameter store. One using an external service like Hashicorp Vault, a widely popular key-value and secret storage and the other one using Amazon S3, a popular object storage.</p> working_with_own_provider_vault.pycustom_provider_vault.pyworking_with_own_provider_s3.pycustom_provider_s3.py <pre><code>from typing import Any\n\nimport hvac\nimport requests\nfrom custom_provider_vault import VaultProvider\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\n# In production you must use Vault over HTTPS and certificates.\nvault_provider = VaultProvider(vault_url=\"http://192.168.68.105:8200/\", vault_token=\"YOUR_TOKEN\")\ndef lambda_handler(event: dict, context: LambdaContext):\n\n    try:\n        # Retrieve a single parameter\nendpoint_comments: Any = vault_provider.get(\"comments_endpoint\", transform=\"json\")\n# you can get all parameters using get_multiple and specifying vault mount point\n        # # for testing purposes we will not use it\nall_parameters: Any = vault_provider.get_multiple(\"/\")\nlogger.info(all_parameters)\n\n        # the value of this parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments[\"url\"])\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except hvac.exceptions.InvalidPath as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n    # general exception\n    except Exception as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>import json\nfrom typing import Dict\n\nfrom hvac import Client\n\nfrom aws_lambda_powertools.utilities.parameters import BaseProvider\nclass VaultProvider(BaseProvider):\ndef __init__(self, vault_url: str, vault_token: str) -&gt; None:\n\n        super().__init__()\n\n        self.vault_client = Client(url=vault_url, verify=False, timeout=10)\n        self.vault_client.token = vault_token\n\ndef _get(self, name: str, **sdk_options) -&gt; str:\n# for example proposal, the mountpoint is always /secret\n        kv_configuration = self.vault_client.secrets.kv.v2.read_secret(path=name)\n\n        return json.dumps(kv_configuration[\"data\"][\"data\"])\n\ndef _get_multiple(self, path: str, **sdk_options) -&gt; Dict[str, str]:\nlist_secrets = {}\n        all_secrets = self.vault_client.secrets.kv.v2.list_secrets(path=path)\n\n        # for example proposal, the mountpoint is always /secret\n        for secret in all_secrets[\"data\"][\"keys\"]:\n            kv_configuration = self.vault_client.secrets.kv.v2.read_secret(path=secret)\n\n            for key, value in kv_configuration[\"data\"][\"data\"].items():\n                list_secrets[key] = value\n\n        return list_secrets\n</code></pre> <pre><code>from typing import Any\n\nimport requests\nfrom custom_provider_s3 import S3Provider\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nlogger = Logger()\n\ns3_provider = S3Provider(bucket_name=\"bucket_name\")\ndef lambda_handler(event: dict, context: LambdaContext):\n\n    try:\n        # Retrieve a single parameter using key\nendpoint_comments: Any = s3_provider.get(\"comments_endpoint\")\n# you can get all parameters using get_multiple and specifying a bucket prefix\n        # # for testing purposes we will not use it\nall_parameters: Any = s3_provider.get_multiple(\"/\")\nlogger.info(all_parameters)\n\n        # the value of this parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    # general exception\n    except Exception as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>import copy\nfrom typing import Dict\n\nimport boto3\n\nfrom aws_lambda_powertools.utilities.parameters import BaseProvider\nclass S3Provider(BaseProvider):\ndef __init__(self, bucket_name: str):\n        # Initialize the client to your custom parameter store\n        # E.g.:\n\n        super().__init__()\n\n        self.bucket_name = bucket_name\n        self.client = boto3.client(\"s3\")\n\ndef _get(self, name: str, **sdk_options) -&gt; str:\n# Retrieve a single value\n        # E.g.:\n\n        sdk_options[\"Bucket\"] = self.bucket_name\n        sdk_options[\"Key\"] = name\n\n        response = self.client.get_object(**sdk_options)\n        return response[\"Body\"].read().decode()\n\ndef _get_multiple(self, path: str, **sdk_options) -&gt; Dict[str, str]:\n# Retrieve multiple values\n        # E.g.:\n\n        list_sdk_options = copy.deepcopy(sdk_options)\n\n        list_sdk_options[\"Bucket\"] = self.bucket_name\n        list_sdk_options[\"Prefix\"] = path\n\n        list_response = self.client.list_objects_v2(**list_sdk_options)\n\n        parameters = {}\n\n        for obj in list_response.get(\"Contents\", []):\n            get_sdk_options = copy.deepcopy(sdk_options)\n\n            get_sdk_options[\"Bucket\"] = self.bucket_name\n            get_sdk_options[\"Key\"] = obj[\"Key\"]\n\n            get_response = self.client.get_object(**get_sdk_options)\n\n            parameters[obj[\"Key\"]] = get_response[\"Body\"].read().decode()\n\n        return parameters\n</code></pre>"},{"location":"utilities/parameters/#deserializing-values-with-transform-parameter","title":"Deserializing values with transform parameter","text":"<p>For parameters stored in JSON or Base64 format, you can use the <code>transform</code> argument for deserialization.</p> Info <p>The <code>transform</code> argument is available across all providers, including the high level functions.</p> working_with_transform_high_level.pyworking_with_transform_provider.py <pre><code>from typing import Any\n\nimport requests\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    try:\n        # Retrieve a single parameter\nendpoint_comments: Any = parameters.get_parameter(\"/lambda-powertools/endpoint_comments\", transform=\"json\")\n# the value of this parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre> <pre><code>from typing import Any\n\nimport requests\nfrom botocore.config import Config\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nconfig = Config(region_name=\"sa-east-1\")\nappconf_provider = parameters.AppConfigProvider(environment=\"dev\", application=\"comments\", config=config)\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n    try:\n        # Retrieve a single parameter\nendpoint_comments: Any = appconf_provider.get(\"config\", transform=\"json\")\n# the value of this parameter is https://jsonplaceholder.typicode.com/comments/\n        comments: requests.Response = requests.get(endpoint_comments)\n\n        return {\"comments\": comments.json()[:10], \"statusCode\": 200}\n    except parameters.exceptions.GetParameterError as error:\n        return {\"comments\": None, \"message\": str(error), \"statusCode\": 400}\n</code></pre>"},{"location":"utilities/parameters/#partial-transform-failures-with-get_multiple","title":"Partial transform failures with <code>get_multiple()</code>","text":"<p>If you use <code>transform</code> with <code>get_multiple()</code>, you can have a single malformed parameter value. To prevent failing the entire request, the method will return a <code>None</code> value for the parameters that failed to transform.</p> <p>You can override this by setting the <code>raise_on_transform_error</code> argument to <code>True</code>. If you do so, a single transform error will raise a <code>TransformParameterError</code> exception.</p> <p>For example, if you have three parameters, /param/a, /param/b and /param/c, but /param/c is malformed:</p> handling_error_transform.py <pre><code>from typing import Any\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nssm_provider = parameters.SSMProvider()\n\n\ndef lambda_handler(event: dict, context: LambdaContext):\n    # This will display:\n    # /param/a: [some value]\n    # /param/b: [some value]\n    # /param/c: None\nvalues: Any = ssm_provider.get_multiple(\"/param\", transform=\"json\")\nfor key, value in values.items():\n        print(f\"{key}: {value}\")\n\n    try:\n        # This will raise a TransformParameterError exception\nvalues = ssm_provider.get_multiple(\"/param\", transform=\"json\", raise_on_transform_error=True)\nexcept parameters.exceptions.TransformParameterError:\n        ...\n</code></pre>"},{"location":"utilities/parameters/#auto-transform-values-on-suffix","title":"Auto-transform values on suffix","text":"<p>If you use <code>transform</code> with <code>get_multiple()</code>, you might want to retrieve and transform parameters encoded in different formats.</p> <p>You can do this with a single request by using <code>transform=\"auto\"</code>. This will instruct any Parameter to to infer its type based on the suffix and transform it accordingly.</p> Info <p><code>transform=\"auto\"</code> feature is available across all providers, including the high level functions.</p> working_with_auto_transform.py <pre><code>from aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nssm_provider = parameters.SSMProvider()\ndef lambda_handler(event: dict, context: LambdaContext):\nvalues = ssm_provider.get_multiple(\"/param\", transform=\"auto\")\nreturn values\n</code></pre> <p>For example, if you have two parameters with the following suffixes <code>.json</code> and <code>.binary</code>:</p> Parameter name Parameter value /param/a.json [some encoded value] /param/a.binary [some encoded value] <p>The return of <code>ssm_provider.get_multiple(\"/param\", transform=\"auto\")</code> call will be a dictionary like:</p> <pre><code>{\n\"a.json\": [some value],\n\"b.binary\": [some value]\n}\n</code></pre>"},{"location":"utilities/parameters/#passing-additional-sdk-arguments","title":"Passing additional SDK arguments","text":"<p>You can use arbitrary keyword arguments to pass it directly to the underlying SDK method.</p> working_with_sdk_additional_arguments.py <pre><code>from aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nsecrets_provider = parameters.SecretsProvider()\ndef lambda_handler(event: dict, context: LambdaContext):\n    # The 'VersionId' argument will be passed to the underlying get_secret_value() call.\nvalue = secrets_provider.get(\"my-secret\", VersionId=\"e62ec170-6b01-48c7-94f3-d7497851a8d2\")\nreturn value\n</code></pre> <p>Here is the mapping between this utility's functions and methods and the underlying SDK:</p> Provider Function/Method Client name Function name SSM Parameter Store <code>get_parameter</code> <code>ssm</code> get_parameter SSM Parameter Store <code>get_parameters</code> <code>ssm</code> get_parameters_by_path SSM Parameter Store <code>SSMProvider.get</code> <code>ssm</code> get_parameter SSM Parameter Store <code>SSMProvider.get_multiple</code> <code>ssm</code> get_parameters_by_path Secrets Manager <code>get_secret</code> <code>secretsmanager</code> get_secret_value Secrets Manager <code>SecretsProvider.get</code> <code>secretsmanager</code> get_secret_value DynamoDB <code>DynamoDBProvider.get</code> <code>dynamodb</code> (Table resource) DynamoDB <code>DynamoDBProvider.get_multiple</code> <code>dynamodb</code> (Table resource) App Config <code>get_app_config</code> <code>appconfigdata</code> start_configuration_session and get_latest_configuration"},{"location":"utilities/parameters/#bring-your-own-boto-client","title":"Bring your own boto client","text":"<p>You can use <code>boto3_client</code> parameter via any of the available Provider Classes. Some providers expect a low level boto3 client while others expect a high level boto3 client, here is the mapping for each of them:</p> Provider Type Boto client construction SSMProvider low level <code>boto3.client(\"ssm\")</code> SecretsProvider low level <code>boto3.client(\"secrets\")</code> AppConfigProvider low level <code>boto3.client(\"appconfigdata\")</code> DynamoDBProvider high level <code>boto3.resource(\"dynamodb\")</code> <p>Bringing them together in a single code snippet would look like this:</p> custom_boto3_all_providers.py <pre><code>import boto3\nfrom botocore.config import Config\n\nfrom aws_lambda_powertools.utilities import parameters\nconfig = Config(region_name=\"us-west-1\")\n# construct boto clients with any custom configuration\nssm = boto3.client(\"ssm\", config=config)\nsecrets = boto3.client(\"secrets\", config=config)\nappconfig = boto3.client(\"appconfigdata\", config=config)\ndynamodb = boto3.resource(\"dynamodb\", config=config)\n\nssm_provider = parameters.SSMProvider(boto3_client=ssm)\nsecrets_provider = parameters.SecretsProvider(boto3_client=secrets)\nappconf_provider = parameters.AppConfigProvider(boto3_client=appconfig, environment=\"my_env\", application=\"my_app\")\ndynamodb_provider = parameters.DynamoDBProvider(boto3_client=dynamodb, table_name=\"my-table\")\n</code></pre> When is this useful? <p>Injecting a custom boto3 client can make unit/snapshot testing easier, including SDK customizations.</p>"},{"location":"utilities/parameters/#customizing-boto-configuration","title":"Customizing boto configuration","text":"<p>The <code>config</code> , <code>boto3_session</code>, and <code>boto3_client</code>  parameters enable you to pass in a custom botocore config object , boto3 session, or  a boto3 client when constructing any of the built-in provider classes.</p> Tip <p>You can use a custom session for retrieving parameters cross-account/region and for snapshot testing.</p> <p>When using VPC private endpoints, you can pass a custom client altogether. It's also useful for testing when injecting fake instances.</p> custom_boto_session.pycustom_boto_config.pycustom_boto_client.py <pre><code>import boto3\n\nfrom aws_lambda_powertools.utilities import parameters\n\nboto3_session = boto3.session.Session()\nssm_provider = parameters.SSMProvider(boto3_session=boto3_session)\ndef handler(event, context):\n    # Retrieve a single parameter\n    value = ssm_provider.get(\"/my/parameter\")\n\n    return value\n</code></pre> <pre><code>from botocore.config import Config\n\nfrom aws_lambda_powertools.utilities import parameters\n\nboto_config = Config()\nssm_provider = parameters.SSMProvider(config=boto_config)\ndef handler(event, context):\n    # Retrieve a single parameter\n    value = ssm_provider.get(\"/my/parameter\")\n\n    return value\n</code></pre> <pre><code>import boto3\n\nfrom aws_lambda_powertools.utilities import parameters\n\nboto3_client = boto3.client(\"ssm\")\nssm_provider = parameters.SSMProvider(boto3_client=boto3_client)\ndef handler(event, context):\n    # Retrieve a single parameter\n    value = ssm_provider.get(\"/my/parameter\")\n\n    return value\n</code></pre>"},{"location":"utilities/parameters/#testing-your-code","title":"Testing your code","text":""},{"location":"utilities/parameters/#mocking-parameter-values","title":"Mocking parameter values","text":"<p>For unit testing your applications, you can mock the calls to the parameters utility to avoid calling AWS APIs. This can be achieved in a number of ways - in this example, we use the pytest monkeypatch fixture to patch the <code>parameters.get_parameter</code> method:</p> test_single_mock.pysingle_mock.py <pre><code>import src.single_mock as single_mock\n\n\ndef test_handler(monkeypatch):\ndef mockreturn(name):\n        return \"mock_value\"\n\nmonkeypatch.setattr(single_mock.parameters, \"get_parameter\", mockreturn)\nreturn_val = single_mock.handler({}, {})\n    assert return_val.get(\"message\") == \"mock_value\"\n</code></pre> <pre><code>from aws_lambda_powertools.utilities import parameters\n\n\ndef handler(event, context):\n    # Retrieve a single parameter\n    value = parameters.get_parameter(\"my-parameter-name\")\n    return {\"message\": value}\n</code></pre> <p>If we need to use this pattern across multiple tests, we can avoid repetition by refactoring to use our own pytest fixture:</p> test_with_fixture.py <pre><code>import pytest\nimport src.single_mock as single_mock\n\n\n@pytest.fixture\ndef mock_parameter_response(monkeypatch):\n    def mockreturn(name):\n        return \"mock_value\"\n\nmonkeypatch.setattr(single_mock.parameters, \"get_parameter\", mockreturn)\n# Pass our fixture as an argument to all tests where we want to mock the get_parameter response\ndef test_handler(mock_parameter_response):\n    return_val = single_mock.handler({}, {})\n    assert return_val.get(\"message\") == \"mock_value\"\n</code></pre> <p>Alternatively, if we need more fully featured mocking (for example checking the arguments passed to <code>get_parameter</code>), we can use unittest.mock from the python stdlib instead of pytest's <code>monkeypatch</code> fixture. In this example, we use the patch decorator to replace the <code>aws_lambda_powertools.utilities.parameters.get_parameter</code> function with a MagicMock object named <code>get_parameter_mock</code>.</p> test_with_monkeypatch.py <pre><code>from unittest.mock import patch\n\nimport src.single_mock as single_mock\n\n\n# Replaces \"aws_lambda_powertools.utilities.parameters.get_parameter\" with a Mock object\n@patch(\"aws_lambda_powertools.utilities.parameters.get_parameter\")\ndef test_handler(get_parameter_mock):\n    get_parameter_mock.return_value = \"mock_value\"\n\n    return_val = single_mock.handler({}, {})\nget_parameter_mock.assert_called_with(\"my-parameter-name\")\nassert return_val.get(\"message\") == \"mock_value\"\n</code></pre>"},{"location":"utilities/parameters/#clearing-cache","title":"Clearing cache","text":"<p>Parameters utility caches all parameter values for performance and cost reasons. However, this can have unintended interference in tests using the same parameter name.</p> <p>Within your tests, you can use <code>clear_cache</code> method available in every provider. When using multiple providers or higher level functions like <code>get_parameter</code>, use <code>clear_caches</code> standalone function to clear cache globally.</p> test_clear_cache_method.pytest_clear_cache_global.pyapp.py <pre><code>import pytest\nimport src.app as app\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef clear_parameters_cache():\n    yield\napp.ssm_provider.clear_cache()  # This will clear SSMProvider cache\n@pytest.fixture\ndef mock_parameter_response(monkeypatch):\n    def mockreturn(name):\n        return \"mock_value\"\n\n    monkeypatch.setattr(app.ssm_provider, \"get\", mockreturn)\n\n\n# Pass our fixture as an argument to all tests where we want to mock the get_parameter response\ndef test_handler(mock_parameter_response):\n    return_val = app.handler({}, {})\n    assert return_val.get(\"message\") == \"mock_value\"\n</code></pre> <pre><code>import pytest\nimport src.app as app\n\nfrom aws_lambda_powertools.utilities import parameters\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef clear_parameters_cache():\n    yield\nparameters.clear_caches()  # This will clear all providers cache\n@pytest.fixture\ndef mock_parameter_response(monkeypatch):\n    def mockreturn(name):\n        return \"mock_value\"\n\n    monkeypatch.setattr(app.ssm_provider, \"get\", mockreturn)\n\n\n# Pass our fixture as an argument to all tests where we want to mock the get_parameter response\ndef test_handler(mock_parameter_response):\n    return_val = app.handler({}, {})\n    assert return_val.get(\"message\") == \"mock_value\"\n</code></pre> <pre><code>from botocore.config import Config\n\nfrom aws_lambda_powertools.utilities import parameters\n\nssm_provider = parameters.SSMProvider(config=Config(region_name=\"us-west-1\"))\n\n\ndef handler(event, context):\n    value = ssm_provider.get(\"/my/parameter\")\n    return {\"message\": value}\n</code></pre>"},{"location":"utilities/parser/","title":"Parser (Pydantic)","text":"<p>This utility provides data parsing and deep validation using Pydantic.</p>"},{"location":"utilities/parser/#key-features","title":"Key features","text":"<ul> <li>Defines data in pure Python classes, then parse, validate and extract only what you want</li> <li>Built-in envelopes to unwrap, extend, and validate popular event sources payloads</li> <li>Enforces type hints at runtime with user-friendly errors</li> </ul>"},{"location":"utilities/parser/#getting-started","title":"Getting started","text":""},{"location":"utilities/parser/#install","title":"Install","text":"<p>This is not necessary if you're installing Powertools via Lambda Layer/SAR</p> <p>Add <code>aws-lambda-powertools[parser]</code> as a dependency in your preferred tool: e.g., requirements.txt, pyproject.toml. This will ensure you have the required dependencies before using Parser.</p> Warning <p>This will increase the compressed package size by &gt;10MB due to the Pydantic dependency.</p> <p>To reduce the impact on the package size at the expense of 30%-50% of its performance Pydantic can also be installed without binary files:</p> <p>Pip example: <code>SKIP_CYTHON=1 pip install --no-binary pydantic aws-lambda-powertools[parser]</code></p>"},{"location":"utilities/parser/#defining-models","title":"Defining models","text":"<p>You can define models to parse incoming events by inheriting from <code>BaseModel</code>.</p> Defining an Order data model<pre><code>from aws_lambda_powertools.utilities.parser import BaseModel\nfrom typing import List, Optional\n\nclass OrderItem(BaseModel):\n    id: int\n    quantity: int\n    description: str\n\nclass Order(BaseModel):\n    id: int\n    description: str\n    items: List[OrderItem] # nesting models are supported\n    optional_field: Optional[str] # this field may or may not be available when parsing\n</code></pre> <p>These are simply Python classes that inherit from BaseModel. Parser enforces type hints declared in your model at runtime.</p>"},{"location":"utilities/parser/#parsing-events","title":"Parsing events","text":"<p>You can parse inbound events using event_parser decorator, or the standalone <code>parse</code> function. Both are also able to parse either dictionary or JSON string as an input.</p>"},{"location":"utilities/parser/#event_parser-decorator","title":"event_parser decorator","text":"<p>Use the decorator for fail fast scenarios where you want your Lambda function to raise an exception in the event of a malformed payload.</p> <p><code>event_parser</code> decorator will throw a <code>ValidationError</code> if your event cannot be parsed according to the model.</p> Note <p>This decorator will replace the <code>event</code> object with the parsed model if successful. This means you might be careful when nesting other decorators that expect <code>event</code> to be a <code>dict</code>.</p> Parsing and validating upon invocation with event_parser decorator<pre><code>from aws_lambda_powertools.utilities.parser import event_parser, BaseModel\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nfrom typing import List, Optional\n\nimport json\n\nclass OrderItem(BaseModel):\n    id: int\n    quantity: int\n    description: str\n\nclass Order(BaseModel):\n    id: int\n    description: str\n    items: List[OrderItem] # nesting models are supported\n    optional_field: Optional[str] # this field may or may not be available when parsing\n\n\n@event_parser(model=Order)\ndef handler(event: Order, context: LambdaContext):\n    print(event.id)\n    print(event.description)\n    print(event.items)\n\n    order_items = [item for item in event.items]\n    ...\n\npayload = {\n    \"id\": 10876546789,\n    \"description\": \"My order\",\n    \"items\": [\n        {\n            \"id\": 1015938732,\n            \"quantity\": 1,\n            \"description\": \"item xpto\"\n        }\n    ]\n}\n\nhandler(event=payload, context=LambdaContext())\nhandler(event=json.dumps(payload), context=LambdaContext()) # also works if event is a JSON string\n</code></pre>"},{"location":"utilities/parser/#parse-function","title":"parse function","text":"<p>Use this standalone function when you want more control over the data validation process, for example returning a 400 error for malformed payloads.</p> Using standalone parse function for more flexibility<pre><code>from aws_lambda_powertools.utilities.parser import parse, BaseModel, ValidationError\nfrom typing import List, Optional\n\nclass OrderItem(BaseModel):\n    id: int\n    quantity: int\n    description: str\n\nclass Order(BaseModel):\n    id: int\n    description: str\n    items: List[OrderItem] # nesting models are supported\n    optional_field: Optional[str] # this field may or may not be available when parsing\n\n\npayload = {\n    \"id\": 10876546789,\n    \"description\": \"My order\",\n    \"items\": [\n        {\n# this will cause a validation error\n\"id\": [1015938732],\n            \"quantity\": 1,\n            \"description\": \"item xpto\"\n        }\n    ]\n}\n\ndef my_function():\n    try:\nparsed_payload: Order = parse(event=payload, model=Order)\n# payload dict is now parsed into our model\n        return parsed_payload.items\n    except ValidationError:\n        return {\n            \"status_code\": 400,\n            \"message\": \"Invalid order\"\n        }\n</code></pre>"},{"location":"utilities/parser/#built-in-models","title":"Built-in models","text":"<p>Parser comes with the following built-in models:</p> Model name Description DynamoDBStreamModel Lambda Event Source payload for Amazon DynamoDB Streams EventBridgeModel Lambda Event Source payload for Amazon EventBridge SqsModel Lambda Event Source payload for Amazon SQS AlbModel Lambda Event Source payload for Amazon Application Load Balancer CloudwatchLogsModel Lambda Event Source payload for Amazon CloudWatch Logs S3Model Lambda Event Source payload for Amazon S3 S3ObjectLambdaEvent Lambda Event Source payload for Amazon S3 Object Lambda S3EventNotificationEventBridgeModel Lambda Event Source payload for Amazon S3 Event Notification to EventBridge. KinesisDataStreamModel Lambda Event Source payload for Amazon Kinesis Data Streams KinesisFirehoseModel Lambda Event Source payload for Amazon Kinesis Firehose SesModel Lambda Event Source payload for Amazon Simple Email Service SnsModel Lambda Event Source payload for Amazon Simple Notification Service APIGatewayProxyEventModel Lambda Event Source payload for Amazon API Gateway APIGatewayProxyEventV2Model Lambda Event Source payload for Amazon API Gateway v2 payload LambdaFunctionUrlModel Lambda Event Source payload for Lambda Function URL payload KafkaSelfManagedEventModel Lambda Event Source payload for self managed Kafka payload KafkaMskEventModel Lambda Event Source payload for AWS MSK payload"},{"location":"utilities/parser/#extending-built-in-models","title":"Extending built-in models","text":"<p>You can extend them to include your own models, and yet have all other known fields parsed along the way.</p> Tip <p>For Mypy users, we only allow type override for fields where payload is injected e.g. <code>detail</code>, <code>body</code>, etc.</p> Extending EventBridge model as an example<pre><code>from aws_lambda_powertools.utilities.parser import parse, BaseModel\nfrom aws_lambda_powertools.utilities.parser.models import EventBridgeModel\n\nfrom typing import List, Optional\n\nclass OrderItem(BaseModel):\n    id: int\n    quantity: int\n    description: str\n\nclass Order(BaseModel):\n    id: int\n    description: str\n    items: List[OrderItem]\n\nclass OrderEventModel(EventBridgeModel):\ndetail: Order\npayload = {\n    \"version\": \"0\",\n    \"id\": \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\",\n    \"detail-type\": \"OrderPurchased\",\n    \"source\": \"OrderService\",\n    \"account\": \"111122223333\",\n    \"time\": \"2020-10-22T18:43:48Z\",\n    \"region\": \"us-west-1\",\n    \"resources\": [\"some_additional\"],\n\"detail\": {\n\"id\": 10876546789,\n        \"description\": \"My order\",\n        \"items\": [\n            {\n                \"id\": 1015938732,\n                \"quantity\": 1,\n                \"description\": \"item xpto\"\n            }\n        ]\n    }\n}\n\nret = parse(model=OrderEventModel, event=payload)\nassert ret.source == \"OrderService\"\nassert ret.detail.description == \"My order\"\nassert ret.detail_type == \"OrderPurchased\" # we rename it to snake_case since detail-type is an invalid name\n\nfor order_item in ret.detail.items:\n    ...\n</code></pre> <p>What's going on here, you might ask:</p> <ol> <li>We imported our built-in model <code>EventBridgeModel</code> from the parser utility</li> <li>Defined how our <code>Order</code> should look like</li> <li>Defined how part of our EventBridge event should look like by overriding <code>detail</code> key within our <code>OrderEventModel</code></li> <li>Parser parsed the original event against <code>OrderEventModel</code></li> </ol> Tip <p>When extending a <code>string</code> field containing JSON, you need to wrap the field with Pydantic's Json Type:</p> <pre><code>from pydantic import BaseModel, Json\n\nfrom aws_lambda_powertools.utilities.parser import event_parser\nfrom aws_lambda_powertools.utilities.parser.models import APIGatewayProxyEventV2Model\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\nclass CancelOrder(BaseModel):\n    order_id: int\n    reason: str\n\n\nclass CancelOrderModel(APIGatewayProxyEventV2Model):\nbody: Json[CancelOrder]  # type: ignore[assignment]\n@event_parser(model=CancelOrderModel)\ndef handler(event: CancelOrderModel, context: LambdaContext):\ncancel_order: CancelOrder = event.body\nassert cancel_order.order_id is not None\n</code></pre> <p>Alternatively, you could use a Pydantic validator to transform the JSON string into a dict before the mapping:</p> <pre><code>import json\n\nfrom pydantic import BaseModel, validator\n\nfrom aws_lambda_powertools.utilities.parser import event_parser\nfrom aws_lambda_powertools.utilities.parser.models import APIGatewayProxyEventV2Model\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\nclass CancelOrder(BaseModel):\n    order_id: int\n    reason: str\n\n\nclass CancelOrderModel(APIGatewayProxyEventV2Model):\n    body: CancelOrder  # type: ignore[assignment]\n\n@validator(\"body\", pre=True)\ndef transform_body_to_dict(cls, value: str):\nreturn json.loads(value)\n@event_parser(model=CancelOrderModel)\ndef handler(event: CancelOrderModel, context: LambdaContext):\ncancel_order: CancelOrder = event.body\nassert cancel_order.order_id is not None\n</code></pre>"},{"location":"utilities/parser/#envelopes","title":"Envelopes","text":"<p>When trying to parse your payloads wrapped in a known structure, you might encounter the following situations:</p> <ul> <li>Your actual payload is wrapped around a known structure, for example Lambda Event Sources like EventBridge</li> <li>You're only interested in a portion of the payload, for example parsing the <code>detail</code> of custom events in EventBridge, or <code>body</code> of SQS records</li> </ul> <p>You can either solve these situations by creating a model of these known structures, parsing them, then extracting and parsing a key where your payload is.</p> <p>This can become difficult quite quickly. Parser makes this problem easier through a feature named <code>Envelope</code>.</p> <p>Envelopes can be used via <code>envelope</code> parameter available in both <code>parse</code> function and <code>event_parser</code> decorator.</p> <p>Here's an example of parsing a model found in an event coming from EventBridge, where all you want is what's inside the <code>detail</code> key.</p> Parsing payload in a given key only using envelope feature<pre><code>from aws_lambda_powertools.utilities.parser import event_parser, parse, BaseModel, envelopes\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nclass UserModel(BaseModel):\n    username: str\n    password1: str\n    password2: str\n\npayload = {\n    \"version\": \"0\",\n    \"id\": \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\",\n    \"detail-type\": \"CustomerSignedUp\",\n    \"source\": \"CustomerService\",\n    \"account\": \"111122223333\",\n    \"time\": \"2020-10-22T18:43:48Z\",\n    \"region\": \"us-west-1\",\n    \"resources\": [\"some_additional_\"],\n\"detail\": {\n\"username\": \"universe\",\n\"password1\": \"myp@ssword\",\n\"password2\": \"repeat password\"\n}\n}\n\nret = parse(model=UserModel, envelope=envelopes.EventBridgeEnvelope, event=payload)\n# Parsed model only contains our actual model, not the entire EventBridge + Payload parsed\nassert ret.password1 == ret.password2\n\n# Same behaviour but using our decorator\n@event_parser(model=UserModel, envelope=envelopes.EventBridgeEnvelope)\ndef handler(event: UserModel, context: LambdaContext):\n    assert event.password1 == event.password2\n</code></pre> <p>What's going on here, you might ask:</p> <ol> <li>We imported built-in <code>envelopes</code> from the parser utility</li> <li>Used <code>envelopes.EventBridgeEnvelope</code> as the envelope for our <code>UserModel</code> model</li> <li>Parser parsed the original event against the EventBridge model</li> <li>Parser then parsed the <code>detail</code> key using <code>UserModel</code></li> </ol>"},{"location":"utilities/parser/#built-in-envelopes","title":"Built-in envelopes","text":"<p>Parser comes with the following built-in envelopes, where <code>Model</code> in the return section is your given model.</p> Envelope name Behaviour Return DynamoDBStreamEnvelope 1. Parses data using <code>DynamoDBStreamModel</code>.  2. Parses records in <code>NewImage</code> and <code>OldImage</code> keys using your model.  3. Returns a list with a dictionary containing <code>NewImage</code> and <code>OldImage</code> keys <code>List[Dict[str, Optional[Model]]]</code> EventBridgeEnvelope 1. Parses data using <code>EventBridgeModel</code>.  2. Parses <code>detail</code> key using your model and returns it. <code>Model</code> SqsEnvelope 1. Parses data using <code>SqsModel</code>.  2. Parses records in <code>body</code> key using your model and return them in a list. <code>List[Model]</code> CloudWatchLogsEnvelope 1. Parses data using <code>CloudwatchLogsModel</code> which will base64 decode and decompress it.  2. Parses records in <code>message</code> key using your model and return them in a list. <code>List[Model]</code> KinesisDataStreamEnvelope 1. Parses data using <code>KinesisDataStreamModel</code> which will base64 decode it.  2. Parses records in in <code>Records</code> key using your model and returns them in a list. <code>List[Model]</code> KinesisFirehoseEnvelope 1. Parses data using <code>KinesisFirehoseModel</code> which will base64 decode it.  2. Parses records in in <code>Records</code> key using your model and returns them in a list. <code>List[Model]</code> SnsEnvelope 1. Parses data using <code>SnsModel</code>.  2. Parses records in <code>body</code> key using your model and return them in a list. <code>List[Model]</code> SnsSqsEnvelope 1. Parses data using <code>SqsModel</code>.  2. Parses SNS records in <code>body</code> key using <code>SnsNotificationModel</code>.  3. Parses data in <code>Message</code> key using your model and return them in a list. <code>List[Model]</code> ApiGatewayEnvelope 1. Parses data using <code>APIGatewayProxyEventModel</code>.  2. Parses <code>body</code> key using your model and returns it. <code>Model</code> ApiGatewayV2Envelope 1. Parses data using <code>APIGatewayProxyEventV2Model</code>.  2. Parses <code>body</code> key using your model and returns it. <code>Model</code> LambdaFunctionUrlEnvelope 1. Parses data using <code>LambdaFunctionUrlModel</code>.  2. Parses <code>body</code> key using your model and returns it. <code>Model</code> KafkaEnvelope 1. Parses data using <code>KafkaRecordModel</code>.  2. Parses <code>value</code> key using your model and returns it. <code>Model</code>"},{"location":"utilities/parser/#bringing-your-own-envelope","title":"Bringing your own envelope","text":"<p>You can create your own Envelope model and logic by inheriting from <code>BaseEnvelope</code>, and implementing the <code>parse</code> method.</p> <p>Here's a snippet of how the EventBridge envelope we demonstrated previously is implemented.</p> EventBridge ModelEventBridge Envelope <pre><code>from datetime import datetime\nfrom typing import Any, Dict, List\n\nfrom aws_lambda_powertools.utilities.parser import BaseModel, Field\n\n\nclass EventBridgeModel(BaseModel):\n    version: str\n    id: str  # noqa: A003,VNE003\n    source: str\n    account: str\n    time: datetime\n    region: str\n    resources: List[str]\n    detail_type: str = Field(None, alias=\"detail-type\")\n    detail: Dict[str, Any]\n</code></pre> <pre><code>from aws_lambda_powertools.utilities.parser import BaseEnvelope, models\nfrom aws_lambda_powertools.utilities.parser.models import EventBridgeModel\n\nfrom typing import Any, Dict, Optional, TypeVar\n\nModel = TypeVar(\"Model\", bound=BaseModel)\n\nclass EventBridgeEnvelope(BaseEnvelope):\ndef parse(self, data: Optional[Union[Dict[str, Any], Any]], model: Model) -&gt; Optional[Model]:\n\"\"\"Parses data found with model provided\n\n        Parameters\n        ----------\n        data : Dict\n            Lambda event to be parsed\n        model : Model\n            Data model provided to parse after extracting data using envelope\n\n        Returns\n        -------\n        Any\n            Parsed detail payload with model provided\n        \"\"\"\nparsed_envelope = EventBridgeModel.parse_obj(data)\nreturn self._parse(data=parsed_envelope.detail, model=model)\n</code></pre> <p>What's going on here, you might ask:</p> <ol> <li>We defined an envelope named <code>EventBridgeEnvelope</code> inheriting from <code>BaseEnvelope</code></li> <li>Implemented the <code>parse</code> abstract method taking <code>data</code> and <code>model</code> as parameters</li> <li>Then, we parsed the incoming data with our envelope to confirm it matches EventBridge's structure defined in <code>EventBridgeModel</code></li> <li>Lastly, we call <code>_parse</code> from <code>BaseEnvelope</code> to parse the data in our envelope (.detail) using the customer model</li> </ol>"},{"location":"utilities/parser/#data-model-validation","title":"Data model validation","text":"Warning <p>This is radically different from the Validator utility which validates events against JSON Schema.</p> <p>You can use parser's validator for deep inspection of object values and complex relationships.</p> <p>There are two types of class method decorators you can use:</p> <ul> <li><code>validator</code> - Useful to quickly validate an individual field and its value</li> <li><code>root_validator</code> - Useful to validate the entire model's data</li> </ul> <p>Keep the following in mind regardless of which decorator you end up using it:</p> <ul> <li>You must raise either <code>ValueError</code>, <code>TypeError</code>, or <code>AssertionError</code> when value is not compliant</li> <li>You must return the value(s) itself if compliant</li> </ul>"},{"location":"utilities/parser/#validating-fields","title":"validating fields","text":"<p>Quick validation to verify whether the field <code>message</code> has the value of <code>hello world</code>.</p> Data field validation with validator<pre><code>from aws_lambda_powertools.utilities.parser import parse, BaseModel, validator\n\nclass HelloWorldModel(BaseModel):\n    message: str\n\n@validator('message')\ndef is_hello_world(cls, v):\n        if v != \"hello world\":\n            raise ValueError(\"Message must be hello world!\")\n        return v\n\nparse(model=HelloWorldModel, event={\"message\": \"hello universe\"})\n</code></pre> <p>If you run as-is, you should expect the following error with the message we provided in our exception:</p> Sample validation error message<pre><code>message\n  Message must be hello world! (type=value_error)\n</code></pre> <p>Alternatively, you can pass <code>'*'</code> as an argument for the decorator so that you can validate every value available.</p> Validating all data fields with custom logic<pre><code>from aws_lambda_powertools.utilities.parser import parse, BaseModel, validator\n\nclass HelloWorldModel(BaseModel):\n    message: str\n    sender: str\n\n@validator('*')\ndef has_whitespace(cls, v):\n        if ' ' not in v:\n            raise ValueError(\"Must have whitespace...\")\n\n        return v\n\nparse(model=HelloWorldModel, event={\"message\": \"hello universe\", \"sender\": \"universe\"})\n</code></pre>"},{"location":"utilities/parser/#validating-entire-model","title":"validating entire model","text":"<p><code>root_validator</code> can help when you have a complex validation mechanism. For example finding whether data has been omitted, comparing field values, etc.</p> Comparing and validating multiple fields at once with root_validator<pre><code>from aws_lambda_powertools.utilities.parser import parse, BaseModel, root_validator\n\nclass UserModel(BaseModel):\n    username: str\n    password1: str\n    password2: str\n\n    @root_validator\n    def check_passwords_match(cls, values):\n        pw1, pw2 = values.get('password1'), values.get('password2')\n        if pw1 is not None and pw2 is not None and pw1 != pw2:\n            raise ValueError('passwords do not match')\n        return values\n\npayload = {\n    \"username\": \"universe\",\n    \"password1\": \"myp@ssword\",\n    \"password2\": \"repeat password\"\n}\n\nparse(model=UserModel, event=payload)\n</code></pre> Info <p>You can read more about validating list items, reusing validators, validating raw inputs, and a lot more in Pydantic's documentation.</p>"},{"location":"utilities/parser/#advanced-use-cases","title":"Advanced use cases","text":"Tip: Looking to auto-generate models from JSON, YAML, JSON Schemas, OpenApi, etc? <p>Use Koudai Aono's data model code generation tool for Pydantic</p> <p>There are number of advanced use cases well documented in Pydantic's doc such as creating immutable models, declaring fields with dynamic values.</p> Pydantic helper functions <p>Pydantic also offers functions to parse models from files, dicts, string, etc.</p> <p>Two possible unknown use cases are Models and exception' serialization. Models have methods to export them as <code>dict</code>, <code>JSON</code>, <code>JSON Schema</code>, and Validation exceptions can be exported as JSON.</p> Converting data models in various formats<pre><code>from aws_lambda_powertools.utilities import Logger\nfrom aws_lambda_powertools.utilities.parser import parse, BaseModel, ValidationError, validator\n\nlogger = Logger(service=\"user\")\n\nclass UserModel(BaseModel):\n    username: str\n    password1: str\n    password2: str\n\npayload = {\n    \"username\": \"universe\",\n    \"password1\": \"myp@ssword\",\n    \"password2\": \"repeat password\"\n}\n\ndef my_function():\n    try:\n        return parse(model=UserModel, event=payload)\n    except ValidationError as e:\nlogger.exception(e.json())\nreturn {\n            \"status_code\": 400,\n            \"message\": \"Invalid username\"\n        }\n\nUser: UserModel = my_function()\nuser_dict = User.dict()\nuser_json = User.json()\nuser_json_schema_as_dict = User.schema()\nuser_json_schema_as_json = User.schema_json(indent=2)\n</code></pre> <p>These can be quite useful when manipulating models that later need to be serialized as inputs for services like DynamoDB, EventBridge, etc.</p>"},{"location":"utilities/parser/#faq","title":"FAQ","text":"<p>When should I use parser vs data_classes utility?</p> <p>Use data classes utility when you're after autocomplete, self-documented attributes and helpers to extract data from common event sources.</p> <p>Parser is best suited for those looking for a trade-off between defining their models for deep validation, parsing and autocomplete for an additional dependency to be brought in.</p> <p>How do I import X from Pydantic?</p> <p>We export most common classes, exceptions, and utilities from Pydantic as part of parser e.g. <code>from aws_lambda_powertools.utilities.parser import BaseModel</code>.</p> <p>If what you're trying to use isn't available as part of the high level import system, use the following escape hatch mechanism:</p> Pydantic import escape hatch<pre><code>from aws_lambda_powertools.utilities.parser.pydantic import &lt;what you'd like to import'&gt;\n</code></pre>"},{"location":"utilities/streaming/","title":"Streaming","text":"<p>The streaming utility handles datasets larger than the available memory as streaming data.</p>"},{"location":"utilities/streaming/#key-features","title":"Key Features","text":"<ul> <li>Stream Amazon S3 objects with a file-like interface with minimal memory consumption</li> <li>Built-in popular data transformations to decompress and deserialize (gzip, CSV, and ZIP)</li> <li>Build your own data transformation and add it to the pipeline</li> </ul>"},{"location":"utilities/streaming/#background","title":"Background","text":"<p>Within Lambda, processing S3 objects larger than the allocated amount of memory can lead to out of memory or timeout situations. For cost efficiency, your S3 objects may be encoded and compressed in various formats (gzip, CSV, zip files, etc), increasing the  amount of non-business logic and reliability risks.</p> <p>Streaming utility makes this process easier by fetching parts of your data as you consume it, and transparently applying data transformations to the data stream. This allows you to process one, a few, or all rows of your large dataset while consuming a few MBs only.</p>"},{"location":"utilities/streaming/#getting-started","title":"Getting started","text":""},{"location":"utilities/streaming/#streaming-from-a-s3-object","title":"Streaming from a S3 object","text":"<p>With <code>S3Object</code>, you'll need the bucket, object key, and optionally a version ID to stream its content.</p> <p>We will fetch parts of your data from S3 as you process each line, consuming only the absolute minimal amount of memory.</p> Non-versioned bucketVersioned bucket <pre><code>from typing import Dict\n\nfrom aws_lambda_powertools.utilities.streaming.s3_object import S3Object\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: Dict[str, str], context: LambdaContext):\ns3 = S3Object(bucket=event[\"bucket\"], key=event[\"key\"])\nfor line in s3:\nprint(line)\n</code></pre> <pre><code>from typing import Dict\n\nfrom aws_lambda_powertools.utilities.streaming.s3_object import S3Object\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: Dict[str, str], context: LambdaContext):\ns3 = S3Object(bucket=event[\"bucket\"], key=event[\"key\"], version_id=event[\"version_id\"])\nfor line in s3:\nprint(line)\n</code></pre>"},{"location":"utilities/streaming/#data-transformations","title":"Data transformations","text":"<p>Think of data transformations like a data processing pipeline - apply one or more in order.</p> <p>As data is streamed, you can apply transformations to your data like decompressing gzip content and deserializing a CSV into a dictionary.</p> <p>For popular data transformations like CSV or Gzip, you can quickly enable it at the constructor level:</p> Decompressing and deserializing CSV <pre><code>from typing import Dict\n\nfrom aws_lambda_powertools.utilities.streaming.s3_object import S3Object\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: Dict[str, str], context: LambdaContext):\ns3 = S3Object(bucket=event[\"bucket\"], key=event[\"key\"], is_gzip=True, is_csv=True)\nfor line in s3:\n        print(line)\n</code></pre> <p>Alternatively, you can apply transformations later via the <code>transform</code> method. By default, it will return the transformed stream you can use to read its contents. If you prefer in-place modifications, use <code>in_place=True</code>.</p> When is this useful? <p>In scenarios where you might have a reusable logic to apply common transformations. This might be a function or a class that receives an instance of <code>S3Object</code>.</p> Returning a new objectTransform in-place <pre><code>from typing import Dict\n\nfrom aws_lambda_powertools.utilities.streaming.s3_object import S3Object\nfrom aws_lambda_powertools.utilities.streaming.transformations import (\n    CsvTransform,\n    GzipTransform,\n)\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: Dict[str, str], context: LambdaContext):\n    s3 = S3Object(bucket=event[\"bucket\"], key=event[\"key\"])\ndata = s3.transform([GzipTransform(), CsvTransform()])\nfor line in data:\n        print(line)  # returns a dict\n</code></pre> <p>Note that when using <code>in_place=True</code>, there is no return (<code>None</code>).</p> <pre><code>from typing import Dict\n\nfrom aws_lambda_powertools.utilities.streaming.s3_object import S3Object\nfrom aws_lambda_powertools.utilities.streaming.transformations import (\n    CsvTransform,\n    GzipTransform,\n)\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: Dict[str, str], context: LambdaContext):\n    s3 = S3Object(bucket=event[\"bucket\"], key=event[\"key\"])\ns3.transform([GzipTransform(), CsvTransform()], in_place=True)\nfor line in s3:\n        print(line)  # returns a dict\n</code></pre>"},{"location":"utilities/streaming/#handling-zip-files","title":"Handling ZIP files","text":"<p><code>ZipTransform</code> doesn't support combining other transformations.</p> <p>This is because a Zip file contains multiple files while transformations apply to a single stream.</p> <p>That said, you can still open a specific file as a stream, reading only the necessary bytes to extract it:</p> Reading an individual file in the zip as a stream<pre><code>from aws_lambda_powertools.utilities.streaming import S3Object\nfrom aws_lambda_powertools.utilities.streaming.transformations import ZipTransform\n\ns3object = S3Object(bucket=\"bucket\", key=\"key\")\nzip_reader = s3object.transform(ZipTransform())\nwith zip_reader.open(\"filename.txt\") as f:\nfor line in f:\n        print(line)\n</code></pre>"},{"location":"utilities/streaming/#built-in-data-transformations","title":"Built-in data transformations","text":"<p>We provide popular built-in transformations that you can apply against your streaming data.</p> Name Description Class name Gzip Gunzips the stream of data using the gzip library GzipTransform Zip Exposes the stream as a ZipFile object ZipTransform CSV Parses each CSV line as a CSV object, returning dictionary objects CsvTransform"},{"location":"utilities/streaming/#advanced","title":"Advanced","text":""},{"location":"utilities/streaming/#skipping-or-reading-backwards","title":"Skipping or reading backwards","text":"<p><code>S3Object</code> implements Python I/O interface. This means you can use <code>seek</code> to start reading contents of your file from any particular position, saving you processing time.</p>"},{"location":"utilities/streaming/#reading-backwards","title":"Reading backwards","text":"<p>For example, let's imagine you have a large CSV file, each row has a non-uniform size (bytes), and you want to read and process the last row only.</p> non_uniform_sample.csv<pre><code>id,name,location\n1,Ruben Fonseca, Denmark\n2,Heitor Lessa, Netherlands\n3,Leandro Damascena, Portugal\n</code></pre> <p>You found out the last row has exactly 30 bytes. We can use <code>seek()</code> to skip to the end of the file, read 30 bytes, then transform to CSV.</p> Reading only the last CSV row<pre><code>import io\nfrom typing import Dict\n\nfrom aws_lambda_powertools.utilities.streaming.s3_object import S3Object\nfrom aws_lambda_powertools.utilities.streaming.transformations import CsvTransform\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nLAST_ROW_SIZE = 30\nCSV_HEADERS = [\"id\", \"name\", \"location\"]\n\n\ndef lambda_handler(event: Dict[str, str], context: LambdaContext):\n    sample_csv = S3Object(bucket=event[\"bucket\"], key=\"sample.csv\")\n\n    # From the end of the file, jump exactly 30 bytes backwards\nsample_csv.seek(-LAST_ROW_SIZE, io.SEEK_END)\n# Transform portion of data into CSV with our headers\nsample_csv.transform(CsvTransform(fieldnames=CSV_HEADERS), in_place=True)\n# We will only read the last portion of the file from S3\n    # as we're only interested in the last 'location' from our dataset\n    for last_row in sample_csv:\n        print(last_row[\"location\"])\n</code></pre>"},{"location":"utilities/streaming/#skipping","title":"Skipping","text":"<p>What if we want to jump the first N rows?</p> <p>You can also solve with <code>seek</code>, but let's take a large uniform CSV file to make this easier to grasp.</p> uniform_sample.csv<pre><code>reading,position,type\n21.3,5,+\n23.4,4,+\n21.3,0,-\n</code></pre> <p>You found out that each row has 8 bytes, the header line has 21 bytes, and every new line has 1 byte. You want to skip the first 100 lines.</p> Skipping the first 100 rows<pre><code>import io\nfrom typing import Dict\n\nfrom aws_lambda_powertools.utilities.streaming.s3_object import S3Object\nfrom aws_lambda_powertools.utilities.streaming.transformations import CsvTransform\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\"\"\"\nAssuming the CSV files contains rows after the header always has 8 bytes + 1 byte newline:\n\nreading,position,type\n21.3,5,+\n23.4,4,+\n21.3,0,-\n...\n\"\"\"\n\nCSV_HEADERS = [\"reading\", \"position\", \"type\"]\nROW_SIZE = 8 + 1  # 1 byte newline\nHEADER_SIZE = 21 + 1  # 1 byte newline\nLINES_TO_JUMP = 100\n\n\ndef lambda_handler(event: Dict[str, str], context: LambdaContext):\n    sample_csv = S3Object(bucket=event[\"bucket\"], key=event[\"key\"])\n\n    # Skip the header line\nsample_csv.seek(HEADER_SIZE, io.SEEK_SET)\n# Jump 100 lines of 9 bytes each (8 bytes of data + 1 byte newline)\nsample_csv.seek(LINES_TO_JUMP * ROW_SIZE, io.SEEK_CUR)\nsample_csv.transform(CsvTransform(), in_place=True)\n    for row in sample_csv:\n        print(row[\"reading\"])\n</code></pre>"},{"location":"utilities/streaming/#custom-options-for-data-transformations","title":"Custom options for data transformations","text":"<p>We will propagate additional options to the underlying implementation for each transform class.</p> Name Available options GzipTransform GzipFile constructor ZipTransform ZipFile constructor CsvTransform DictReader constructor <p>For instance, take <code>ZipTransform</code>. You can use the <code>compression</code> parameter if you want to unzip an S3 object compressed with <code>LZMA</code>.</p> Unzipping LZMA data <pre><code>import zipfile\nfrom typing import Dict\n\nfrom aws_lambda_powertools.utilities.streaming.s3_object import S3Object\nfrom aws_lambda_powertools.utilities.streaming.transformations import ZipTransform\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: Dict[str, str], context: LambdaContext):\n    s3 = S3Object(bucket=event[\"bucket\"], key=event[\"key\"])\n\nzf = s3.transform(ZipTransform(compression=zipfile.ZIP_LZMA))\nprint(zf.nameslist())\n    zf.extract(zf.namelist()[0], \"/tmp\")\n</code></pre> <p>Or, if you want to load a tab-separated file (TSV), you can use the <code>delimiter</code> parameter in the <code>CsvTransform</code>:</p> Deserializing tab-separated data values <pre><code>from typing import Dict\n\nfrom aws_lambda_powertools.utilities.streaming.s3_object import S3Object\nfrom aws_lambda_powertools.utilities.streaming.transformations import CsvTransform\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\n\ndef lambda_handler(event: Dict[str, str], context: LambdaContext):\n    s3 = S3Object(bucket=event[\"bucket\"], key=event[\"key\"])\n\ntsv_stream = s3.transform(CsvTransform(delimiter=\"\\t\"))\nfor obj in tsv_stream:\n        print(obj)\n</code></pre>"},{"location":"utilities/streaming/#building-your-own-data-transformation","title":"Building your own data transformation","text":"<p>You can build your own custom data transformation by extending the <code>BaseTransform</code> class. The <code>transform</code> method receives an <code>IO[bytes]</code> object, and you are responsible for returning an <code>IO[bytes]</code> object.</p> Custom JSON transform <pre><code>import io\nfrom typing import IO, Optional\n\nimport ijson\n\nfrom aws_lambda_powertools.utilities.streaming.transformations import BaseTransform\n\n\n# Using io.RawIOBase gets us default implementations of many of the common IO methods\nclass JsonDeserializer(io.RawIOBase):\ndef __init__(self, input_stream: IO[bytes]):\nself.input = ijson.items(input_stream, \"\", multiple_values=True)\ndef read(self, size: int = -1) -&gt; Optional[bytes]:\n        raise NotImplementedError(f\"{__name__} does not implement read\")\n\n    def readline(self, size: Optional[int] = None) -&gt; bytes:\n        raise NotImplementedError(f\"{__name__} does not implement readline\")\n\n    def read_object(self) -&gt; dict:\n        return self.input.__next__()\n\n    def __next__(self):\n        return self.read_object()\n\n\nclass JsonTransform(BaseTransform):\ndef transform(self, input_stream: IO[bytes]) -&gt; JsonDeserializer:\nreturn JsonDeserializer(input_stream=input_stream)\n</code></pre>"},{"location":"utilities/streaming/#testing-your-code","title":"Testing your code","text":""},{"location":"utilities/streaming/#asserting-data-transformations","title":"Asserting data transformations","text":"<p>Create an input payload using <code>io.BytesIO</code> and assert the response of the transformation:</p> assert_transformation.pyassert_transformation_module.py <pre><code>import io\n\nimport boto3\nfrom assert_transformation_module import UpperTransform\nfrom botocore import stub\n\nfrom aws_lambda_powertools.utilities.streaming import S3Object\nfrom aws_lambda_powertools.utilities.streaming.compat import PowertoolsStreamingBody\n\n\ndef test_upper_transform():\n    # GIVEN\ndata_stream = io.BytesIO(b\"hello world\")\n# WHEN\ndata_stream = UpperTransform().transform(data_stream)\n# THEN\n    assert data_stream.read() == b\"HELLO WORLD\"\n\n\ndef test_s3_object_with_upper_transform():\n    # GIVEN\n    payload = b\"hello world\"\ns3_client = boto3.client(\"s3\")\ns3_stub = stub.Stubber(s3_client)\ns3_stub.add_response(\n\"get_object\", {\"Body\": PowertoolsStreamingBody(raw_stream=io.BytesIO(payload), content_length=len(payload))}\n)\ns3_stub.activate()\n# WHEN\ndata_stream = S3Object(bucket=\"bucket\", key=\"key\", boto3_client=s3_client)\ndata_stream.transform(UpperTransform(), in_place=True)\n# THEN\n    assert data_stream.read() == b\"HELLO WORLD\"\n</code></pre> <pre><code>import io\nfrom typing import IO, Optional\n\nfrom aws_lambda_powertools.utilities.streaming.transformations import BaseTransform\n\n\nclass UpperIO(io.RawIOBase):\n    def __init__(self, input_stream: IO[bytes], encoding: str):\n        self.encoding = encoding\n        self.input_stream = io.TextIOWrapper(input_stream, encoding=encoding)\n\n    def read(self, size: int = -1) -&gt; Optional[bytes]:\n        data = self.input_stream.read(size)\n        return data.upper().encode(self.encoding)\n\nclass UpperTransform(BaseTransform):\n    def transform(self, input_stream: IO[bytes]) -&gt; UpperIO:\n        return UpperIO(input_stream=input_stream, encoding=\"utf-8\")\n</code></pre>"},{"location":"utilities/streaming/#known-limitations","title":"Known limitations","text":""},{"location":"utilities/streaming/#aws-x-ray-segment-size-limit","title":"AWS X-Ray segment size limit","text":"<p>We make multiple API calls to S3 as you read chunks from your S3 object. If your function is decorated with Tracer, you can easily hit AWS X-Ray 64K segment size when processing large files.</p> <p>Use tracer decorators in parts where you don't read your <code>S3Object</code> instead.</p>"},{"location":"utilities/typing/","title":"Typing","text":"<p>This typing utility provides static typing classes that can be used to ease the development by providing the IDE type hints.</p>"},{"location":"utilities/typing/#key-features","title":"Key features","text":"<ul> <li>Add static typing classes</li> <li>Ease the development by leveraging your IDE's type hints</li> <li>Avoid common typing mistakes in Python</li> </ul>"},{"location":"utilities/typing/#getting-started","title":"Getting started","text":"Tip <p>All examples shared in this documentation are available within the project repository.</p> <p>We provide static typing for any context methods or properties implemented by Lambda context object.</p>"},{"location":"utilities/typing/#lambdacontext","title":"LambdaContext","text":"<p>The <code>LambdaContext</code> typing is typically used in the handler method for the Lambda function.</p> getting_started_validator_decorator_function.py <pre><code>from aws_lambda_powertools.utilities.typing import LambdaContext\ndef handler(event: dict, context: LambdaContext) -&gt; dict:\n# Insert business logic\n    return event\n</code></pre>"},{"location":"utilities/typing/#working-with-context-methods-and-properties","title":"Working with context methods and properties","text":"<p>Using <code>LambdaContext</code> typing makes it possible to access information and hints of all properties and methods implemented by Lambda context object.</p> working_with_context_function.py <pre><code>from time import sleep\n\nimport requests\n\nfrom aws_lambda_powertools import Logger\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nlogger = Logger()\n\n\ndef lambda_handler(event, context: LambdaContext) -&gt; dict:\n\n    limit_execution: int = 1000  # milliseconds\n\n    # scrape website and exit before lambda timeout\nwhile context.get_remaining_time_in_millis() &gt; limit_execution:\ncomments: requests.Response = requests.get(\"https://jsonplaceholder.typicode.com/comments\")\n        # add logic here and save the results of the request to an S3 bucket, for example.\n\n        logger.info(\n            {\n                \"operation\": \"scrape_website\",\n                \"request_id\": context.aws_request_id,\n\"remaining_time\": context.get_remaining_time_in_millis(),\n\"comments\": comments.json()[:2],\n}\n        )\n\n        sleep(1)\n\n    return {\"message\": \"Success\"}\n</code></pre> <p> </p>"},{"location":"utilities/validation/","title":"Validation","text":"<p>This utility provides JSON Schema validation for events and responses, including JMESPath support to unwrap events before validation.</p>"},{"location":"utilities/validation/#key-features","title":"Key features","text":"<ul> <li>Validate incoming event and response</li> <li>JMESPath support to unwrap events before validation applies</li> <li>Built-in envelopes to unwrap popular event sources payloads</li> </ul>"},{"location":"utilities/validation/#getting-started","title":"Getting started","text":"Tip <p>All examples shared in this documentation are available within the project repository.</p> <p>You can validate inbound and outbound events using <code>validator</code> decorator.</p> <p>You can also use the standalone <code>validate</code> function, if you want more control over the validation process such as handling a validation error.</p> Tip: Using JSON Schemas for the first time? <p>Check this step-by-step tour in the official JSON Schema website.</p> <p>We support any JSONSchema draft supported by fastjsonschema library.</p> Warning <p>Both <code>validator</code> decorator and <code>validate</code> standalone function expects your JSON Schema to be a dictionary, not a filename.</p>"},{"location":"utilities/validation/#install","title":"Install","text":"<p>This is not necessary if you're installing Powertools via Lambda Layer/SAR</p> <p>Add <code>aws-lambda-powertools[validation]</code> as a dependency in your preferred tool: e.g., requirements.txt, pyproject.toml. This will ensure you have the required dependencies before using Validation.</p>"},{"location":"utilities/validation/#validator-decorator","title":"Validator decorator","text":"<p>Validator decorator is typically used to validate either inbound or functions' response.</p> <p>It will fail fast with <code>SchemaValidationError</code> exception if event or response doesn't conform with given JSON Schema.</p> getting_started_validator_decorator_function.pygetting_started_validator_decorator_schema.pygetting_started_validator_decorator_payload.json <pre><code>from dataclasses import dataclass, field\nfrom uuid import uuid4\n\nimport getting_started_validator_decorator_schema as schemas\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nfrom aws_lambda_powertools.utilities.validation import validator\n# we can get list of allowed IPs from AWS Parameter Store using Parameters Utility\n# See: https://awslabs.github.io/aws-lambda-powertools-python/latest/utilities/parameters/\nALLOWED_IPS = parameters.get_parameter(\"/lambda-powertools/allowed_ips\")\n\n\nclass UserPermissionsError(Exception):\n    ...\n\n\n@dataclass\nclass User:\n    ip: str\n    permissions: list\n    user_id: str = field(default_factory=lambda: f\"{uuid4()}\")\n    name: str = \"Project Lambda Powertools\"\n\n\n# using a decorator to validate input and output data\n@validator(inbound_schema=schemas.INPUT, outbound_schema=schemas.OUTPUT)\ndef lambda_handler(event, context: LambdaContext) -&gt; dict:\n\n    try:\n        user_details: dict = {}\n\n        # get permissions by user_id and project\n        if (\n            event.get(\"user_id\") == \"0d44b083-8206-4a3a-aa95-5d392a99be4a\"\n            and event.get(\"project\") == \"powertools\"\n            and event.get(\"ip\") in ALLOWED_IPS\n        ):\n            user_details = User(ip=event.get(\"ip\"), permissions=[\"read\", \"write\"]).__dict__\n\n# the body must be an object because must match OUTPUT schema, otherwise it fails\nreturn {\"body\": user_details or None, \"statusCode\": 200 if user_details else 204}\n    except Exception as e:\n        raise UserPermissionsError(str(e))\n</code></pre> <pre><code>INPUT = {\n    \"$schema\": \"http://json-schema.org/draft-07/schema\",\n    \"$id\": \"http://example.com/example.json\",\n    \"type\": \"object\",\n    \"title\": \"Sample schema\",\n    \"description\": \"The root schema comprises the entire JSON document.\",\n    \"examples\": [{\"user_id\": \"0d44b083-8206-4a3a-aa95-5d392a99be4a\", \"project\": \"powertools\", \"ip\": \"192.168.0.1\"}],\n    \"required\": [\"user_id\", \"project\", \"ip\"],\n    \"properties\": {\n\"user_id\": {\n\"$id\": \"#/properties/user_id\",\n\"type\": \"string\",\n\"title\": \"The user_id\",\n            \"examples\": [\"0d44b083-8206-4a3a-aa95-5d392a99be4a\"],\n            \"maxLength\": 50,\n        },\n\"project\": {\n\"$id\": \"#/properties/project\",\n\"type\": \"string\",\n\"title\": \"The project\",\n            \"examples\": [\"powertools\"],\n            \"maxLength\": 30,\n        },\n\"ip\": {\n\"$id\": \"#/properties/ip\",\n\"type\": \"string\",\n\"title\": \"The ip\",\n\"format\": \"ipv4\",\n\"examples\": [\"192.168.0.1\"],\n            \"maxLength\": 30,\n        },\n    },\n}\n\nOUTPUT = {\n    \"$schema\": \"http://json-schema.org/draft-07/schema\",\n    \"$id\": \"http://example.com/example.json\",\n    \"type\": \"object\",\n    \"title\": \"Sample outgoing schema\",\n    \"description\": \"The root schema comprises the entire JSON document.\",\n    \"examples\": [{\"statusCode\": 200, \"body\": {}}],\n    \"required\": [\"statusCode\", \"body\"],\n    \"properties\": {\n\"statusCode\": {\n\"$id\": \"#/properties/statusCode\",\n\"type\": \"integer\",\n\"title\": \"The statusCode\",\n            \"examples\": [200],\n            \"maxLength\": 3,\n        },\n\"body\": {\n\"$id\": \"#/properties/body\",\n\"type\": \"object\",\n\"title\": \"The body\",\n            \"examples\": [\n                '{\"ip\": \"192.168.0.1\", \"permissions\": [\"read\", \"write\"], \"user_id\": \"7576b683-295e-4f69-b558-70e789de1b18\", \"name\": \"Project Lambda Powertools\"}'  # noqa E501\n            ],\n        },\n    },\n}\n</code></pre> <pre><code>{\n\"user_id\": \"0d44b083-8206-4a3a-aa95-5d392a99be4a\",\n\"project\": \"powertools\",\n\"ip\": \"192.168.0.1\"\n}\n</code></pre> Note <p>It's not a requirement to validate both inbound and outbound schemas - You can either use one, or both.</p>"},{"location":"utilities/validation/#validate-function","title":"Validate function","text":"<p>Validate standalone function is typically used within the Lambda handler, or any other methods that perform data validation.</p> <p>You can also gracefully handle schema validation errors by catching <code>SchemaValidationError</code> exception.</p> getting_started_validator_standalone_function.pygetting_started_validator_standalone_schema.pygetting_started_validator_standalone_payload.json <pre><code>import getting_started_validator_standalone_schema as schemas\n\nfrom aws_lambda_powertools.utilities import parameters\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nfrom aws_lambda_powertools.utilities.validation import SchemaValidationError, validate\n# we can get list of allowed IPs from AWS Parameter Store using Parameters Utility\n# See: https://awslabs.github.io/aws-lambda-powertools-python/latest/utilities/parameters/\nALLOWED_IPS = parameters.get_parameter(\"/lambda-powertools/allowed_ips\")\n\n\ndef lambda_handler(event, context: LambdaContext) -&gt; dict:\n    try:\n        user_authenticated: str = \"\"\n\n# using standalone function to validate input data only\nvalidate(event=event, schema=schemas.INPUT)\nif (\n            event.get(\"user_id\") == \"0d44b083-8206-4a3a-aa95-5d392a99be4a\"\n            and event.get(\"project\") == \"powertools\"\n            and event.get(\"ip\") in ALLOWED_IPS\n        ):\n            user_authenticated = \"Allowed\"\n\n# in this example the body can be of any type because we are not validating the OUTPUT\nreturn {\"body\": user_authenticated, \"statusCode\": 200 if user_authenticated else 204}\n    except SchemaValidationError as exception:\n        # SchemaValidationError indicates where a data mismatch is\n        return {\"body\": str(exception), \"statusCode\": 400}\n</code></pre> <pre><code>INPUT = {\n    \"$schema\": \"http://json-schema.org/draft-07/schema\",\n    \"$id\": \"http://example.com/example.json\",\n    \"type\": \"object\",\n    \"title\": \"Sample schema\",\n    \"description\": \"The root schema comprises the entire JSON document.\",\n\"examples\": [{\"user_id\": \"0d44b083-8206-4a3a-aa95-5d392a99be4a\", \"powertools\": \"lessa\", \"ip\": \"192.168.0.1\"}],\n\"required\": [\"user_id\", \"project\", \"ip\"],\n\"properties\": {\n\"user_id\": {\n\"$id\": \"#/properties/user_id\",\n\"type\": \"string\",\n\"title\": \"The user_id\",\n            \"examples\": [\"0d44b083-8206-4a3a-aa95-5d392a99be4a\"],\n            \"maxLength\": 50,\n        },\n\"project\": {\n\"$id\": \"#/properties/project\",\n\"type\": \"string\",\n\"title\": \"The project\",\n            \"examples\": [\"powertools\"],\n            \"maxLength\": 30,\n        },\n\"ip\": {\n\"$id\": \"#/properties/ip\",\n\"type\": \"string\",\n\"title\": \"The ip\",\n\"format\": \"ipv4\",\n\"examples\": [\"192.168.0.1\"],\n            \"maxLength\": 30,\n        },\n    },\n}\n</code></pre> <pre><code>{\n\"user_id\": \"0d44b083-8206-4a3a-aa95-5d392a99be4a\",\n\"project\": \"powertools\",\n\"ip\": \"192.168.0.1\"\n}\n</code></pre>"},{"location":"utilities/validation/#unwrapping-events-prior-to-validation","title":"Unwrapping events prior to validation","text":"<p>You might want to validate only a portion of your event - This is what the <code>envelope</code> parameter is for.</p> <p>Envelopes are JMESPath expressions to extract a portion of JSON you want before applying JSON Schema validation.</p> <p>Here is a sample custom EventBridge event, where we only validate what's inside the <code>detail</code> key:</p> getting_started_validator_unwrapping_function.pygetting_started_validator_unwrapping_schema.pygetting_started_validator_unwrapping_payload.json <pre><code>import boto3\nimport getting_started_validator_unwrapping_schema as schemas\nfrom aws_lambda_powertools.utilities.data_classes.event_bridge_event import (\n    EventBridgeEvent,\n)\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nfrom aws_lambda_powertools.utilities.validation import validator\ns3_client = boto3.resource(\"s3\")\n\n\n# we use the 'envelope' parameter to extract the payload inside the 'detail' key before validating\n@validator(inbound_schema=schemas.INPUT, envelope=\"detail\")\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    my_event = EventBridgeEvent(event)\n    data = my_event.detail.get(\"data\", {})\n    s3_bucket, s3_key = data.get(\"s3_bucket\"), data.get(\"s3_key\")\n\n    try:\n        s3_object = s3_client.Object(bucket_name=s3_bucket, key=s3_key)\n        payload = s3_object.get()[\"Body\"]\n        content = payload.read().decode(\"utf-8\")\n\n        return {\"message\": process_data_object(content), \"success\": True}\n    except s3_client.meta.client.exceptions.NoSuchBucket as exception:\n        return return_error_message(str(exception))\n    except s3_client.meta.client.exceptions.NoSuchKey as exception:\n        return return_error_message(str(exception))\n\n\ndef return_error_message(message: str) -&gt; dict:\n    return {\"message\": message, \"success\": False}\n\n\ndef process_data_object(content: str) -&gt; str:\n    # insert logic here\n    return \"Data OK\"\n</code></pre> <pre><code>INPUT = {\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"$id\": \"https://example.com/object1660222326.json\",\n    \"type\": \"object\",\n    \"title\": \"Sample schema\",\n    \"description\": \"The root schema comprises the entire JSON document.\",\n    \"examples\": [\n        {\n\"data\": {\n\"s3_bucket\": \"aws-lambda-powertools\",\n\"s3_key\": \"event.txt\",\n\"file_size\": 200,\n\"file_type\": \"text/plain\",\n}\n}\n    ],\n    \"required\": [\"data\"],\n    \"properties\": {\n        \"data\": {\n            \"$id\": \"#root/data\",\n            \"title\": \"Root\",\n            \"type\": \"object\",\n\"required\": [\"s3_bucket\", \"s3_key\", \"file_size\", \"file_type\"],\n\"properties\": {\n\"s3_bucket\": {\n\"$id\": \"#root/data/s3_bucket\",\n                    \"title\": \"The S3 Bucker\",\n\"type\": \"string\",\n\"default\": \"\",\n                    \"examples\": [\"aws-lambda-powertools\"],\n                    \"pattern\": \"^.*$\",\n                },\n\"s3_key\": {\n\"$id\": \"#root/data/s3_key\",\n                    \"title\": \"The S3 Key\",\n\"type\": \"string\",\n\"default\": \"\",\n                    \"examples\": [\"folder/event.txt\"],\n                    \"pattern\": \"^.*$\",\n                },\n\"file_size\": {\n\"$id\": \"#root/data/file_size\",\n                    \"title\": \"The file size\",\n\"type\": \"integer\",\n\"examples\": [200],\n                    \"default\": 0,\n                },\n\"file_type\": {\n\"$id\": \"#root/data/file_type\",\n                    \"title\": \"The file type\",\n\"type\": \"string\",\n\"default\": \"\",\n                    \"examples\": [\"text/plain\"],\n                    \"pattern\": \"^.*$\",\n                },\n            },\n        }\n    },\n}\n</code></pre> <pre><code>{\n\"id\": \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\",\n\"detail-type\": \"CustomEvent\",\n\"source\": \"mycompany.service\",\n\"account\": \"123456789012\",\n\"time\": \"1970-01-01T00:00:00Z\",\n\"region\": \"us-east-1\",\n\"resources\": [],\n\"detail\": {\n\"data\": {\n\"s3_bucket\": \"aws-lambda-powertools\",\n\"s3_key\": \"folder/event.txt\",\n\"file_size\": 200,\n\"file_type\": \"text/plain\"\n}\n}\n}\n</code></pre> <p>This is quite powerful because you can use JMESPath Query language to extract records from arrays, combine pipe and function expressions.</p> <p>When combined, these features allow you to extract what you need before validating the actual payload.</p>"},{"location":"utilities/validation/#built-in-envelopes","title":"Built-in envelopes","text":"<p>We provide built-in envelopes to easily extract the payload from popular event sources.</p> unwrapping_popular_event_source_function.pyunwrapping_popular_event_source_schema.pyunwrapping_popular_event_source_payload.json <pre><code>import boto3\nimport unwrapping_popular_event_source_schema as schemas\nfrom botocore.exceptions import ClientError\n\nfrom aws_lambda_powertools.utilities.data_classes.event_bridge_event import (\n    EventBridgeEvent,\n)\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nfrom aws_lambda_powertools.utilities.validation import envelopes, validator\n# extracting detail from EventBridge custom event\n# see: https://awslabs.github.io/aws-lambda-powertools-python/latest/utilities/jmespath_functions/#built-in-envelopes\n@validator(inbound_schema=schemas.INPUT, envelope=envelopes.EVENTBRIDGE)\ndef lambda_handler(event: dict, context: LambdaContext) -&gt; dict:\n    my_event = EventBridgeEvent(event)\n    ec2_client = boto3.resource(\"ec2\", region_name=my_event.region)\n\n    try:\n        instance_id = my_event.detail.get(\"instance_id\")\n        instance = ec2_client.Instance(instance_id)\n        instance.stop()\n\n        return {\"message\": f\"Successfully stopped {instance_id}\", \"success\": True}\n    except ClientError as exception:\n        return {\"message\": str(exception), \"success\": False}\n</code></pre> <pre><code>INPUT = {\n    \"definitions\": {},\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"$id\": \"https://example.com/object1660233148.json\",\n    \"title\": \"Root\",\n    \"type\": \"object\",\n\"required\": [\"instance_id\", \"region\"],\n\"properties\": {\n\"instance_id\": {\n\"$id\": \"#root/instance_id\",\n            \"title\": \"Instance_id\",\n\"type\": \"string\",\n\"default\": \"\",\n            \"examples\": [\"i-042dd005362091826\"],\n            \"pattern\": \"^.*$\",\n        },\n\"region\": {\n\"$id\": \"#root/region\",\n            \"title\": \"Region\",\n\"type\": \"string\",\n\"default\": \"\",\n            \"examples\": [\"us-east-1\"],\n            \"pattern\": \"^.*$\",\n        },\n    },\n}\n</code></pre> <pre><code>{\n\"id\": \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\",\n\"detail-type\": \"Scheduled Event\",\n\"source\": \"aws.events\",\n\"account\": \"123456789012\",\n\"time\": \"1970-01-01T00:00:00Z\",\n\"region\": \"us-east-1\",\n\"resources\": [\n\"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\"\n],\n\"detail\": {\n\"instance_id\": \"i-042dd005362091826\",\n\"region\": \"us-east-2\"\n}\n}\n</code></pre> <p>Here is a handy table with built-in envelopes along with their JMESPath expressions in case you want to build your own.</p> Envelope JMESPath expression <code>API_GATEWAY_HTTP</code> <code>powertools_json(body)</code> <code>API_GATEWAY_REST</code> <code>powertools_json(body)</code> <code>CLOUDWATCH_EVENTS_SCHEDULED</code> <code>detail</code> <code>CLOUDWATCH_LOGS</code> <code>awslogs.powertools_base64_gzip(data) | powertools_json(@).logEvents[*]</code> <code>EVENTBRIDGE</code> <code>detail</code> <code>KINESIS_DATA_STREAM</code> <code>Records[*].kinesis.powertools_json(powertools_base64(data))</code> <code>SNS</code> <code>Records[0].Sns.Message | powertools_json(@)</code> <code>SQS</code> <code>Records[*].powertools_json(body)</code>"},{"location":"utilities/validation/#advanced","title":"Advanced","text":""},{"location":"utilities/validation/#validating-custom-formats","title":"Validating custom formats","text":"Note <p>JSON Schema DRAFT 7 has many new built-in formats such as date, time, and specifically a regex format which might be a better replacement for a custom format, if you do have control over the schema.</p> <p>JSON Schemas with custom formats like <code>awsaccountid</code> will fail validation. If you have these, you can pass them using <code>formats</code> parameter:</p> custom_json_schema_type_format.json<pre><code>{\n\"accountid\": {\n\"format\": \"awsaccountid\",\n\"type\": \"string\"\n}\n}\n</code></pre> <p>For each format defined in a dictionary key, you must use a regex, or a function that returns a boolean to instruct the validator on how to proceed when encountering that type.</p> custom_format_function.pycustom_format_schema.pycustom_format_payload.json <pre><code>import json\nimport re\n\nimport boto3\nimport custom_format_schema as schemas\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\nfrom aws_lambda_powertools.utilities.validation import SchemaValidationError, validate\n# awsaccountid must have 12 digits\ncustom_format = {\"awsaccountid\": lambda value: re.match(r\"^(\\d{12})$\", value)}\ndef lambda_handler(event, context: LambdaContext) -&gt; dict:\n    try:\n        # validate input using custom json format\nvalidate(event=event, schema=schemas.INPUT, formats=custom_format)\nclient_organization = boto3.client(\"organizations\", region_name=event.get(\"region\"))\n        account_data = client_organization.describe_account(AccountId=event.get(\"accountid\"))\n\n        return {\n            \"account\": json.dumps(account_data.get(\"Account\"), default=str),\n            \"message\": \"Success\",\n            \"statusCode\": 200,\n        }\nexcept SchemaValidationError as exception:\nreturn return_error_message(str(exception))\n    except Exception as exception:\n        return return_error_message(str(exception))\n\n\ndef return_error_message(message: str) -&gt; dict:\n    return {\"account\": None, \"message\": message, \"statusCode\": 400}\n</code></pre> <pre><code>INPUT = {\n    \"definitions\": {},\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"$id\": \"https://example.com/object1660245931.json\",\n    \"title\": \"Root\",\n    \"type\": \"object\",\n\"required\": [\"accountid\", \"region\"],\n\"properties\": {\n\"accountid\": {\n\"$id\": \"#root/accountid\",\n            \"title\": \"The accountid\",\n\"type\": \"string\",\n\"format\": \"awsaccountid\",\n\"default\": \"\",\n            \"examples\": [\"123456789012\"],\n        },\n\"region\": {\n\"$id\": \"#root/region\",\n            \"title\": \"The region\",\n\"type\": \"string\",\n\"default\": \"\",\n            \"examples\": [\"us-east-1\"],\n            \"pattern\": \"^.*$\",\n        },\n    },\n}\n</code></pre> <pre><code>{\n\"accountid\": \"200984112386\",\n\"region\": \"us-east-1\"\n}\n</code></pre>"},{"location":"utilities/validation/#built-in-jmespath-functions","title":"Built-in JMESPath functions","text":"<p>You might have events or responses that contain non-encoded JSON, where you need to decode before validating them.</p> <p>You can use our built-in JMESPath functions within your expressions to do exactly that to deserialize JSON Strings, decode base64, and decompress gzip data.</p> Info <p>We use these for built-in envelopes to easily to decode and unwrap events from sources like Kinesis, CloudWatch Logs, etc.</p>"}]}